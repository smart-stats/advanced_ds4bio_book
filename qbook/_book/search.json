[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Advanced Data Science for Public Health",
    "section": "",
    "text": "This is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book for the Advanced Data Science for Bio/Public Health/medical classes. Since data science isn’t super well defined, advanced data science is even less so. My opinion is that we needed the umbrella term data science because there was a lot in the processes of analyzing data that got ignored in traditional disciplines and training programs. (Of course, what gets ignored is different depeneding on which discipline or program.) I’m mostly going to focus on concepts and implementation that were historically ignored in our (JHU Biostat) program, which is heavily focused on biostatistical inference, probability modeling, public health/bio/medical data analyses and ML.\n\n\nI’m going to assume that you have a lot of basic data science tools down already. If not, here’s some notes. For this book you’ll need: prior programming experience, calculus, linear algebra, unix, python, R and some basic statistics.\n\n\n\nThis is a two quarter course. The first quater is devoted to tools and the second is deveoted to theory. So the book is divided in half that way."
  },
  {
    "objectID": "interactive.html",
    "href": "interactive.html",
    "title": "2  Interactive graphics",
    "section": "",
    "text": "In your other DS courses, you’ve learned how to create static graphics uses R, ggplot, matplotlib, seaborn … You’ve probably also learned how to create client side interactive graphics using libraries like plotly and maybe also learned client-server interactivity with shiny, dash …\nIn this section we’re going to dig deeper into client side graphics, which are almost always done via html, css, javascript and a javascript plotting library. We’re going to focus on d3.js, a well known javascript library for creating interactive data visulalizations.\nTools like d3 are mostly for creating professional data web graphics. So, most of our daily graphics use will just use python/R/julia/matlab … or plotting libraries like plotly. Usually, you want to prototype graphics outside of d3. Here, we’ll give you a smidge of using d3 to get you started if your goal is to become a graphics expert."
  },
  {
    "objectID": "interactive.html#introduction-to-d3",
    "href": "interactive.html#introduction-to-d3",
    "title": "2  Interactive graphics",
    "section": "2.1 Introduction to D3",
    "text": "2.1 Introduction to D3\nLet’s get started. I’m going to assume that you have a basic knowledge of html, css and a little bit of javascript. D3 works by manipulating html elements. Let’s select every paragraph element in a document.\n<!DOCTYPE html>\n<html lang=\"en\">\n\n<head>\n    <script src=\"https://d3js.org/d3.v5.min.js\"></script>\n</head>\n\n<body>\n    <p> Advanced </p>\n    <p> Data science </p> \n        <script>\n            let pselect = d3.selectAll(\"p\")\n            //let pselect = d3.select(\"p\").style(\"color\", \"green\");\n            //let pselect = d3.selectAll(\"p\").style(\"color\", \"green\");\n        </script>\n    </body>\n</html>\nGoing forward, we’ll omit most of the html commands.\n\nThe command <script src=\"https://d3js.org/d3.v5.min.js\"></script> loads d3 from a CDN. You could also download it locally if you’d like.\nThe script let pselect = d3.selectAll(\"p\").style(\"color\", \"green\"); creates a variable pselect that is all of the html paragraph elements\nTry doing this, loading the web page, then try uncommenting each other script line in turn and refreshing\nIn chrome do Ctrl-shift-i to get the developer console and inspect the variable pselect.\nNesting select or selectAll will select elements within the selected elements.\nYou can also select by id or class."
  },
  {
    "objectID": "interactive.html#a-simple-example",
    "href": "interactive.html#a-simple-example",
    "title": "2  Interactive graphics",
    "section": "2.2 A simple example",
    "text": "2.2 A simple example\nLet’s go through an example where we plot brain volumetric ROI data on the log scale using D3.\n<style>\n    .bar {\n        background: #f5b634;\n        border: 4px solid #0769ad;\n        height: 20px;\n    }\n</style>\n<body>\n        <script>\n            let roiData = [\n                {\"roi\": \"Telencephalon_L\", \"volume\" : 531111},\n                {\"roi\": \"Telencephalon_R\", \"volume\" : 543404},\n                {\"roi\": \"Diencephalon_L\",  \"volume\" : 9683  },\n                {\"roi\": \"Diencephalon_R\",  \"volume\" : 9678  },\n                {\"roi\": \"Mesencephalon\",   \"volume\" : 10268 },\n                {\"roi\": \"Metencephalon\",   \"volume\" : 159402},\n                {\"roi\": \"Myelencephalon\",  \"volume\" : 4973  },\n                {\"roi\": \"CSF\",             \"volume\" : 109776}\n            ];\n    \n            let divSelection = d3.select(\"body\") \n                    .selectAll(\"div\")\n                    .data(roiData)\n                    .enter()\n                    .append('div')\n                    .attr(\"class\", \"bar\")\n                    .style(\"width\", (d) => {return Math.log(d.volume) * 20 + \"px\"; })\n                    .text(d => d.roi)\n                    .on(\"mouseover\", function(){\n                        d3.select(this)\n                        .style(\"background-color\", \"orange\");\n                    })\n                    .on(\"mouseout\", function(){\n                        d3.select(this)\n                        .style(\"background-color\",\"#33A2FF\" )\n                    })        </script>\n    </body>\n\nThe data(roiDat) selects our dataset\nThe enter() and append('div') commands add div elements to the html document, one per data element.\nThe attr method considers our bar stylesheet style\nThe style method changes the style so that the bars have the width of our data. The notation (d) => {return d.volume * .001 + \"px\"} is a function that selects the ROI element of the data, multiplies it by .001 then converts it to text with px at the end.\nThe text method at the end appends the text to our plot\nThe on methods say what to do when one mouses over and off the bars. You can see now that they turn orange then back. Remove the mouseout .on call and see what happens.\n\nThe output looks like this. Hover over a bar to test. (Look at the file in d3/roi1.html)"
  },
  {
    "objectID": "interactive.html#working-through-a-realistic-example",
    "href": "interactive.html#working-through-a-realistic-example",
    "title": "2  Interactive graphics",
    "section": "2.3 Working through a realistic example",
    "text": "2.3 Working through a realistic example\nUnder assets/kirby_pivot.csv is a dataset with the kirby 21 data pivoted to have regions as columns. Let’s work through a d3 example of ploting right versus left asymmetry in the telencephalon (the largest area of the brain including the cortex and central white matter).\nHere’s the scatterplot that I’ve got so far. For HW, add text labels to the point, or a tooltip that gives point information when you hover over it.\n\nThe code for the plot is in d3/roi2.html. Let’s go over some of the main parts of the d3 code here. First, we set up the graphic\nconst h = 500\nconst w = 500\n\n// create the background\nlet svg = d3.select(\"body\")\n    .append(\"svg\")\n    .attr(\"width\" , h)\n    .attr(\"height\", w);\nNext we load in the data. First, we create a function that does a little row processing for us. Honestly, it’s probably better to just do this in python/R/julia … beforehand, but it’s worth showing here. We create variables for the log ratio between the right and left hemispheres and the log of the geometric mean. We’ll use this to create a Tukey mean/difference plot of the log of the volumes.\n//create the variables we're interested in\nlet rowConverter = function(d) {\n    return {\n        id : d.id,\n        //y is going to be the log difference R-L\n        logratio : Math.log(parseFloat(d.Telencephalon_R)) - Math.log(parseFloat(d.Telencephalon_L)),\n        //x is going to be the average log \n        loggm : (Math.log(parseFloat(d.Telencephalon_L)) + Math.log(parseFloat(d.Telencephalon_R))) * .5\n    };\n    }\n\n//the location where I'm pulling the csv from\nlet dataloc = \"https://raw.githubusercontent.com/smart-stats/advanced_ds4bio_book/main/qbook/assets/kirby_pivot.csv\"\n\n//read in the data and parse the rows \nkirby_pivot = d3.csv(dataloc, rowConverter)\nModern js uses something called ‘promises’, which alllows for asynchronous evaluation. When we read in our csv file, it gets created as a promise and not an array like we need. The result is that our plotting commands need to then be called as a method from the promise object. The reason for this is so that it only uses the data when the data is actually loaded (i.e. promise fulfilled.) So, the plotting commmands for us look like this.\nkirby_pivot.then(dat => {\n    PLOTTING COMMANDS\n})\nJust a reminder that the notation d => g(d) is JS shorthand for function(d) {return g(d);} and is used heavily in d3 coding. Now let’s fill in PLOTTING COMMANDS. First, let’s fill in some utility functions. We get the range of our x and y values to help set up our axes. d3 scales map our function values to a range we want. So let’s create scale maps for x, y and color and then also set up axes using those scales. We’ll also go ahead on plot our axes so they’re on the bottom.\nmaxx = d3.max(dat, d => d.loggm)\nminx = d3.min(dat, d => d.loggm)\nmaxy = d3.max(dat, d => d.logratio)\nminy = d3.min(dat, d => d.logratio)\n\n//fudge is the boundary otherwise points get chopped off\nlet fudge = 50\n\nlet yScale = d3.scaleLinear()\n    .domain([miny, maxy])\n    .range([h-fudge, fudge])\n\nlet pointScale = d3.scaleLinear()\n    .domain([miny, maxy])\n    .range([5, 10])\n\nlet colorScale = d3.scaleLinear()\n    .domain([miny, maxy])\n    .range([0, 1])\n\n\nlet xScale = d3.scaleLinear()\n    .domain([minx, maxx])\n    .range([w-fudge, fudge]);\n\n// define the axes\nlet xaxis = d3.axisBottom().scale(xScale)\nlet yaxis = d3.axisLeft().scale(yScale)\nsvg.append(\"g\")\n    .attr(\"class\", \"axis\")\n    .attr(\"transform\", \"translate(0,\" + (h - fudge) + \")\")\n    .call(xaxis)\n\nsvg.append(\"g\")\n    .attr(\"class\", \"axis\")\n    .attr(\"transform\", \"translate(\" + fudge + \",0)\")\n    .call(yaxis)\nNow let’s create the plot. We’re going to add circles at each location, which is attributes cx and cy. Notice we use our previous defined scales to give their locations. Also, we’ll set the color and size relative to the logratio. Finally, when we mouseover a point, let’s change the radius then change it back when we mouseoff.\nsvg.selectAll(\"circle\")\n    .data(dat)\n    .enter()\n    .append(\"circle\")\n    .attr(\"cy\", d => yScale(d.logratio))\n    .attr(\"cx\", d => xScale(d.loggm))\n    .attr(\"r\",  d => pointScale(d.logratio))\n    .attr(\"fill\", d => d3.interpolateWarm(colorScale(d.logratio)))\n    .attr(\"stroke\", \"black\")\n    .on(\"mouseover\", function() {\n        d3.select(this)\n            .attr(\"r\", 30)\n        })\n    .on(\"mouseout\", function() {\n        d3.select(this)\n        .attr(\"r\",  d => pointScale(d.logratio))\n    })\nObviously, this is a lot of work for a simple scatterplot. The difference is that here you have total control over plotting and interactivity elements."
  },
  {
    "objectID": "interactive.html#observable-and-observable-plot",
    "href": "interactive.html#observable-and-observable-plot",
    "title": "2  Interactive graphics",
    "section": "2.4 Observable and Observable Plot",
    "text": "2.4 Observable and Observable Plot\nObserverable is a notebook for working with d3. It’s quite neat since mixing javascript coding in a web notebook, which itself is written in javascript, makes for an interesting setup. Typically, one would do the data preprocessing in R, python, julia … then do the advanced graphing in d3. In addition to accepting d3 as inputs, observable has a slightly higher set of utility functions called observable plot. (Quarto, which this document is in, allows for observable cells.) So, let’s read in some ROI data and plot it in observable plot. Note this is the average of the Type I Level I ROIs. Notice this is much easier than using d3 directly.\n\ndata = FileAttachment(\"assets/kirby_avg.csv\").csv();\nPlot.plot({\nmarks: [Plot.barY(data, {x: \"roi\", y: \"volume\", fill : 'roi'})],\n    x: {tickRotate: 45},\n    color: {scheme: \"spectral\"},    \n    height: 400,\n    width: 400,\n    marginBottom: 100\n\n})"
  },
  {
    "objectID": "interactive.html#links",
    "href": "interactive.html#links",
    "title": "2  Interactive graphics",
    "section": "2.5 Links",
    "text": "2.5 Links\n\nObservable is not javascript\nd3 tutorial.\nd3 gallery"
  },
  {
    "objectID": "interactive.html#project",
    "href": "interactive.html#project",
    "title": "2  Interactive graphics",
    "section": "2.6 Project",
    "text": "2.6 Project\n\nCreate a D3 graphic web page that displays a scatterplot of your chosing. Show point information on hover.\nOn the same web page, create a D3 graphic web page that displays a stacked bar chart for the Kirby 21 data. Hover data should show subject information and increase the size of the bar. Here’s a plotly version to get a sense.\n\n\nimport pandas as pd\nimport plotly.express as px\nimport numpy as np\ndat = pd.read_csv(\"https://raw.githubusercontent.com/smart-stats/ds4bio_book/main/book/assetts/kirby21.csv\").drop(['Unnamed: 0'], axis = 1)\ndat = dat.assign(id_char = dat.id.astype(str))\nfig = px.bar(dat, x = \"id_char\", y = \"volume\", color = \"roi\")\nfig.show()\n\n                                                \n\n\n\nSubmit your webpages and all supporting code to your assignment repo"
  },
  {
    "objectID": "webscraping.html",
    "href": "webscraping.html",
    "title": "3  Advanced web scrapping",
    "section": "",
    "text": "Before you start webscraping make sure to consider what you’re doing. Does your scraping violate TOS? Will it inconvenience the site, other users? Per Uncle Ben: WGPCGR.\nAlso, before you begin web scraping, look for a download data option or existing solution. Probably someone has run up against the same problem and worked it out. For example, we’re going to scrape some wikipedia tables, which there’s a million other solutions for, including a wikipedia api."
  },
  {
    "objectID": "webscraping.html#basic-web-scraping",
    "href": "webscraping.html#basic-web-scraping",
    "title": "3  Advanced web scrapping",
    "section": "3.2 Basic web scraping",
    "text": "3.2 Basic web scraping\nLet’s show an example of static page parsing. Consider scraping the table of top 10 heat waves from wikipedia. First, we open the url, then parse it using BeautifulSoup, then load it into a pandas dataframe.\n\nfrom urllib.request import urlopen\nfrom bs4 import BeautifulSoup as bs\nimport pandas as pd\nurl = \"https://en.wikipedia.org/wiki/List_of_natural_disasters_by_death_toll\"\nhtml = urlopen(url)\nparsed = bs(html, 'html.parser').findAll(\"table\")\npd.read_html(str(parsed))[11]\n\n\n\n\n  \n    \n      \n      Rank\n      Death toll\n      Event\n      Location\n      Date\n    \n  \n  \n    \n      0\n      1.0\n      72000\n      2003 European heat wave\n      Europe\n      2003\n    \n    \n      1\n      2.0\n      56000\n      2010 Russian heat wave\n      Russia\n      2010\n    \n    \n      2\n      3.0\n      41,072[40]\n      1911 French heat wave\n      France\n      1911\n    \n    \n      3\n      4.0\n      12125\n      2022 European heat waves\n      Europe\n      2022\n    \n    \n      4\n      5.0\n      9500\n      1901 eastern United States heat wave\n      United States\n      1901\n    \n    \n      5\n      6.0\n      5,000–10,000\n      1988–1990 North American drought\n      United States\n      1988\n    \n    \n      6\n      7.0\n      3951\n      2019 European heat waves\n      Europe\n      2019\n    \n    \n      7\n      8.0\n      3,418[41]\n      2006 European heat wave\n      Europe\n      2006\n    \n    \n      8\n      9.0\n      2,541[41]\n      1998 Indian heat wave\n      India\n      1998\n    \n    \n      9\n      10.0\n      2500\n      2015 Indian heat wave\n      India\n      2015\n    \n  \n\n\n\n\nThe workflow as as follows:\n\nWe used the developer console on the webpage to inspect the page and its properties.\nWe opened the url with urlopen\nWe parsed the webpage with BeautifulSoup then used the method findAll on that to search for every table\nPandas has a utility that converts a html tables into a dataframe. In this case it creates a list of tables, where the 12th one is the heatwaves. Note it needs the data to be converted to a string before proceeding.\n\nThis variation of web scraping couldn’t be easier. However, what if the content we’re interested in only exists after interacting with the page? Then we need a more sophisticated solution."
  },
  {
    "objectID": "webscraping.html#form-filling",
    "href": "webscraping.html#form-filling",
    "title": "3  Advanced web scrapping",
    "section": "3.3 Form filling",
    "text": "3.3 Form filling\nWeb scraping can require posting to forms, such as logins. This can be done directly with python / R without elaborate programming, for example using the requests library. However, make sure you aren’t violating a web site’s TOS and also make sure you’re not posting your password to github as you commit scraping code. In general, don’t create a security hole for your account by web scraping it. Again, also check to make sure that the site doesn’t have an API with an authentication solution already before writing the code to post authentication. Many websites that want you to programmatically grab the data build an API."
  },
  {
    "objectID": "webscraping.html#programmatically-web-browsing",
    "href": "webscraping.html#programmatically-web-browsing",
    "title": "3  Advanced web scrapping",
    "section": "3.4 Programmatically web browsing",
    "text": "3.4 Programmatically web browsing\nSome web scraping requires us to interact with the webpage. This requires a much more advanced solution where we programmatically use a web browser to interact with the page. I’m using selenium and chromedriver. To do this, I had to download chromedriver and set it so that it was in my unix PATH.\n\nfrom selenium import webdriver\ndriver = webdriver.Chrome()\ndriver.quit()\n\nIf all went well, a chrome window appeared then closed. That’s the browser we’re going to program. If you look closely at the browser before you close it, there’s a banner up to that says “Chrome is being controlled by automated test software.” Let’s go through the example on the selenium docs here. First let’s vist a few pages. We’ll go to my totally awesome web page that I meticulously maintain every day then duckduckgo. We’ll wait a few seconds in between. My site is created and hosted by google sites, which seems reasonable that they would store a cookie so that I can log in and edit my site (which I almost never do). Duckduckgo is a privacy browser, so let’s check to see if they create a cookie. (Hint, I noticed that selenium doesn’t like redirects, so use the actual page url.)\n\ndriver = webdriver.Chrome()\ndriver.get(\"https://sites.google.com/view/bcaffo/home\")\nprint(driver.get_cookies())\ndriver.implicitly_wait(5)\n## Let's get rid of all cookies before we visit duckduckgo\ndriver.delete_all_cookies()\ndriver.get(\"https://duckduckgo.com/\")\nprint(driver.get_cookies())\n\nFor me, at least, this prints out the cookie info for my google site then nothing for ddg. (I’m not evaluating the code in quarto since I don’t want to bring up the browser when I compile the document.)\nNow let’s find the page elements that we’d like to interact with. There’s a text box that we want to submit a search command into and a button that we’ll need to press. When I go to ddg and press CTRL-I I find that the search box is:\n<input id=\"search_form_input_homepage\" class=\"js-search-input search__input--adv\" type=\"text\" autocomplete=\"off\" name=\"q\" tabindex=\"1\" value=\"\" autocapitalize=\"off\" autocorrect=\"off\" placeholder=\"Search the web without being tracked\">\nNotice, the name=\"q\" html name for the search form. When I dig around and find the submit button, it’s code is:\n<input id=\"search_button_homepage\" class=\"search__button  js-search-button\" type=\"submit\" tabindex=\"2\" value=\"S\">\nNotice its id is search_button_homepage. Let’s find these elements.\n\nsearch_box = driver.find_element(by=By.NAME, value=\"q\")\nsearch_button = driver.find_element(by=By.ID, value=\"search_button_homepage\")\n\nNow let’s send the info and press submit\n\nsearch_box.send_keys(\"Selenium\")\nsearch_button.click()\ndriver.implicitly_wait(10)\ndriver.save_screenshot(\"assets/images/webscraping.png\")\npage_source = driver.page_source\ndriver.close()\n\nHere, we saved the page_source as a variable that then can be parsed with other html parses (like bs4). Play around with the methods associated with driver and navigate the web. You’ll see that selenium is pretty incredible. Here’s the screenshot that we took:\n\n\n\nScreenshot of webscraping"
  },
  {
    "objectID": "databases.html",
    "href": "databases.html",
    "title": "4  Databases",
    "section": "",
    "text": "You’ve probably already learned about some variation of databases, either sql, nosql, spark, a cloud db, … Often, the backend of these databases can be quite complicated, while the front end requires SQL querries or something similar. We’ll look at a non-relational database format that is specifically useful for scientific computing called hdf5. HDF5 has implementations in many languages, but we’ll look at python. This is a hierarchical data format specifically useful for large array calculations.\nLet’s create a basic h5py file. First, let’s load our stuff.\nNow, let’s create an empty hdf5 file. Here’s the basic code; the option w is open for writing. There’s also w-, r, r+, a for write protected, read only, read/write, read/write and create. The first time I ran it I used:\nThen, subsequently\nNow let’s populate it with some data. The hdf5 file works almost like a directory where we can store hierarchical data. For example, suppose that we want sensors stored in a superstructure called sensors and want to fill in the data for sensor1 and sensor1.\nNow we can do normal np stuff on this sensor. However, hdf5 is only bringing in the part that we are using into memory. This allows us to work with very large files. Also, as we show here, you can name the data to a variable since that’s more convenient."
  },
  {
    "objectID": "databases.html#blockwise-basic-statistical-calculations",
    "href": "databases.html#blockwise-basic-statistical-calculations",
    "title": "4  Databases",
    "section": "4.1 Blockwise basic statistical calculations",
    "text": "4.1 Blockwise basic statistical calculations\nNow, consider taking the mean of both variables. Imagine that the time series is so long it’s not feasible to load into memory. So, we want to read it in blocks. You want your blocks to be as big as possible, since that’s fastest. In our case, of course, none of this is necessary.\nOur goal in this section is to do the following: calculate the empirical mean and variance for each sensor, center and scale each sensor, and write those changes to those variables, calculate the sample correlation then calculate the residual for sensor1 given sensor2. (I think typically you wouldn’t want to overwrite the original data; but, this is for pedagogical purposes.) We want our data organized so sensors are stored in a hierarchical “folder” called sensors and processed data is in a different folder.\nWe’re just simulating iid standard normals. So, we have a rough idea of the answers we should get, since the the data are theoretically mean 0, variance 1 and uncorrelated. After our calculations, they will have empirical mean 0 and variance 1 and the empirical correlation between the residual and sensor 2 will be 0.\nLet’s consider a block variation of the inner product. \\[\n<a, b> = \\sum_{i=0}^{n-1} a_i b_i = \\sum_{i=0}^{n/B} \\sum_{j=0}^{B-1} a_{j + i B} b_{j + i B}\n\\] (if \\(n\\) is divisible by \\(B\\). Otherwise you have to figure out what to do with the final block, which isn’t hard but makes the notation messier.) So, for example, the (sample) mean is then \\(<x, J>/n\\) where \\(J\\) is a vector of ones.\nLet’s calculate the mean using blockwise calculations.\n\nn = s1.shape[0]\nB = 32\n## mean center the blocks\nmean1 = 0\nmean2 = 0\nfor i in range(int(n/B)):\n    block_indices = np.array(range(B)) + i * B\n    mean1 += s1[block_indices].sum() / n \n    mean2 += s2[block_indices].sum() / n\n\n[mean1, mean2]\n\n[-0.015711642478409196, 0.028214120737558506]\n\n\nLet’s now center our time series.\n\nfor i in range(int(n/B)):\n    block_indices = np.array(range(B)) + i * B\n    s1[block_indices] -= mean1  \n    s2[block_indices] -= mean2\n\nNow the (unbiased, sample) variance of centered vector \\(a\\) is simply \\(<a, a>/(n-1)\\).\n\nv1, v2 = 0, 0\nfor i in range(int(n/B)):\n    block_indices = np.array(range(B)) + i * B\n    v1 += np.sum(s1[block_indices] ** 2) / (n - 1)\n    v2 += np.sum(s2[block_indices] ** 2) / (n - 1)\n[v1, v2]\n\n[0.9810864288923165, 1.081352963715044]\n\n\nNow let’s scale our vectors as\n\nsd1 = np.sqrt(v1)\nsd2 = np.sqrt(v2)\nfor i in range(int(n/B)):\n    block_indices = np.array(range(B)) + i * B\n    s1[block_indices] /= v1  \n    s2[block_indices] /= v2\n\nNow that our vectors are centered and scaled, the empirical correlation is simply \\(<a, b>/(n-1)\\). Let’s do that\n\ncor = 0\nfor i in range(int(n/B)):\n    block_indices = np.array(range(B)) + i * B\n    cor += np.sum(s1[block_indices] * s2[block_indices]) / (n-1) \ncor\n\n-0.07816937260162463\n\n\nFinally, we want to “regress out” s2 from s1. Since we normalized our series, the correlation is slope coefficient from linear regression (regardless of the outcome and dependent variable) and the intercept is zero (since we centered). Thus, the residual we want is \\(e_{12} = s_1 - \\rho s_2\\) where \\(\\rho\\) is the correlation.\n\nf['processed/resid_s1_s2'] = np.empty(n)\ne12 = f['processed/resid_s1_s2']\nfor i in range(int(n/B)):\n    block_indices = np.array(range(B)) + i * B\n    e12[block_indices] += s1[block_indices] - cor * s2[block_indices] \n\nNow we have our new processed data stored in a vector. To close our database simply do:\n\nf.close()\n\nNow our processed data is stored on disk.\n\nf = h5py.File('sensor.hdf5', 'r')\nf['processed/resid_s1_s2']\n\n<HDF5 dataset \"resid_s1_s2\": shape (1024,), type \"<f8\">\n\n\n\nf.close()"
  },
  {
    "objectID": "data_analysis_theory.html",
    "href": "data_analysis_theory.html",
    "title": "9  Data science, conceptually",
    "section": "",
    "text": "In this chapter, we’ll focus on data science theory, thinking and philosophy. We’re going to omit any standard treatment of statistical theory and inference, like maximum likelihood optimality and asymptotics, since those are well covered elsewhere. However, it’s worth mentioning that those topics are obviously part of data science theory.\nInstead, we’ll focus on meta-inferential and meta-empirical science questions, as well as some of the conceptual and theoretical language that’s worth knowing."
  },
  {
    "objectID": "data_analysis_theory.html#all-models-are-wrong",
    "href": "data_analysis_theory.html#all-models-are-wrong",
    "title": "9  Data science, conceptually",
    "section": "9.2 All models are wrong",
    "text": "9.2 All models are wrong\n\n9.2.1 Wrong means wrong\n“All models are wrong, but some are useful”, or some variant, is a quote from statistician George Box that is so well known and used that it has a lengthy wikipedia page. Restricting our attention to probabilistic models, it is interesting to note that this quote, which is near universally agreed upon, has implications that are often not. For example, the quote suggests that there is not, and never has been: an IID sample, normally distributed data (as in having been generated from a normal distribution), a true population stochastic model … In other words, there is no correct probabilistic model, ever, ever, ever (according to the quote).\nOne way to interpret this is that there are correct probability models, we just haven’t found them yet, or maybe can’t find them via some incompleteness law. If we ever find one, I guess we’d have to change the quote to “All models were wrong …”. But, I don’t think the quote is implying the existence of true probabilistic models that we don’t, or can’t, know. I tend to think it is suggesting that, by in large, randomness doesn’t exist and hence probabilitity models are, like Newtonian mechanics, just models, not truth.\nThis is a well discussed topic in philosophy. On some meaningful level, the quote is obviously true. Most things we’re interested in are clearly purely functionally caused by antecedent variables, some of which we know and can measure and some of which we can’t. This is obviously true of of things like die rolls or casino games and (especially) random number generators, where we know the actual deterministic formula. [REFERENCES].\nBut does ranomness exist for some weird quantum setting, or is it just a useful model? The best answer for this question came from an experiment in quantum physics …"
  },
  {
    "objectID": "data_analysis_theory.html#some-models-are-useful",
    "href": "data_analysis_theory.html#some-models-are-useful",
    "title": "9  Data science, conceptually",
    "section": "9.3 Some models are useful",
    "text": "9.3 Some models are useful\nDo we even care if models are ultimately correct? The quote ends with, “some models are useful”. How are they useful?"
  },
  {
    "objectID": "data_science_as_a_science.html",
    "href": "data_science_as_a_science.html",
    "title": "10  Data science as an applied science",
    "section": "",
    "text": "The term “Data science” is typically used to refer to a set of tools, techniques and thought processes used to perform scientific inquiries using data. But is that a scientific topic worthy of study in its own right? It is. And, from a practical, theoretical and philosophical level, it’s already extremely well studied in statistics, computer science, engineering and philosophy departments.\nDespite these fields, at the Data Science Lab at JHU, we’ve had lengthy discussions about data science as an inductive, empirical applied science, like biology or medicine, rather than an deductive discipline, like mathematics or statistical theory, or rather than a set of heuristics, like rules of thumb and agreed upon best practices. An inductive, empirical, applied science itself needs data science, since it’s empirical and thus depends on data. So, maybe there’s an infinite regress of some sort making this an ultimately doomed endeavor. But, nevertheless, we persist.\nThere are some fields of data analysis that are well covered, let’s talk about them first.\n\n\nGeneral EDA and visualization is covered in the next chapter. There is a vast literature of experiments to understand perception of visual data. This is a more neatly circumscribed area of data science as a science. This is possibly because the general field of visual perception, from a variety of angles, is well developed. Let’s cover a specific example, observer studies in radiology.\n\n\nIn diagnostic radiology, new technology in the form of new machines for imaging or new ways of processing images, are being invented constantly. To evaluate these new technologies, a gold standard is to have randomized studies where the underlying truth is known. The images from the new technology and images from a control technology are then randomized to observers. Several issues come about in observer studies. First, establishing truth can be hard. This is often done by digital or physical phantoms or in instances where patients are longitudinally followed up and the disease status is made apparent. A second issue is in the cost associated with observers. Often, instead of attending radiologists, the studies are done with residents or fellows, or students who have received specific training to qualify as reasonable proxies. An interesting alternative approach is to have digital observers."
  },
  {
    "objectID": "graphics.html",
    "href": "graphics.html",
    "title": "11  Theory of graphical display",
    "section": "",
    "text": "One of the main design arguments for the graphical display of information is data / ink maximization Tufte (1990). This is the idea that idea that as much of the “ink” (non-background pixels) of the plot as possible should be displaying data.\nData/ink maximalization has been criticized empirically. For example, Inbar, Tractinsky, and Meyer (2007) conducted a study with 87 undergraduates and found a clear preference for the non-maximized variations. Another line of argument discusses the “paradox of simplicity” Norman (2007), Eytam, Tractinsky, and Lowengart (2017), whereby we have a strong aesthetic preference for simplicity, but also want flexibility and maximum utility.\n\nBertin (1983)\n\n\n\n\nWickham et al. (2010)\n\n\n\n\n\nCleveland (1987)\nCleveland and McGill (1984)\nCleveland and Devlin (1980)\nCarswell (1992)\nCleveland and McGill (1986)\nMagical thinking Diaconis (2006)"
  },
  {
    "objectID": "graphics.html#implementation",
    "href": "graphics.html#implementation",
    "title": "11  Theory of graphical display",
    "section": "11.2 Implementation",
    "text": "11.2 Implementation\n\n11.2.1 Grammar of graphics\n\nWilkinson (2012)\nWilkinson (2013)\nWickham (2010)\n\n\n\n11.2.2 Narative storytelling\nEdward and Jeffrey (Segel and Heer (2010)) argue regarding the use of modern interactive tools in data narrative storytelling. They give seven canonical genres of narrative visulation."
  },
  {
    "objectID": "graphics.html#historical-graphics",
    "href": "graphics.html#historical-graphics",
    "title": "11  Theory of graphical display",
    "section": "11.3 Historical graphics",
    "text": "11.3 Historical graphics"
  },
  {
    "objectID": "graphics.html#graph-galleries",
    "href": "graphics.html#graph-galleries",
    "title": "11  Theory of graphical display",
    "section": "11.4 Graph galleries",
    "text": "11.4 Graph galleries\n\n\n\n\nBertin, Jacques. 1983. Semiology of Graphics. University of Wisconsin press.\n\n\nCarswell, C Melody. 1992. “Choosing Specifiers: An Evaluation of the Basic Tasks Model of Graphical Perception.” Human Factors 34 (5): 535–54.\n\n\nCleveland, William S. 1987. “Research in Statistical Graphics.” Journal of the American Statistical Association 82 (398): 419–23.\n\n\nCleveland, William S, and Susan J Devlin. 1980. “Calendar Effects in Monthly Time Series: Detection by Spectrum Analysis and Graphical Methods.” Journal of the American Statistical Association 75 (371): 487–96.\n\n\nCleveland, William S, and Robert McGill. 1984. “The Many Faces of a Scatterplot.” Journal of the American Statistical Association 79 (388): 807–22.\n\n\n———. 1986. “An Experiment in Graphical Perception.” International Journal of Man-Machine Studies 25 (5): 491–500.\n\n\nDiaconis, Persi. 2006. “Theories of Data Analysis: From Magical Thinking Through Classical Statistics.” Exploring Data Tables, Trends, and Shapes, 1–36.\n\n\nEytam, Eleanor, Noam Tractinsky, and Oded Lowengart. 2017. “The Paradox of Simplicity: Effects of Role on the Preference and Choice of Product Visual Simplicity Level.” International Journal of Human-Computer Studies 105: 43–55.\n\n\nInbar, Ohad, Noam Tractinsky, and Joachim Meyer. 2007. “Minimalism in Information Visualization: Attitudes Towards Maximizing the Data-Ink Ratio.” In Proceedings of the 14th European Conference on Cognitive Ergonomics: Invent! Explore!, 185–88.\n\n\nNorman, Donald A. 2007. “Simplicity Is Highly Overrated.” Interactions 14 (2): 40–41.\n\n\nSegel, Edward, and Jeffrey Heer. 2010. “Narrative Visualization: Telling Stories with Data.” IEEE Transactions on Visualization and Computer Graphics 16 (6): 1139–48.\n\n\nTufte, ER. 1990. “Data-Ink Maximization and Graphical Design.” Oikos, 130–44.\n\n\nWickham, Hadley. 2010. “A Layered Grammar of Graphics.” Journal of Computational and Graphical Statistics 19 (1): 3–28.\n\n\nWickham, Hadley, Dianne Cook, Heike Hofmann, and Andreas Buja. 2010. “Graphical Inference for Infovis.” IEEE Transactions on Visualization and Computer Graphics 16 (6): 973–79.\n\n\nWilkinson, Leland. 2012. “The Grammar of Graphics.” In Handbook of Computational Statistics, 375–414. Springer.\n\n\n———. 2013. The Grammar of Graphics. Springer Science & Business Media."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Bertin, Jacques. 1983. Semiology of Graphics. University of\nWisconsin press.\n\n\nCarswell, C Melody. 1992. “Choosing Specifiers: An Evaluation of\nthe Basic Tasks Model of Graphical Perception.” Human\nFactors 34 (5): 535–54.\n\n\nCleveland, William S. 1987. “Research in Statistical\nGraphics.” Journal of the American Statistical\nAssociation 82 (398): 419–23.\n\n\nCleveland, William S, and Susan J Devlin. 1980. “Calendar Effects\nin Monthly Time Series: Detection by Spectrum Analysis and Graphical\nMethods.” Journal of the American Statistical\nAssociation 75 (371): 487–96.\n\n\nCleveland, William S, and Robert McGill. 1984. “The Many Faces of\na Scatterplot.” Journal of the American Statistical\nAssociation 79 (388): 807–22.\n\n\n———. 1986. “An Experiment in Graphical Perception.”\nInternational Journal of Man-Machine Studies 25 (5): 491–500.\n\n\nDiaconis, Persi. 2006. “Theories of Data Analysis: From Magical\nThinking Through Classical Statistics.” Exploring Data\nTables, Trends, and Shapes, 1–36.\n\n\nEytam, Eleanor, Noam Tractinsky, and Oded Lowengart. 2017. “The\nParadox of Simplicity: Effects of Role on the Preference and Choice of\nProduct Visual Simplicity Level.” International Journal of\nHuman-Computer Studies 105: 43–55.\n\n\nInbar, Ohad, Noam Tractinsky, and Joachim Meyer. 2007. “Minimalism\nin Information Visualization: Attitudes Towards Maximizing the Data-Ink\nRatio.” In Proceedings of the 14th European Conference on\nCognitive Ergonomics: Invent! Explore!, 185–88.\n\n\nNorman, Donald A. 2007. “Simplicity Is Highly Overrated.”\nInteractions 14 (2): 40–41.\n\n\nSegel, Edward, and Jeffrey Heer. 2010. “Narrative Visualization:\nTelling Stories with Data.” IEEE Transactions on\nVisualization and Computer Graphics 16 (6): 1139–48.\n\n\nTufte, ER. 1990. “Data-Ink Maximization and Graphical\nDesign.” Oikos, 130–44.\n\n\nWickham, Hadley. 2010. “A Layered Grammar of Graphics.”\nJournal of Computational and Graphical Statistics 19 (1): 3–28.\n\n\nWickham, Hadley, Dianne Cook, Heike Hofmann, and Andreas Buja. 2010.\n“Graphical Inference for Infovis.” IEEE Transactions on\nVisualization and Computer Graphics 16 (6): 973–79.\n\n\nWilkinson, Leland. 2012. “The Grammar of Graphics.” In\nHandbook of Computational Statistics, 375–414. Springer.\n\n\n———. 2013. The Grammar of Graphics. Springer Science &\nBusiness Media."
  }
]