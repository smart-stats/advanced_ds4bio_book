[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Advanced Data Science for Public Health",
    "section": "",
    "text": "This is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book for the Advanced Data Science for Bio/Public Health/medical classes. Since data science isn’t super well defined, advanced data science is even less so. My opinion is that we needed the umbrella term data science because there was a lot in the processes of analyzing data that got ignored in traditional disciplines and training programs. (Of course, what gets ignored is different depeneding on which discipline or program.) I’m mostly going to focus on concepts and implementation that were historically ignored in our (JHU Biostat) program, which is heavily focused on biostatistical inference, probability modeling, public health/bio/medical data analyses and ML.\n\n\nI’m going to assume that you have a lot of basic data science tools down already. If not, here’s some notes. For this book you’ll need: prior programming experience, calculus, linear algebra, unix, python, R and some basic statistics.\n\n\n\nThis is a two quarter course. The first quater is devoted to tools and the second is deveoted to theory. So the book is divided in half that way."
  },
  {
    "objectID": "interactive.html",
    "href": "interactive.html",
    "title": "2  Interactive graphics",
    "section": "",
    "text": "In your other DS courses, you’ve learned how to create static graphics uses R, ggplot, matplotlib, seaborn … You’ve probably also learned how to create client side interactive graphics using libraries like plotly and maybe also learned client-server interactivity with shiny, dash …\nIn this section we’re going to dig deeper into client side graphics, which are almost always done via html, css, javascript and a javascript plotting library. We’re going to focus on d3.js, a well known javascript library for creating interactive data visulalizations.\nTools like d3 are mostly for creating professional data web graphics. So, most of our daily graphics use will just use python/R/julia/matlab … or plotting libraries like plotly. Usually, you want to prototype graphics outside of d3. Here, we’ll give you a smidge of using d3 to get you started if your goal is to become a graphics expert."
  },
  {
    "objectID": "interactive.html#introduction-to-d3",
    "href": "interactive.html#introduction-to-d3",
    "title": "2  Interactive graphics",
    "section": "2.1 Introduction to D3",
    "text": "2.1 Introduction to D3\nLet’s get started. I’m going to assume that you have a basic knowledge of html, css and a little bit of javascript. D3 works by manipulating html elements. Let’s select every paragraph element in a document.\n<!DOCTYPE html>\n<html lang=\"en\">\n\n<head>\n    <script src=\"https://d3js.org/d3.v5.min.js\"></script>\n</head>\n\n<body>\n    <p> Advanced </p>\n    <p> Data science </p> \n        <script>\n            let pselect = d3.selectAll(\"p\")\n            //let pselect = d3.select(\"p\").style(\"color\", \"green\");\n            //let pselect = d3.selectAll(\"p\").style(\"color\", \"green\");\n        </script>\n    </body>\n</html>\nGoing forward, we’ll omit most of the html commands.\n\nThe command <script src=\"https://d3js.org/d3.v5.min.js\"></script> loads d3 from a CDN. You could also download it locally if you’d like.\nThe script let pselect = d3.selectAll(\"p\").style(\"color\", \"green\"); creates a variable pselect that is all of the html paragraph elements\nTry doing this, loading the web page, then try uncommenting each other script line in turn and refreshing\nIn chrome do Ctrl-shift-i to get the developer console and inspect the variable pselect.\nNesting select or selectAll will select elements within the selected elements.\nYou can also select by id or class."
  },
  {
    "objectID": "interactive.html#a-simple-example",
    "href": "interactive.html#a-simple-example",
    "title": "2  Interactive graphics",
    "section": "2.2 A simple example",
    "text": "2.2 A simple example\nLet’s go through an example where we plot brain volumetric ROI data on the log scale using D3.\n<style>\n    .bar {\n        background: #f5b634;\n        border: 4px solid #0769ad;\n        height: 20px;\n    }\n</style>\n<body>\n        <script>\n            let roiData = [\n                {\"roi\": \"Telencephalon_L\", \"volume\" : 531111},\n                {\"roi\": \"Telencephalon_R\", \"volume\" : 543404},\n                {\"roi\": \"Diencephalon_L\",  \"volume\" : 9683  },\n                {\"roi\": \"Diencephalon_R\",  \"volume\" : 9678  },\n                {\"roi\": \"Mesencephalon\",   \"volume\" : 10268 },\n                {\"roi\": \"Metencephalon\",   \"volume\" : 159402},\n                {\"roi\": \"Myelencephalon\",  \"volume\" : 4973  },\n                {\"roi\": \"CSF\",             \"volume\" : 109776}\n            ];\n    \n            let divSelection = d3.select(\"body\") \n                    .selectAll(\"div\")\n                    .data(roiData)\n                    .enter()\n                    .append('div')\n                    .attr(\"class\", \"bar\")\n                    .style(\"width\", (d) => {return Math.log(d.volume) * 20 + \"px\"; })\n                    .text(d => d.roi)\n                    .on(\"mouseover\", function(){\n                        d3.select(this)\n                        .style(\"background-color\", \"orange\");\n                    })\n                    .on(\"mouseout\", function(){\n                        d3.select(this)\n                        .style(\"background-color\",\"#33A2FF\" )\n                    })        </script>\n    </body>\n\nThe data(roiDat) selects our dataset\nThe enter() and append('div') commands add div elements to the html document, one per data element.\nThe attr method considers our bar stylesheet style\nThe style method changes the style so that the bars have the width of our data. The notation (d) => {return d.volume * .001 + \"px\"} is a function that selects the ROI element of the data, multiplies it by .001 then converts it to text with px at the end.\nThe text method at the end appends the text to our plot\nThe on methods say what to do when one mouses over and off the bars. You can see now that they turn orange then back. Remove the mouseout .on call and see what happens.\n\nThe output looks like this. Hover over a bar to test. (Look at the file in d3/roi1.html)"
  },
  {
    "objectID": "interactive.html#working-through-a-realistic-example",
    "href": "interactive.html#working-through-a-realistic-example",
    "title": "2  Interactive graphics",
    "section": "2.3 Working through a realistic example",
    "text": "2.3 Working through a realistic example\nUnder assets/kirby_pivot.csv is a dataset with the kirby 21 data pivoted to have regions as columns. Let’s work through a d3 example of ploting right versus left asymmetry in the telencephalon (the largest area of the brain including the cortex and central white matter).\nHere’s the scatterplot that I’ve got so far. For HW, add text labels to the point, or a tooltip that gives point information when you hover over it.\n\nThe code for the plot is in d3/roi2.html. Let’s go over some of the main parts of the d3 code here. First, we set up the graphic\nconst h = 500\nconst w = 500\n\n// create the background\nlet svg = d3.select(\"body\")\n    .append(\"svg\")\n    .attr(\"width\" , h)\n    .attr(\"height\", w);\nNext we load in the data. First, we create a function that does a little row processing for us. Honestly, it’s probably better to just do this in python/R/julia … beforehand, but it’s worth showing here. We create variables for the log ratio between the right and left hemispheres and the log of the geometric mean. We’ll use this to create a Tukey mean/difference plot of the log of the volumes.\n//create the variables we're interested in\nlet rowConverter = function(d) {\n    return {\n        id : d.id,\n        //y is going to be the log difference R-L\n        logratio : Math.log(parseFloat(d.Telencephalon_R)) - Math.log(parseFloat(d.Telencephalon_L)),\n        //x is going to be the average log \n        loggm : (Math.log(parseFloat(d.Telencephalon_L)) + Math.log(parseFloat(d.Telencephalon_R))) * .5\n    };\n    }\n\n//the location where I'm pulling the csv from\nlet dataloc = \"https://raw.githubusercontent.com/smart-stats/advanced_ds4bio_book/main/qbook/assets/kirby_pivot.csv\"\n\n//read in the data and parse the rows \nkirby_pivot = d3.csv(dataloc, rowConverter)\nModern js uses something called ‘promises’, which alllows for asynchronous evaluation. When we read in our csv file, it gets created as a promise and not an array like we need. The result is that our plotting commands need to then be called as a method from the promise object. The reason for this is so that it only uses the data when the data is actually loaded (i.e. promise fulfilled.) So, the plotting commmands for us look like this.\nkirby_pivot.then(dat => {\n    PLOTTING COMMANDS\n})\nJust a reminder that the notation d => g(d) is JS shorthand for function(d) {return g(d);} and is used heavily in d3 coding. Now let’s fill in PLOTTING COMMANDS. First, let’s fill in some utility functions. We get the range of our x and y values to help set up our axes. d3 scales map our function values to a range we want. So let’s create scale maps for x, y and color and then also set up axes using those scales. We’ll also go ahead on plot our axes so they’re on the bottom.\nmaxx = d3.max(dat, d => d.loggm)\nminx = d3.min(dat, d => d.loggm)\nmaxy = d3.max(dat, d => d.logratio)\nminy = d3.min(dat, d => d.logratio)\n\n//fudge is the boundary otherwise points get chopped off\nlet fudge = 50\n\nlet yScale = d3.scaleLinear()\n    .domain([miny, maxy])\n    .range([h-fudge, fudge])\n\nlet pointScale = d3.scaleLinear()\n    .domain([miny, maxy])\n    .range([5, 10])\n\nlet colorScale = d3.scaleLinear()\n    .domain([miny, maxy])\n    .range([0, 1])\n\n\nlet xScale = d3.scaleLinear()\n    .domain([minx, maxx])\n    .range([w-fudge, fudge]);\n\n// define the axes\nlet xaxis = d3.axisBottom().scale(xScale)\nlet yaxis = d3.axisLeft().scale(yScale)\nsvg.append(\"g\")\n    .attr(\"class\", \"axis\")\n    .attr(\"transform\", \"translate(0,\" + (h - fudge) + \")\")\n    .call(xaxis)\n\nsvg.append(\"g\")\n    .attr(\"class\", \"axis\")\n    .attr(\"transform\", \"translate(\" + fudge + \",0)\")\n    .call(yaxis)\nNow let’s create the plot. We’re going to add circles at each location, which is attributes cx and cy. Notice we use our previous defined scales to give their locations. Also, we’ll set the color and size relative to the logratio. Finally, when we mouseover a point, let’s change the radius then change it back when we mouseoff.\nsvg.selectAll(\"circle\")\n    .data(dat)\n    .enter()\n    .append(\"circle\")\n    .attr(\"cy\", d => yScale(d.logratio))\n    .attr(\"cx\", d => xScale(d.loggm))\n    .attr(\"r\",  d => pointScale(d.logratio))\n    .attr(\"fill\", d => d3.interpolateWarm(colorScale(d.logratio)))\n    .attr(\"stroke\", \"black\")\n    .on(\"mouseover\", function() {\n        d3.select(this)\n            .attr(\"r\", 30)\n        })\n    .on(\"mouseout\", function() {\n        d3.select(this)\n        .attr(\"r\",  d => pointScale(d.logratio))\n    })\nObviously, this is a lot of work for a simple scatterplot. The difference is that here you have total control over plotting and interactivity elements."
  },
  {
    "objectID": "interactive.html#observable-and-observable-plot",
    "href": "interactive.html#observable-and-observable-plot",
    "title": "2  Interactive graphics",
    "section": "2.4 Observable and Observable Plot",
    "text": "2.4 Observable and Observable Plot\nObserverable is a notebook for working with d3. It’s quite neat since mixing javascript coding in a web notebook, which itself is written in javascript, makes for an interesting setup. Typically, one would do the data preprocessing in R, python, julia … then do the advanced graphing in d3. In addition to accepting d3 as inputs, observable has a slightly higher set of utility functions called observable plot. (Quarto, which this document is in, allows for observable cells.) So, let’s read in some ROI data and plot it in observable plot. Note this is the average of the Type I Level I ROIs. Notice this is much easier than using d3 directly.\n\ndata = FileAttachment(\"assets/kirby_avg.csv\").csv();\nPlot.plot({\nmarks: [Plot.barY(data, {x: \"roi\", y: \"volume\", fill : 'roi'})],\n    x: {tickRotate: 45},\n    color: {scheme: \"spectral\"},    \n    height: 400,\n    width: 400,\n    marginBottom: 100\n\n})"
  },
  {
    "objectID": "interactive.html#links",
    "href": "interactive.html#links",
    "title": "2  Interactive graphics",
    "section": "2.5 Links",
    "text": "2.5 Links\n\nObservable is not javascript\nd3 tutorial.\nd3 gallery"
  },
  {
    "objectID": "interactive.html#homework",
    "href": "interactive.html#homework",
    "title": "2  Interactive graphics",
    "section": "2.6 Homework",
    "text": "2.6 Homework\n\nCreate a D3 graphic web page that displays a scatterplot of your chosing. Show point information on hover.\nOn the same web page, create a D3 graphic web page that displays a stacked bar chart for the Kirby 21 data. Hover data should show subject information and increase the size of the bar. Here’s a plotly version to get a sense.\n\n\nimport pandas as pd\nimport plotly.express as px\nimport numpy as np\ndat = pd.read_csv(\"https://raw.githubusercontent.com/smart-stats/ds4bio_book/main/book/assetts/kirby21.csv\").drop(['Unnamed: 0'], axis = 1)\ndat = dat.assign(id_char = dat.id.astype(str))\nfig = px.bar(dat, x = \"id_char\", y = \"volume\", color = \"roi\")\nfig.show()\n\n\n                                                \n\n\n\nSubmit your webpages and all supporting code to your assignment repo\nHere’s a hint to the HW in d3/hwHint.html"
  },
  {
    "objectID": "webscraping.html",
    "href": "webscraping.html",
    "title": "3  Advanced web scrapping",
    "section": "",
    "text": "Before you start webscraping make sure to consider what you’re doing. Does your scraping violate TOS? Will it inconvenience the site, other users? Per Uncle Ben: WGPCGR.\nAlso, before you begin web scraping, look for a download data option or existing solution. Probably someone has run up against the same problem and worked it out. For example, we’re going to scrape some wikipedia tables, which there’s a million other solutions for, including a wikipedia api."
  },
  {
    "objectID": "webscraping.html#basic-web-scraping",
    "href": "webscraping.html#basic-web-scraping",
    "title": "3  Advanced web scrapping",
    "section": "3.2 Basic web scraping",
    "text": "3.2 Basic web scraping\nLet’s show an example of static page parsing. Consider scraping the table of top 10 heat waves from wikipedia. First, we open the url, then parse it using BeautifulSoup, then load it into a pandas dataframe.\n\nfrom urllib.request import urlopen\nfrom bs4 import BeautifulSoup as bs\nimport pandas as pd\nurl = \"https://en.wikipedia.org/wiki/List_of_natural_disasters_by_death_toll\"\nhtml = urlopen(url)\nparsed = bs(html, 'html.parser').findAll(\"table\")\npd.read_html(str(parsed))[11]\n\n\n\n\n\n  \n    \n      \n      Rank\n      Death toll\n      Event\n      Location\n      Date\n    \n  \n  \n    \n      0\n      1.0\n      72000\n      2003 European heat wave\n      Europe\n      2003\n    \n    \n      1\n      2.0\n      56000\n      2010 Russian heat wave\n      Russia\n      2010\n    \n    \n      2\n      3.0\n      41,072[41]\n      1911 French heat wave\n      France\n      1911\n    \n    \n      3\n      4.0\n      24789\n      2022 European heat waves\n      Europe\n      2022\n    \n    \n      4\n      5.0\n      9500\n      1901 eastern United States heat wave\n      United States\n      1901\n    \n    \n      5\n      6.0\n      5,000–10,000\n      1988–1990 North American drought\n      United States\n      1988\n    \n    \n      6\n      7.0\n      3951\n      2019 European heat waves\n      Europe\n      2019\n    \n    \n      7\n      8.0\n      3,418[42]\n      2006 European heat wave\n      Europe\n      2006\n    \n    \n      8\n      9.0\n      2,541[42]\n      1998 Indian heat wave\n      India\n      1998\n    \n    \n      9\n      10.0\n      2500\n      2015 Indian heat wave\n      India\n      2015\n    \n  \n\n\n\n\nThe workflow as as follows:\n\nWe used the developer console on the webpage to inspect the page and its properties.\nWe opened the url with urlopen\nWe parsed the webpage with BeautifulSoup then used the method findAll on that to search for every table\nPandas has a utility that converts a html tables into a dataframe. In this case it creates a list of tables, where the 12th one is the heatwaves. Note it needs the data to be converted to a string before proceeding.\n\nThis variation of web scraping couldn’t be easier. However, what if the content we’re interested in only exists after interacting with the page? Then we need a more sophisticated solution."
  },
  {
    "objectID": "webscraping.html#form-filling",
    "href": "webscraping.html#form-filling",
    "title": "3  Advanced web scrapping",
    "section": "3.3 Form filling",
    "text": "3.3 Form filling\nWeb scraping can require posting to forms, such as logins. This can be done directly with python / R without elaborate programming, for example using the requests library. However, make sure you aren’t violating a web site’s TOS and also make sure you’re not posting your password to github as you commit scraping code. In general, don’t create a security hole for your account by web scraping it. Again, also check to make sure that the site doesn’t have an API with an authentication solution already before writing the code to post authentication. Many websites that want you to programmatically grab the data build an API."
  },
  {
    "objectID": "webscraping.html#programmatically-web-browsing",
    "href": "webscraping.html#programmatically-web-browsing",
    "title": "3  Advanced web scrapping",
    "section": "3.4 Programmatically web browsing",
    "text": "3.4 Programmatically web browsing\nSome web scraping requires us to interact with the webpage. This requires a much more advanced solution where we programmatically use a web browser to interact with the page. I’m using selenium and chromedriver. To do this, I had to download chromedriver and set it so that it was in my unix PATH.\n\nfrom selenium import webdriver\ndriver = webdriver.Chrome()\ndriver.quit()\n\nIf all went well, a chrome window appeared then closed. That’s the browser we’re going to program. If you look closely at the browser before you close it, there’s a banner up to that says “Chrome is being controlled by automated test software.” Let’s go through the example on the selenium docs here. First let’s vist a few pages. We’ll go to my totally awesome web page that I meticulously maintain every day then duckduckgo. We’ll wait a few seconds in between. My site is created and hosted by google sites, which seems reasonable that they would store a cookie so that I can log in and edit my site (which I almost never do). Duckduckgo is a privacy browser, so let’s check to see if they create a cookie. (Hint, I noticed that selenium doesn’t like redirects, so use the actual page url.)\n\ndriver = webdriver.Chrome()\ndriver.get(\"https://sites.google.com/view/bcaffo/home\")\nprint(driver.get_cookies())\ndriver.implicitly_wait(5)\n## Let's get rid of all cookies before we visit duckduckgo\ndriver.delete_all_cookies()\ndriver.get(\"https://duckduckgo.com/\")\nprint(driver.get_cookies())\n\nFor me, at least, this prints out the cookie info for my google site then nothing for ddg. (I’m not evaluating the code in quarto since I don’t want to bring up the browser when I compile the document.)\nNow let’s find the page elements that we’d like to interact with. There’s a text box that we want to submit a search command into and a button that we’ll need to press. When I go to ddg and press CTRL-I I find that the search box is:\n<input id=\"search_form_input_homepage\" class=\"js-search-input search__input--adv\" type=\"text\" autocomplete=\"off\" name=\"q\" tabindex=\"1\" value=\"\" autocapitalize=\"off\" autocorrect=\"off\" placeholder=\"Search the web without being tracked\">\nNotice, the name=\"q\" html name for the search form. When I dig around and find the submit button, it’s code is:\n<input id=\"search_button_homepage\" class=\"search__button  js-search-button\" type=\"submit\" tabindex=\"2\" value=\"S\">\nNotice its id is search_button_homepage. Let’s find these elements.\n\nsearch_box = driver.find_element(by=By.NAME, value=\"q\")\nsearch_button = driver.find_element(by=By.ID, value=\"search_button_homepage\")\n\nNow let’s send the info and press submit\n\nsearch_box.send_keys(\"Selenium\")\nsearch_button.click()\ndriver.implicitly_wait(10)\ndriver.save_screenshot(\"assets/images/webscraping.png\")\npage_source = driver.page_source\ndriver.close()\n\nHere, we saved the page_source as a variable that then can be parsed with other html parses (like bs4). Play around with the methods associated with driver and navigate the web. You’ll see that selenium is pretty incredible. Here’s the screenshot that we took:\n\n\n\nScreenshot of webscraping"
  },
  {
    "objectID": "webscraping.html#homework",
    "href": "webscraping.html#homework",
    "title": "3  Advanced web scrapping",
    "section": "3.5 Homework",
    "text": "3.5 Homework\n\nWrite a function that takes a search term, enters it into this link and returns the number of characters from the output.\nWrite a function that solves THE MAZE and returns your current location at its solution"
  },
  {
    "objectID": "images.html",
    "href": "images.html",
    "title": "5  Working with images",
    "section": "",
    "text": "Images broadly come in two types, vector and raster. Vector graphics are in formats like pdf, eps, svg and raster graphics are like jpeg, gif, png. Vector graphics store the image constructs and shapes. So, a vector graphics renderer can zoom in indefinitely on a shape and its edges will appear sharp. Vector fonts work this way. Raster graphics basically store a matrix and the pixels on the screen show the values of that matrix. Bitmapped fonts work this way. Of course, vector graphics have to be converted to raster to be actually displayed by the computer. Finally, some rater graphics formats have compression, which we won’t really discuss."
  },
  {
    "objectID": "images.html#working-with-raster-graphics",
    "href": "images.html#working-with-raster-graphics",
    "title": "5  Working with images",
    "section": "5.2 Working with raster graphics",
    "text": "5.2 Working with raster graphics\nRaster images are typically stored as an array. Grayscale images are matrices with the image intensity as the value and color pictures are stored as 3D arrays with the two main dimensions and color channels. A library for working with regular images in python is called PIL.\nThere are different raster specifications. RGB has 3 color channels, red, green and blue. CMYK has four: cyan, magenta, yellow and black. It’s interesting to note that the use of color channels existed before color cameras, when photographers would use different filters and additive and subtractive processes. The photograph below was created in 1877 by Louis Ducos du Hauron.\n\n\n\nColor image\n\n\nReading and working with images in python is quite easy because of the Python Image Library (PIL).\n\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nimg = Image.open(\"assets/images/Duhauron1877.jpg\")\n## You can see it with this, or img.show()\nimg\n\n\n\n\nPIL objects come with a ton of methods. For example, if we want to know whether we have an RGB or CMYK image, just print its mode.\n\nprint(img.mode)\n\nRGB\n\n\n\nr, g, b = img.split()\n\nplt.figure(figsize=(10,4));\nplt.subplot(1, 3, 1);\nplt.axis('off');\nplt.imshow(r);\n\nplt.subplot(1, 3, 2);\nplt.axis('off');\nplt.imshow(g);\n\nplt.subplot(1, 3, 3);\nplt.axis('off');\nplt.imshow(b);\n\n\n\n\nIf you’re tired of working with the image as a PIL object, it’s easy to convert to a np array.\n\nimg_array = np.array(img)\nimg_array.shape\n\n(1132, 1548, 3)\n\n\nBefore we leave PIL, it should be said that most image operations can be done in it. For example, cropping.\n\nbbox = [500, 630, 700, 760]\ncropped = img.crop(bbox)\ncropped\n\n\n\n\nWe can rotate the house and put it back\n\nrot = cropped.transpose(Image.Transpose.ROTATE_180)\nrot\n\n\n\n\n\n##Note this overwrites the image\nimg.paste(rot, bbox)\nimg"
  },
  {
    "objectID": "images.html#image-mathematics",
    "href": "images.html#image-mathematics",
    "title": "5  Working with images",
    "section": "5.3 Image mathematics",
    "text": "5.3 Image mathematics\n\n5.3.1 Convolutions\n\n5.3.1.1 1D transforms\nConvolutions are an important topic in mathematics, statistics, signal processing … Let’s discuss 1D convolutions first. A real valued convolution of two continuous signals, \\(X(t)\\) and \\(K(t)\\) is defined as \\(X* K\\)\n\\[\n(X* K)(t) = \\int_{-\\infty}^{\\infty} X(u) K(t-u) du\n= \\int_{-\\infty}^{\\infty} X(t-v) K(v) dv,\n\\]\nwhere the equality is determined by a simple change of variable argument. The discrete analog is\n\\[\n(X* K)(t) = \\sum_{u = -\\infty}^{\\infty} X(u) K(t-u)\n= \\sum_{v = -\\infty}^{\\infty} X(t-v) K(v)\n\\]\nThe convolution has many, many uses in data science and statistics. For example, the convolution of densities or mass functions is the respective density or mass function for the sum of random variables from those distributions. In applied data analysis, you can think of the convolution between \\(X\\) and \\(K\\) as smearing the function \\(K\\) over the function \\(X\\). Thus, it plays a key role in smoothing. Let’s try an example using the covid data and a box kernel. We take \\(K(t) = I\\{0 \\leq t < M\\} / M\\) (i.e. is 1 for times 0 to \\(M-1\\), then rescaled so it sums to 1). Assume that \\(N\\geq M\\) and that \\(X(t)\\) and \\(K(t)\\) are \\(0\\) and for \\(t < 0\\) or \\(t > N\\). Then, our convolution works out to be\n\\[\n(X* K)(t)\n= \\sum_{u = -\\infty}^{\\infty} X(u) K(t-u)\n= \\sum_{u = 0}^{N} X(u) K(t-u)\n= \\sum_{u = t}^{t + M - 1} X(u) K(t -u)\n= \\sum_{u = t}^{t + M - 1} X(u) / M\n\\]\nThat is, our convolution is a moving average of \\(X\\) where the convolution at point \\(t\\) is the average of the points between \\(t\\) and \\(t + M - 1\\). So, the convolution, as we’ve defined it, at point \\(t\\) is the moving average at point \\(t + (M-1)/2\\) (ie. it’s shifted by \\((M-1)/2\\)). Also, at the end (\\(t \\geq N - M + 1\\)), we’re averaging in the assumed zero values of the \\(X\\). This might be reasonable to do, or maybe not. The fact that we’re padding the end and not the beginning is just because of the range of index values we defined the kernel on. We’d have the same problem only on the other end if \\(K(t) = I(-M < t \\leq 0)/M\\). Of course, the computer will start summing things at index 0 regardless. However, it can shift the kernel relative to the signal arbitrarily by zero padding one end or the other or both. A reasonable strategy is to set it so that it averages in \\((M-1)/2\\) on both ends. Numpy allows you to only look at the range of \\(N - M\\) middle values where this isn’t an issue (argument mode = \"valid\").\nNote we could make the kernel weight points differently than just a box kernel. A popular choice is a Gaussian distribution.\nAlso, the convolution has \\(N+M-1\\) points. So, it has more time points than the original signal. Numpy has options to shift the convolution back into the same space as the original signal for you (i.e. has \\(N\\) points, mode = \"same\"). Or, you can just do it yourself if you do mode = \"full\", just shift by \\((M-1)/2\\). Similarly shift for mode = \"valid\" (but the convolution has fewer points in this case, so it won’t have corresponding points with \\(X\\) at the very beginning and end).\nHere’s an example using Italy’s daily covid case count data. We plot the data and the convolution smoothed data. In the bottom panels, we show the residuals to highlight the difference.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\ndat = pd.read_csv('https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv')\n## Get Italy, drop everyrthing except dates, convert to long (unstack converts to tuple)\nX = dat[dat['Country/Region'] == 'Italy'].drop([\"Province/State\", \"Country/Region\", \"Lat\", \"Long\"], axis=1).unstack()\n## convert from tuple to array\nX = np.asarray(X)  \n## get case counts instead of cumulative counts\nX = X[1 : X.size] - X[0 : (X.size - 1)]\n## get the first non zero entry\nX =  X[np.min(np.where(X !=  0)) : X.size]\nplt.plot(X)\n\n\n\n\nNow let’s plot the convolutions with different options in np.convolve.\n\n## 41 day moving average\nN = len(X)\nM = 41\n\nfig, axes = plt.subplots(2, 3, figsize = [12.4, 12.4])\naxes[0,0].plot(X)\naxes[0,1].plot(X)\naxes[0,2].plot(X)\n\nK = np.ones(M) / M\n\n## Plot the convolution with the argument 'same'\n## this gives N (assumed greater than M) points\nXC = np.convolve(X, K, 'same')\naxes[0,0].plot(XC)\naxes[1,0].plot(X - XC)\n\n## Plot the convolution with the argument 'full'\n## which gives N+M-1 total pionts\nXC = np.convolve(X, K, 'full')\ntemp = np.pad(X, (M-1, 0), 'constant') \naxes[0,1].plot(XC)\naxes[1,1].plot(temp- XC)\n\n\n## Plot the convolution with the convolution shifted back by (M-1)/2\nXCshifted = XC[ (int((M - 1)/2)) : int(len(XC) - (M - 1)/2) ]\naxes[0, 2].plot(XCshifted)\naxes[1, 2].plot(X - XCshifted)\n## 41 day moving average\nN = len(X)\nM = 41\n\nfig, axes = plt.subplots(2, 3, figsize = [12.4, 12.4])\naxes[0,0].plot(X)\naxes[0,1].plot(X)\naxes[0,2].plot(X)\n\nK = np.ones(M) / M\n\n## Plot the convolution with the argument 'same'\n## this gives N (assumed greater than M) points\nXC = np.convolve(X, K, 'same')\naxes[0,0].plot(XC)\naxes[1,0].plot(X - XC)\n\n## Plot the convolution with the argument 'full'\n## which gives N+M-1 total pionts\nXC = np.convolve(X, K, 'full')\ntemp = np.pad(X, (M-1, 0), 'constant') \naxes[0,1].plot(XC)\naxes[1,1].plot(temp- XC)\n\n\n## Plot the convolution with the convolution shifted back by (M-1)/2\nXCshifted = XC[ (int((M - 1)/2)) : int(len(XC) - (M - 1)/2) ]\naxes[0, 2].plot(XCshifted)\naxes[1, 2].plot(X - XCshifted)\n\n\n\n\n\n\n\nLet’s show that the first point and end point of the convolution are the averages of \\((M-1)/2\\) points and and \\((M-1)/2+1\\) zeros at the beginning or end of the original signal just to show that our intuition is correct.\n\ntemp = np.convolve(X, K, 'same')\n[\n  # the first convolution point (temp[0]) and the average of the\n  # the first (M-1) / 2 X points and (M-1)/2 + 1 zeros\n  [temp[0],     X[0 : int(    (M - 1) / 2)].sum() / M],\n  # the last convolution point (temp[N-1]) and the average of the\n  # the last (M-1) / 2 X points and (M-1)/2 + 1 zeros\n  [temp[N - 1], X[int(N - (M - 1) / 2 - 1)  : N].sum() / M]\n \n]\n\n[[0.07317073170731708, 0.07317073170731707],\n [16135.414634146342, 16135.414634146342]]\n\n\nAlso, I averaged a lot (41 days) in order to make the shift very apparent. Let’s look at the performance for less wide of a kernel.\n\n## 21 day moving average\nM = 21\nK = np.ones(M) / M\n\nfig, axes = plt.subplots(1, 2, figsize = [12.4, 6.2])\nXC = np.convolve(X, K, 'same')\naxes[0].plot(X)\naxes[0].plot(XC)\naxes[1].plot(X - XC)\n\n\n\n\nIt should be stated that the convolution operation is multiplication in Fourier space. So, functions like np.convolve are performing FFTs in the background. However, if you’re going to do this yourself, make sure to keep track of indices and zero padding. (I.e. the bookkeeping.) Otherwise, the FFT wraps around and you get a little of the end averaged in with the beginning and vice versa. I work out getting the same answer as mode = “same” below.\n\nfig, axes = plt.subplots(1, 2, figsize = [12.4, 6.2])\n\n## Pad the X with zeros in the back, need at least M-1 \npad_width = (0, M - 1)\nXpadded = np.pad(X, pad_width, \"constant\")\n## Pad the kernel in the back with N-1, so both the kernel\n## and the X are of length, N+M-1\nKpadded = np.pad(K, (0, N - 1))\n\n## Note we take the real part b/c the complex part is all effectively \n## machine 0\nconvolution = np.fft.ifft(np.fft.fft(Xpadded) * np.fft.fft(Kpadded)).real\n\n## At this point the convolution is of length N + M - 1\n## To get it comparable with the original X, subtract (M-1)/2 indices\n## from each end\nconvolution = convolution[ int((M-1)/2) : int(N+(M-1)/2)]\n\n## Let's see how we did\naxes[0].plot(X)\naxes[0].plot(convolution)\n\n#Show they're the same by plotting the subtraction\naxes[1].plot(convolution - XC)\n\n\n\n\n\n\n5.3.1.2 2D transforms\nFor two dimensions, the convolution is similar\n\\[\n(X ** K)(i,j) = \\sum_{u=-\\infty}^{\\infty} \\sum_{v=-\\infty}^{\\infty}\nX(u, v)  K(i -u, k - v) = \\sum_{u=-\\infty}^{\\infty} \\sum_{v=-\\infty}^{\\infty}\nK(u, v)  X(i -u, k - v)  \n\\]\nOnce again, let’s think where \\(X\\) is of dimension \\((N_1, N_2)\\) and 0 outside of that range, and\n\\[\nK(u, v) = I(0 \\leq u < M_1, 0 \\leq v < M_2) / (M_1 M_2)\n\\]\n(i.e. \\(K\\) is a box on \\(M_1 \\leq N_1\\), \\(M_2 < N_2\\)). Then, applying the exact same argument as before, the convolution is:\n\\[\n(X ** K)(i,j) = \\sum_{u=i}^{M_1 + i - 1} \\sum_{v=j}^{M_2 + j - 1}\nX(u, v) / (M_1 M_2)\n\\]\nThat is, the convolution at point \\((i,j)\\) is the average of the neighboring points. Also, all of the same bookkeeping, zero padding and Fourier transform stuff apply (using the 2D FFT).\nFor regular kernels (box kernels, 2D Gaussians), convolution smooths the image, which has the efffect of making it blurrier. The kernel width determines how blurry the image will then be. This is typically done to denoise an image (to blur out the noise). Let’s try it on a cartoon image of Brian. We’ll just stick to a black and white image so that it’s 2D. A color image has 3 color channels, so is a 3D array. (However, you see the patten; you should be able to extend this to 3D with little problem.)\n\nimport PIL\nimport scipy.signal as sp\nimport urllib.request\n\n\nimgURL = \"https://github.com/smart-stats/ds4bio_book/raw/main/book/bcCartoon.png\"\nurllib.request.urlretrieve(imgURL, \"bcCartoon.png\")\nimg = np.asarray(PIL.Image.open(\"bcCartoon.png\").convert(\"L\"))\n\nplt.xticks([])\nplt.yticks([])\nplt.imshow(img, cmap='gray', vmin=0, vmax=255)\n\n<matplotlib.image.AxesImage at 0x7ad3271d28f0>\n\n\n\n\n\nNow let’s take this image and convolve it with different kernels of different window lengths.\n\ndef kernel(i, j):\n  return np.ones((i, j)) / np.prod([i, j])\n\nplt.figure(figsize=[12.4, 12.4])\nimgC = sp.convolve2d(img, kernel(4, 4))\nplt.subplot(2, 2, 1)\nplt.xticks([])\nplt.yticks([])\nplt.imshow(imgC, cmap='gray', vmin=0, vmax=255)\nplt.title(\"4x4\")\n\nimgC = sp.convolve2d(img, kernel(8, 8))\nplt.subplot(2, 2, 2)\nplt.xticks([])\nplt.yticks([])\nplt.imshow(imgC, cmap='gray', vmin=0, vmax=255)\nplt.title(\"8x8\")\n\nimgC = sp.convolve2d(img, kernel(16, 16))\nplt.subplot(2, 2, 3)\nplt.xticks([])\nplt.yticks([])\nplt.imshow(imgC, cmap='gray', vmin=0, vmax=255)\nplt.title(\"16x16\")\n\nboxsize = (5, 5)\nimgC = sp.convolve2d(img, kernel(32,32))\nplt.subplot(2, 2, 4)\nplt.xticks([])\nplt.yticks([])\nplt.imshow(imgC, cmap='gray', vmin=0, vmax=255)\nplt.title(\"32x32\")\n\nText(0.5, 1.0, '32x32')\n\n\n\n\n\n\n\n5.3.1.3 Convolutional neural networks\nOf course, your kernel doesn’t have to be a box, or a truncated, discretized bivariate Gaussian density or even be non-negative. It’s helpful for smoothers to have non-negative kernels, since they’re just taking a generalized variation of a moving average that way. But, we want to use convolutions\nmore generally. Here, let’s take a kernel that is part of the image (left eye) and convolve it. I’ll make the kernel super peaked at eye features by extracting the eye and raising it to the 4th power.\nSo a relu activation function plus a bias term would then be able to highlight different thresheld variations of this convolution image. For example, here I add a bias term to the convolution then apply a leaky relu. You can see it just highlights the one area where the eye is. A leaky relu is\n\\[\nlrelu(x, c) = \\left\\{\n  \\begin{array}{ll}\n  x & \\text{if $x > 0$} \\\\\n  x * c & \\text{otherwise}\n  \\end{array}\n  \\right.\n\\]\nwhere \\(c\\) is usually set to a small value. If \\(c=0\\) the leaky relu is just the relu. I set \\(c\\) to be 0.05 so that we can see the background image.\n\nplt.figure(figsize=[12.4, 6.2])\n\nK = img[200 : 270,225 : 322]\nplt.subplot(1, 3, 1)\nplt.xticks([])\nplt.yticks([])\nplt.imshow(K,  cmap='gray', vmin=0, vmax=255)\n## I normalized it this way so that the convolution\n## numbers wouldn't be so big\n## Also, I put it to the 4th power, so it exactly finds \n## the eye.\nK = K ** 4\nK = K / K.sum()\nK = K - K.mean()\n\nimgC = sp.convolve2d(img, K)\nplt.subplot(1, 3, 2)\nplt.xticks([])\nplt.yticks([])\nplt.imshow(imgC)\nplt.title(\"Convolution\")\n\ntemp = imgC.copy()\n## Add a bias term of -15\ntemp -= 15\n## Perform a leaky relu\ntemp[np.where(temp < 0)] = temp[np.where(temp < 0)] * .05\n\nplt.subplot(1, 3, 3)\nplt.imshow(temp)\nplt.xticks([])\nplt.yticks([])\nplt.title(\"LRELU of convolution + bias\")\n\nText(0.5, 1.0, 'LRELU of convolution + bias')\n\n\n\n\n\nBecause of how convolutions work, this will find this eye anywhere in the image. Here we just add another eye somewhere else and repeat the convolution.\n\nplt.figure(figsize=[12.4, 6.2])\n\n#put another eye in the image\nimgCopy = img.copy()\nimgCopy[60 : 130, 85 : 182] = img[200 : 270,225 : 322]\nplt.subplot(1, 2, 1)\nplt.imshow(imgCopy,  cmap='gray', vmin=0, vmax=255)\nplt.xticks([])\nplt.yticks([])\n\nimgC = sp.convolve2d(imgCopy, K)\n\nplt.subplot(1, 2, 2)\ntemp = imgC.copy()\n## Add a bias term of -15\ntemp -= 15\n## Perform a leaky relu\ntemp[np.where(temp < 0)] = temp[np.where(temp < 0)] * .05\n\nplt.subplot(1, 2, 2)\nplt.imshow(temp)\nplt.xticks([])\nplt.yticks([])\nplt.title(\"LRELU of convolution + bias\")\n\nText(0.5, 1.0, 'LRELU of convolution + bias')\n\n\n\n\n\nSo, we found a custom kernel that highlights this specific feature in images. Convnets layers learn the kernel. That is, CNNs learn the image that gets convolved with the previous layer to produce the next one. Here’s a really great pictorial guide by Sumit Saha.\nNow, let’s discuss some specific vocabulary used in CNNs.\n\nPadding zero padding just like we discussed for 1D transformations\nPooling pooling, often max pooling, is a dimension reduction technique, taking the max in little blocks.\nstride length instead of sliding the kernel by moving it one pixel at a time, move it more to increase computational efficiency and reduce the size of the output convolution."
  },
  {
    "objectID": "databases.html",
    "href": "databases.html",
    "title": "6  Databases",
    "section": "",
    "text": "You’ve probably already learned about some variation of databases, either sql, nosql, spark, a cloud db, … Often, the backend of these databases can be quite complicated, while the front end requires SQL querries or something similar. We’ll look at a non-relational database format that is specifically useful for scientific computing called hdf5. HDF5 has implementations in many languages, but we’ll look at python. This is a hierarchical data format specifically useful for large array calculations.\nLet’s create a basic h5py file. First, let’s load our stuff.\nNow, let’s create an empty hdf5 file. Here’s the basic code; the option w is open for writing. There’s also w-, r, r+, a for write protected, read only, read/write, read/write and create. The first time I ran it I used:\nThen, subsequently\nNow let’s populate it with some data. The hdf5 file works almost like a directory where we can store hierarchical data. For example, suppose that we want sensors stored in a superstructure called sensors and want to fill in the data for sensor1 and sensor1.\nNow we can do normal np stuff on this sensor. However, hdf5 is only bringing in the part that we are using into memory. This allows us to work with very large files. Also, as we show here, you can name the data to a variable since that’s more convenient."
  },
  {
    "objectID": "databases.html#blockwise-basic-statistical-calculations",
    "href": "databases.html#blockwise-basic-statistical-calculations",
    "title": "6  Databases",
    "section": "6.1 Blockwise basic statistical calculations",
    "text": "6.1 Blockwise basic statistical calculations\nNow, consider taking the mean of both variables. Imagine that the time series is so long it’s not feasible to load into memory. So, we want to read it in blocks. You want your blocks to be as big as possible, since that’s fastest. In our case, of course, none of this is necessary.\nOur goal in this section is to do the following: calculate the empirical mean and variance for each sensor, center and scale each sensor, and write those changes to those variables, calculate the sample correlation then calculate the residual for sensor1 given sensor2. (I think typically you wouldn’t want to overwrite the original data; but, this is for pedagogical purposes.) We want our data organized so sensors are stored in a hierarchical “folder” called sensors and processed data is in a different folder.\nWe’re just simulating iid standard normals. So, we have a rough idea of the answers we should get, since the the data are theoretically mean 0, variance 1 and uncorrelated. After our calculations, they will have empirical mean 0 and variance 1 and the empirical correlation between the residual and sensor 2 will be 0.\nLet’s consider a block variation of the inner product. \\[\n<a, b> = \\sum_{i=0}^{n-1} a_i b_i = \\sum_{i=0}^{n/B} \\sum_{j=0}^{B-1} a_{j + i B} b_{j + i B}\n\\] (if \\(n\\) is divisible by \\(B\\). Otherwise you have to figure out what to do with the final block, which isn’t hard but makes the notation messier.) So, for example, the (sample) mean is then \\(<x, J>/n\\) where \\(J\\) is a vector of ones.\nLet’s calculate the mean using blockwise calculations.\n\nn = s1.shape[0]\nB = 32\n## mean center the blocks\nmean1 = 0\nmean2 = 0\nfor i in range(int(n/B)):\n    block_indices = np.array(range(B)) + i * B\n    mean1 += s1[block_indices].sum() / n \n    mean2 += s2[block_indices].sum() / n\n\n[mean1, mean2]\n\n[0.020541009661669534, 0.017165745747150723]\n\n\nLet’s now center our time series.\n\nfor i in range(int(n/B)):\n    block_indices = np.array(range(B)) + i * B\n    s1[block_indices] -= mean1  \n    s2[block_indices] -= mean2\n\nNow the (unbiased, sample) variance of centered vector \\(a\\) is simply \\(<a, a>/(n-1)\\).\n\nv1, v2 = 0, 0\nfor i in range(int(n/B)):\n    block_indices = np.array(range(B)) + i * B\n    v1 += np.sum(s1[block_indices] ** 2) / (n - 1)\n    v2 += np.sum(s2[block_indices] ** 2) / (n - 1)\n[v1, v2]\n\n[0.9662518323554584, 0.9380961438896619]\n\n\nNow let’s scale our vectors as\n\nsd1 = np.sqrt(v1)\nsd2 = np.sqrt(v2)\nfor i in range(int(n/B)):\n    block_indices = np.array(range(B)) + i * B\n    s1[block_indices] /= v1  \n    s2[block_indices] /= v2\n\nNow that our vectors are centered and scaled, the empirical correlation is simply \\(<a, b>/(n-1)\\). Let’s do that\n\ncor = 0\nfor i in range(int(n/B)):\n    block_indices = np.array(range(B)) + i * B\n    cor += np.sum(s1[block_indices] * s2[block_indices]) / (n-1) \ncor\n\n0.0033816525098176796\n\n\nFinally, we want to “regress out” s2 from s1. Since we normalized our series, the correlation is slope coefficient from linear regression (regardless of the outcome and dependent variable) and the intercept is zero (since we centered). Thus, the residual we want is \\(e_{12} = s_1 - \\rho s_2\\) where \\(\\rho\\) is the correlation.\n\nf['processed/resid_s1_s2'] = np.empty(n)\ne12 = f['processed/resid_s1_s2']\nfor i in range(int(n/B)):\n    block_indices = np.array(range(B)) + i * B\n    e12[block_indices] += s1[block_indices] - cor * s2[block_indices] \n\nNow we have our new processed data stored in a vector. To close our database simply do:\n\nf.close()\n\nNow our processed data is stored on disk.\n\nf = h5py.File('sensor.hdf5', 'r')\nf['processed/resid_s1_s2']\n\n<HDF5 dataset \"resid_s1_s2\": shape (1024,), type \"<f8\">\n\n\n\nf.close()"
  },
  {
    "objectID": "databases.html#homework",
    "href": "databases.html#homework",
    "title": "6  Databases",
    "section": "6.2 Homework",
    "text": "6.2 Homework\n\nPerform lots of regressions. Suppose that you have a setting where you would like to perform the operation \\[\n(X'X)^{-1} X' Y\n\\] where \\(X\\) is \\(n\\times p\\) and \\(Y\\) is \\(n\\times v\\). Consider the case where \\(Y\\) is very large (so \\(V\\) is large). Simulate some data where you perform this linear model in block calculations.\nWrite a block matrix multiplication program that takes in two matrices with agreeable dimensions stored as HDF5 and multiplies them in block sizes specified by the user."
  },
  {
    "objectID": "pipelines.html",
    "href": "pipelines.html",
    "title": "7  Pipelines",
    "section": "",
    "text": "We’ll cover make here, which is less commonly used for data science, but is ubiquitously used for software development. A makefile simply says what tasks needs to be done to construct a project. Then, it only runs those tasks that need to be updated. Imagine something like the folowing. I have R code that creates a figure, that figure is required by a latex file.\nproject.pdf : rplots.pdf project.tex\n        pdflatex project.tex\n\nrplots.pdf : gen_rplots.R\n       R --no-save < gen_rplots.R\n\nclean :\n    rm rplots.pdf\n    rm project.pdf\n    rm project.log\nTyping make at the command like will make my file project.pdf. I can type make clean to remove my build files. I can also type make rplots.pdf to make just that file. Make uses timestamps to only update what is needed to be updated. So if I only change project.tex it won’t regenerate my rplots.pdf.\nWhy use something like make when we have rmarkdown, quarto …? The main reason is that make is completely general. You can have any command used in building and any type of output. This is why these kind of build utilities are so ubiquitously used in pipelining."
  },
  {
    "objectID": "data_structures.html",
    "href": "data_structures.html",
    "title": "8  Data structures",
    "section": "",
    "text": "The topic of data structures focuses on how we represent, access and store data, ideally in ways efficient for our purpose. This chapter is a low level teaser of data structures geared towards students who haven’t been exposed to the ideas much at all."
  },
  {
    "objectID": "data_structures.html#hash-tables",
    "href": "data_structures.html#hash-tables",
    "title": "8  Data structures",
    "section": "8.2 Hash tables",
    "text": "8.2 Hash tables\nA common starting point for data structures is a hash table. Suppose we want to store a set of values associated with a set of keys. Consider a list of some students and their PhD theses.\n\n\n\n\n\n\n\nkey\nvalue\n\n\n\n\nLeena\nModelling biomedical data and the foundations of bioequivalence\n\n\nXianbin\nModeling composite outcomes and their component parts\n\n\nShu-Chih\nStructure/function relationships in the analysis of anatomical and functional neuroimaging data\n\n\nHaley\nStatistical methods for inter-subject analysis of neuroscience data\n\n\nBruce\nFrom individuals to populations: application and insights concerning the generalized linear mixed model\n\n\n\nIf we stored these in an text array, say, and we wanted to look up Bruce’s thesis title, we’d have to check each key in turn until we arived at Bruce and then looked up his thesis. This has a worst case scenario of n operations. (Question, if we looked in the order of a random permutation, what is the expected number of lookups?)\nHash tables map the keys to a specific lookup number. Thus, when trying to find Bruce’s value, the has function would perform hash(\"Bruce\") to get the hash value, then to straight to that point in the array. Sounds great!\nThere are some details, of course. Most (all) programming languages have hash tables, or libraries that add on hash tables. For example, the dict structure in python. Since that exists, let’s work in R and create our own hash table.\nWe need a hash function. Let’s create one as the sum of its utf8 values\n\nhash = function(string, mod) sum(utf8ToInt(string)) %% mod\nhash(\"Bruce\", 5)\n\n[1] 2\n\nhash(\"Bruce2\", 5)\n\n[1] 2\n\nhash(\"Haley\", 5)\n\n[1] 4\n\n\nHere the mod is used to truncate our integer value so that our it fits in our list. In our case, let’s assume the list is of length no larger than 5. Ideally, you want you hash functions to have as few collisions, instances where different key texts give the same hash value, as possible. For our simple example, we’re not going to stress out over this much and our hash function is going to give lots of collisions. Let’s create an empty hash table\n\nhash_table = vector(mode = \"list\", length = 5)\n\nNow, let’s add an element\n\n##Note this operates on hash_table outside of the function\nadd_pair = function(key, value, hash_table){\n    n = length(hash_table)\n    new_entry = list(c(key, value))\n    hash_value = hash(key, n)\n    hash_entry = hash_table[[hash_value]]\n    if (is.null(hash_entry)){\n        hash_table[[hash_value]] = new_entry\n    }\n    else {\n        hash_table[[hash_value]] = c(hash_entry, new_entry)\n    }\n    return(hash_table)\n}\nhash_table = add_pair(\"Bruce\", \"From individuals to populations\", hash_table)\nhash_table = add_pair(\"Bruce2\", \"From individuals to populations2\", hash_table)\nhash_table = add_pair(\"Haley\", \"Statistical methods\", hash_table)\nhash_table\n\n[[1]]\nNULL\n\n[[2]]\n[[2]][[1]]\n[1] \"Bruce\"                           \"From individuals to populations\"\n\n[[2]][[2]]\n[1] \"Bruce2\"                           \"From individuals to populations2\"\n\n\n[[3]]\nNULL\n\n[[4]]\n[[4]][[1]]\n[1] \"Haley\"               \"Statistical methods\"\n\n\n[[5]]\nNULL\n\n\nA nefarious character named Bruce2 submitted an incremental thesis. But, alas, must go into the hash table. The keys Bruce and Bruce2 result in collisions from our hash function, keys that have the same hash value. We’re adopting a collision strategy where we just add collision keys in turn. We should also write some code that prevents us from adding the same exact key twice. As it stands we could add Bruce twice when keys need to be unique.\nLet’s write a retrieval function. It needs to find the appropriate hash value, then search within that list for the appropriate key.\n\nretrieve = function(key, hash_table){\n    n = length(hash_table)\n    hash_value = hash(key, n)\n    hash_entry = hash_table[[hash_value]]\n    ## If there's nothing there return null\n    if (is.null(hash_entry)){\n        return(NULL)\n    }\n    else {\n        keys = sapply(hash_entry, function(x) x[1])\n        key_test = key == keys\n        if (any(key_test)){\n            key_index = which(key_test) \n            return(hash_entry[[key_index]][2])\n        }\n        ## If the key isn't there return null\n        else return(NULL)\n    }\n}\nretrieve(\"Bruce\", hash_table)\n\n[1] \"From individuals to populations\"\n\nretrieve(\"Bruce2\", hash_table)\n\n[1] \"From individuals to populations2\"\n\nretrieve(\"bruce\", hash_table)\n\nNULL"
  },
  {
    "objectID": "diymlai.html",
    "href": "diymlai.html",
    "title": "9  DIY ML/AI",
    "section": "",
    "text": "Usually, the nodes are added in so called layers. \\((X_1, X_2)\\) is the input layer, \\((H_{11}, H_{12})\\) is the first hidden layer, \\((H_{21}, H_{22})\\) is the second hidden layer and \\(Y\\) is the output layer. Imagine plugging an \\(X_1\\) and \\(X_2\\) into this network. It would feed forward through the network as\n\\[\n\\begin{align}\nH_{11} = & g_1(W_{011} + W_{111} X_1 + W_{211} X_2) \\\\\nH_{12} = & g_1(W_{012} + W_{112} X_1 + W_{212} X_2) \\\\\nH_{21} = & g_2(W_{021} + W_{121} H_{11} + W_{221} H_{12}) \\\\\nH_{22} = & g_2(W_{022} + W_{122} H_{11} + W_{222} H_{12}) \\\\\n\\hat \\eta = & g_3(W_{031} + W_{131} H_{21} + W_{231} H_{22})\n\\end{align}\n\\]\nwhere \\(g_k\\) are specified activation functions and \\(\\eta\\) is our estimate of \\(Y\\). Typically, we would have a different activation function for the output layer than the others, and the other would have the same activation function. So, for example, if \\(Y\\) was binary, like hypertension diagnosis, then \\(g_1=g_2\\) and \\(g_3\\) would be a sigmoid.\nA typical activation function is a rectified linear unit (RELU), defined as \\(g(x) = x I(x > 0)\\). The neural network is typically fit via a gradient based method, such as gradient descent, assuming a loss function. The loss function is usually based on maximum likelihood.\nLet’s consider fitting the network above using gradient descent and obtaining the derivative via the chain rule. Consider the contribution of a row of data to a SE loss function, \\({\\cal L_i}(w) = (y_i - \\eta_i)^2\\), where \\(\\eta_i\\) is the feed forward of our neural network for row \\(i\\). Let’s look at the derivative with respect to \\(w_{111}\\) where we drop the subscript \\(i\\). First note that only these arrows involve \\(w_{111}\\)\n\n\n\n\n\n\\[\n\\frac{\\partial {\\cal L} }{\\partial w_{111}} =\n\\frac{\\partial {\\cal L} }{\\partial \\eta} \\frac{\\partial \\eta}{\\partial H_{2}}\n\\frac{\\partial H_2 }{\\partial H_{11}}\\frac{\\partial H_{11} }{\\partial w_{111}}\n\\]\nwhere \\(H_2 = (H_{21}, H_{22})^t\\). These parts are:\n\\[\n\\begin{aligned}\n\\frac{\\partial {\\cal L} }{\\partial \\eta} & =  -2 (Y - \\eta) \\\\\n\\frac{\\partial \\eta}{\\partial H_{2}} & = g_3'(W_{031} + W_{131} H_{21} + W_{231} H_{22}) (W_{131},  W_{231}) \\\\\n\\frac{\\partial H_2 }{\\partial H_{11}} &= [g_2'(W_{021} + W_{121} H_{11} + W_{221} H_{12}) W_{121},\n                                          g_2'(W_{022} + W_{122} H_{11} + W_{222} H_{12}) W_{122}]^t\\\\\n\\frac{\\partial H_{11} }{\\partial w_{111}} &= g_1'(W_{011} + W_{111} X_1 + W_{211} X_2) x_1\n\\end{aligned}\n\\]\nThese get multiplied together, using matrix multiplication when required, to form the first derivative for \\(W_{111}\\). This is repeated for all of the weight parameters. Notice this requires keeping track of which nodes have \\(w_{111}\\) in its parent chain and that it travels backwards through the network. For this reason, it is called backpropagation\nLet’s try coding it for this parameter. We’re going to create the model just hard coding the network.\n\nimport numpy as np\n\n## Define our activation function and its derivative\ng = lambda x : np.exp(x) / (1 + np.exp(x))\ng_deriv = lambda x: g(x) * (1 - g(x))\n\n## Here's one row of data\nY, X1, X2 = 100, 2, 3\n\n## Creating some random initialized weights\n## Adding to the dims so that the notation agrees\nW = np.random.normal( size = [3, 4, 3] )\n\nH11 = g(W[0,1,1] + W[1,1,1] * X1  + W[2,1,1] * X2)\nH12 = g(W[0,1,2] + W[1,1,2] * X1  + W[2,1,2] * X2) \nH21 = g(W[0,2,1] + W[1,2,1] * H11 + W[2,2,1] * H12) \nH22 = g(W[0,2,2] + W[1,2,2] * H11 + W[2,2,2] * H12) \nETA = g(W[0,3,1] + W[1,3,1] * H21 + W[2,3,1] * H22)\n\n## Our chain rule sequence of derivatives\nL = (Y - ETA) ** 2\n\n## Backprop calculating the derivatives\ndL_dETA  = -2 * (Y - ETA)\n\ndETA_dH2 = g_deriv(W[0,3,1] + W[1,3,1] * H21 + W[2,3,1] * H22) * np.array((W[1,3,1],  W[2,3,1]))\n\ndH2_dH11 = np.array( \n        ( g_deriv(W[0,2,1] + W[1,2,1] * H11 + W[2,2,1] * H12 ) * W[1,2,1], \n          g_deriv(W[0,2,2] + W[1,2,2] * H11 + W[2,2,2] * H12 ) * W[1,2,2] \n        ) \n)\n\ndH11_dW111 = g_deriv(W[0,1,1] + W[1,1,1] * X1 + W[2,1,1] * X2) * X1\n\n## Here's the backrpop in derivative calculation\ndL_dW111 = dL_dETA * np.sum(dETA_dH2 * dH2_dH11) * dH11_dW111\n\nprint(dL_dW111)\n\n## Let's approximate the derivative numerically\ne = 1e-6\n\n## Perturb W111 a little bit\nW[1,1,1] -= e\n\n## Feed forward through the network with the perturbed W111\nH11 = g(W[0,1,1] + W[1,1,1] * X1  + W[2,1,1] * X2)\nH12 = g(W[0,1,2] + W[1,1,2] * X1  + W[2,1,2] * X2) \nH21 = g(W[0,2,1] + W[1,2,1] * H11 + W[2,2,1] * H12) \nH22 = g(W[0,2,2] + W[1,2,2] * H11 + W[2,2,2] * H12) \nETA = g(W[0,3,1] + W[1,3,1] * H21 + W[2,3,1] * H22)\n\n## Calculate the new loss\nLe = (Y - ETA) ** 2\n\n## Here's the approximate derivative\nprint( (L - Le) / e )\n\n-4.536389700128937\n-4.536390406428836\n\n\nNow let’s calculate the derivative"
  },
  {
    "objectID": "convnet_example.html",
    "href": "convnet_example.html",
    "title": "10  Example convnet",
    "section": "",
    "text": "Medical images offer unique challenges for imaging. A common format for medical images is dicom. Most medical images are 3D or 4D grayscale images. To get a sense of working with medical images, let’s consider a set of 2D color images from the medical mnist library (Yang et al. 2021). A tutorial for working with these images can be found here. These are images of moles to determine whether or not they are cancerous. We’ll follow along with the medmnist code then investigate the output.\nHere are the imports\n\nfrom tqdm import tqdm\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.utils.data as data\nimport torchvision\nimport torchvision.transforms as transforms\nimport medmnist\nfrom medmnist import INFO, Evaluator\nimport matplotlib.pyplot as plt\nimport scipy\n\nThe dermamnist code downloads the data and loads it into a pytorch dataloader. It also keeps track of outcomes.\n\n## Using the code from https://github.com/MedMNIST/MedMNIST/blob/main/examples/getting_started.ipynb\ndata_flag = 'dermamnist'\n\n## This defines our NN parameters\nNUM_EPOCHS = 10\nBATCH_SIZE = 128\nlr = 0.001\n\ninfo = INFO[data_flag]\ntask = info['task']\nn_channels = info['n_channels']\nn_classes = len(info['label'])\n\ndata_transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[.5], std=[.5])\n])\n\n\nDataClass = getattr(medmnist, info['python_class'])\n\n\n# load the data\ntrain_dataset = DataClass(split = 'train', transform = data_transform, download = True)\ntest_dataset  = DataClass(split = 'test' , transform = data_transform, download = True)\npil_dataset   = DataClass(split = 'train',                             download = True)\n\ntrain_loader = data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\ntrain_loader_at_eval = data.DataLoader(dataset=train_dataset, batch_size= 2 * BATCH_SIZE,  shuffle=False)\ntest_loader = data.DataLoader(dataset=test_dataset, batch_size=2*BATCH_SIZE, shuffle=False)\n\nUsing downloaded and verified file: /home/bcaffo/.medmnist/dermamnist.npz\nUsing downloaded and verified file: /home/bcaffo/.medmnist/dermamnist.npz\nUsing downloaded and verified file: /home/bcaffo/.medmnist/dermamnist.npz\n\n\nLet’s look at a montage of images. Here is the Mayo clinic’s page for investigating moles for potential cancer. To quote them, use the ABCDE method, which they define exactly as below\n\nA is for asymmetrical shape. Look for moles with irregular shapes, such as two very different-looking halves.\nB is for irregular border. Look for moles with irregular, notched or scalloped borders — characteristics of melanomas.\nC is for changes in color. Look for growths that have many colors or an uneven distribution of color.\nD is for diameter. Look for new growth in a mole larger than 1/4 inch (about 6 millimeters).\nE is for evolving. Look for changes over time, such as a mole that grows in size or that changes color or shape. Moles may also evolve to develop new signs and symptoms, such as new itchiness or bleeding.\n\nOur data is cross sectional. So parts of C and D and all of E are challenging. We do not have patient retrospective reports.\n\ntrain_dataset.montage(length=20)\n\n/home/bcaffo/miniconda3/envs/ds4bio/lib/python3.10/site-packages/medmnist/utils.py:25: FutureWarning:\n\n`multichannel` is a deprecated argument name for `montage`. It will be removed in version 1.0. Please use `channel_axis` instead.\n\n\n\n\n\n\nHere are the labels\n\ninfo['label']\n\n{'0': 'actinic keratoses and intraepithelial carcinoma',\n '1': 'basal cell carcinoma',\n '2': 'benign keratosis-like lesions',\n '3': 'dermatofibroma',\n '4': 'melanoma',\n '5': 'melanocytic nevi',\n '6': 'vascular lesions'}\n\n\nHere’s a frequency table of the outcomes in the training data. Notice the prevalence of the sixth category (numbered 5), melanocytic nevi. Looking over our data, it’s by far the most prevalent. See below where we see that the this category is 67% of our training data.\n\ntrain_targets = []\nfor i, t in train_loader:\n    train_targets = np.append(train_targets, t.numpy())\n\nimport pandas as pd\npd.DataFrame({'target' : train_targets }).value_counts(normalize = True)\n\ntarget\n5.0       0.669759\n4.0       0.111175\n2.0       0.109747\n1.0       0.051234\n0.0       0.032539\n6.0       0.014129\n3.0       0.011417\ndtype: float64\n\n\nThis is important to note, because 67% trianing data accuracy is obtainable by simply calling every tumor melanocytic nevi.\nNote that the categories are exclusionary. That is, if a mole is of type \\(j\\), it can’t be of type \\(j'\\). This is different than a task where we’re trying to model multiple properties of the mole. For example, imagine if our dermatologist recorded whether a mole satisfied each of the A, B, C or D criteria. Then, the picture could have multiple 1s across different outcomes. Instead, we only have one possible outcome of the 7 for each.\nSo, we use a softmax outcome. If \\(\\eta_j\\) is a final layer of our network corresponding to category \\(j\\), consider defining \\[\nP(Y_i = j) = \\frac{\\exp(\\eta_j)}{\\sum_{j'=1}^J \\exp(\\eta_{j'})}\n\\] where \\(J\\) is the number of categories (7 in our case) and \\(Y_i\\) is an outcome for image \\(i\\). Notice, this sums to 1 over \\(j\\)."
  },
  {
    "objectID": "convnet_example.html#training-the-network",
    "href": "convnet_example.html#training-the-network",
    "title": "10  Example convnet",
    "section": "10.2 Training the network",
    "text": "10.2 Training the network\nHere is the medmnist NN. Note convd has argument in_channels, out_channels, kernel_size. So, in this case the number of channels in is 3 and in the first layer it puts out a 16 channel image obtained by convolving a 3x3x3 kernels with each channel, adding bias terms, then repeating that 15 more times (documentaiton).\n\nclass Net(nn.Module):\n    def __init__(self, in_channels, num_classes):\n        super(Net, self).__init__()\n\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(in_channels, 16, kernel_size=3),\n            nn.BatchNorm2d(16),\n            nn.ReLU())\n\n        self.layer2 = nn.Sequential(\n            nn.Conv2d(16, 16, kernel_size=3),\n            nn.BatchNorm2d(16),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2))\n\n        self.layer3 = nn.Sequential(\n            nn.Conv2d(16, 64, kernel_size=3),\n            nn.BatchNorm2d(64),\n            nn.ReLU())\n        \n        self.layer4 = nn.Sequential(\n            nn.Conv2d(64, 64, kernel_size=3),\n            nn.BatchNorm2d(64),\n            nn.ReLU())\n\n        self.layer5 = nn.Sequential(\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2))\n\n        self.fc = nn.Sequential(\n            nn.Linear(64 * 4 * 4, 128),\n            nn.ReLU(),\n            nn.Linear(128, 128),\n            nn.ReLU(),\n            nn.Linear(128, num_classes))\n\n    def forward(self, x):\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.layer5(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n\nmodel = Net(in_channels=n_channels, num_classes=n_classes)\n    \ncriterion = nn.CrossEntropyLoss()\n    \noptimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n\nNow we can train the network. Just for the sake of time, we’ve set the number of epochs to be fairly low.\n\nfor epoch in range(NUM_EPOCHS):\n    train_correct = 0\n    train_total = 0\n    test_correct = 0\n    test_total = 0\n    \n    model.train()\n    for inputs, targets in train_loader:\n        # forward + backward + optimize\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        \n        if task == 'multi-label, binary-class':\n            targets = targets.to(torch.float32)\n            loss = criterion(outputs, targets)\n        else:\n            targets = targets.squeeze().long()\n            loss = criterion(outputs, targets)\n        \n        loss.backward()\n        optimizer.step()"
  },
  {
    "objectID": "convnet_example.html#evaluating-the-network",
    "href": "convnet_example.html#evaluating-the-network",
    "title": "10  Example convnet",
    "section": "10.3 Evaluating the network",
    "text": "10.3 Evaluating the network\nThere’s some evaluation methods included at the MedMnist repo. Below this calculates the AUCs and the accuracy.\n\ndef test(split):\n    model.eval()\n    y_true = torch.tensor([])\n    y_score = torch.tensor([])\n    \n    data_loader = train_loader_at_eval if split == 'train' else test_loader\n\n    with torch.no_grad():\n        for inputs, targets in data_loader:\n            outputs = model(inputs)\n            outputs = outputs.softmax(dim=-1)\n\n            if task == 'multi-label, binary-class':\n                targets = targets.to(torch.float32)\n            else:\n                targets = targets.squeeze().long()\n                targets = targets.float().resize_(len(targets), 1)\n\n            y_true = torch.cat((y_true, targets), 0)\n            y_score = torch.cat((y_score, outputs), 0)\n\n        y_true = y_true.numpy()\n        y_score = y_score.detach().numpy()\n        \n        evaluator = Evaluator(data_flag, split)\n        metrics = evaluator.evaluate(y_score)\n    \n        print('%s  auc: %.3f  acc:%.3f' % (split, *metrics))\n\n        \nprint('==> Evaluating ...')\ntest('train')\ntest('test')\n\n==> Evaluating ...\n\n\ntrain  auc: 0.888  acc:0.723\n\n\ntest  auc: 0.879  acc:0.706\n\n\nLet’s look into the levels. First let’s grab a batch and run it through the model. Then we’ll look at the predictions for a batch. Recall there are 7 tumor types, so we plot our predictions as an image (in little bars since it’s too long otherwise).\n\ninputs,  targets = iter(test_loader).next()\n\noutputs = model(inputs)\noutputs = outputs.softmax(dim=-1)\n\nI, J = outputs.shape\n\nfor i in range(8):\n    plt.subplot(1,8,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.imshow(outputs.detach().numpy()[int(i * I / 8 ) : int((i + 1) * I / 8), :]);\n\n\n\n\nAs expected, it’s predicting the sixth category, labeled 5 since we’re counting from 0, very often, since it is the most prevalent category. The roughly 90% accuracy is pretty good. But, recall, we get 67% accuracy for free. Here’s the highest predictory category for 16 randomly selected images from our batch, their highest probability category and what their actual category is.\n\nbatch = outputs.detach().numpy()\n\n\n#grab 16 images to plot\nindices = np.random.permutation(np.array(range(I)))[0 : 16]\n\n## The associated \nactual = targets.squeeze(-1).numpy()[indices]\ntarget_pred = batch.argmax(axis = 1)[indices]\ntarget_prob = np.round(batch.max(axis = 1) * 100)[indices]\nimages = inputs.numpy()[indices,:,:,:]\n\nplt.figure(figsize=(10,10))\n\nfor i in range(16):\n    plt.subplot(4,4,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    img = np.transpose(images[i,:,:,:], (1, 2, 0));\n    img = ((img + 1)* 255/2).astype(np.uint8)\n    plt.title(\"P=\"+str(target_prob[i])+\",Yhat=\"+str(target_pred[i])+\",Y=\"+ str(actual[i]))\n    plt.imshow(img);\n\n\n\n\nLet’s look at a cross tabulation of the model prediction versus the actual value.\n\ntargets_pred, targets_actual = [], []\n\nfor i,t in test_loader:\n    targets_pred.append(model(i).softmax(dim=-1))\n    targets_actual.append(t)\n\ntargets_pred = torch.cat(targets_pred, dim = 0).detach().numpy().argmax(axis = 1)\ntargets_actual = torch.cat(targets_actual, dim=0).numpy().squeeze()\n\n\n## This is the confusion  matrix data\nconfusion = pd.DataFrame({'y' : targets_actual, 'yhat' : targets_pred}).value_counts()\nconfusion.head(10)\n\ny  yhat\n5  5       1259\n4  5        152\n2  5        101\n   2         89\n1  5         42\n5  2         37\n4  2         34\n1  1         34\n4  4         33\n0  1         25\ndtype: int64"
  },
  {
    "objectID": "convnet_example.html#visualizing-the-network",
    "href": "convnet_example.html#visualizing-the-network",
    "title": "10  Example convnet",
    "section": "10.4 Visualizing the network",
    "text": "10.4 Visualizing the network\nHere are the convolutions. Note they are color images since there are 3 channels.\n\n### here's a list of the states that the model has kept track of\nstate_dictionary = list(model.state_dict())\nstate_dictionary[1 : 5]        \n### let's get the first round of weights\nlayer1_kernel = model.get_parameter('layer1.0.weight').detach().numpy()\n\nlayer1_kernel = layer1_kernel - layer1_kernel.min()\nlayer1_kernel = layer1_kernel / layer1_kernel.max()\nlayer1_kernel = layer1_kernel * 255\n\nplt.figure(figsize=(5,5))\nfor h in range(16):\n    plt.subplot(4,4,h+1)\n    plt.xticks([])\n    plt.yticks([])\n    img = np.transpose(layer1_kernel[h,:,:,:],(1,2,0)).astype(np.uint8);\n    plt.imshow(img);\n\n\n\n\nLook at the convolutional layers by following the first image (upper left) through the network.\n\nl1 = model.layer1(inputs)\n\n## plot one\nimages = l1.detach().numpy()[indices,:,:,:]\n\ni = 0\n\nplt.figure(figsize=(10,10))\nfor h in range(16):\n    plt.subplot(4,4,h+1)\n    plt.xticks([])\n    plt.yticks([])\n    img = images[i,h,:,:];\n    plt.imshow(img);\n\n\n\n\nLet’s use SHAP to look at our convolutional neural network. SHAP uses Shapley values from game theory to explain NN predictors. Here’s a manuscript that utilizes SHAP values on this problem.\nThe Shapley value is defined in game theory as the contribution of a player to a teams’ output factoring in their cooperation. In neural networks, a pixel could be a considered a player and the gain function as the output. The goal is to see the contribution of the player/pixel, factoring in the contributions and how they interact with the others. The Shapley values are difficult to compute, and so approximations need to be used. Python has a package shap that can be used to calculate and visualize approximated Shapley values for an input.\nThe following code is taken from here\n\nimport shap\nfrom PIL import Image\n\n\nbatch = next(iter(test_loader))\nimages, _ = batch\n\nbackground = images[:100]\ntest_images = images[100:105]\n\ne = shap.DeepExplainer(model, background)\nshap_values = e.shap_values(test_images)\n\nshap_numpy = [np.swapaxes(np.swapaxes(s, 1, -1), 1, 2) for s in shap_values]\ntest_numpy = np.swapaxes(np.swapaxes(test_images.cpu().numpy(), 1, -1), 1, 2)\nshap.image_plot(shap_numpy, -test_numpy)\n\nUsing a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\n\n\n\n\nYang, Jiancheng, Rui Shi, Donglai Wei, Zequan Liu, Lin Zhao, Bilian Ke, Hanspeter Pfister, and Bingbing Ni. 2021. “MedMNIST V2: A Large-Scale Lightweight Benchmark for 2D and 3D Biomedical Image Classification.” arXiv Preprint arXiv:2110.14795."
  },
  {
    "objectID": "unsupervised.html",
    "href": "unsupervised.html",
    "title": "11  Unsupervised learning",
    "section": "",
    "text": "Let \\(\\{X_i\\}\\) for \\(i=1,\\ldots,n\\) be \\(p\\) random vectors with means \\((0,\\ldots,0)^t\\) and variance matrix \\(\\Sigma\\). Consider finding \\(v_1\\), a \\(p\\) dimensional vector with \\(||v_1|| = 1\\) so that \\(v_1^t \\Sigma v_1\\) is maximized. Notice this is equivalent to saying we want to maximize \\(\\mathrm{Var}( X_i^t V_1)\\). The well known solution to this equation is that \\(v_1\\) is the first eigenvector of \\(\\Sigma\\) and \\(\\lambda_1 = \\mathrm{Var}( X_i^t V_1)\\) is the associated eigenvalue. If \\(\\Sigma = V^t \\Lambda V\\) is the eigenvalue decomposition of where \\(V\\) are the eigenvectors and \\(\\Lambda\\) is a diagonal matrix of the eigenvalues ordered from greatest to least, then \\(v_1\\) corresponds to the first column of \\(V\\) and \\(\\lambda_1\\) corresponds to the first element of \\(\\Lambda\\). If one then finds \\(v_k\\) as the vector maximizing \\(v_k^t \\Sigma v_k\\) so that \\(v_k^t v_{k'} = I(k=k')\\), then the \\(v_k\\) are the columns of \\(V\\) and \\(v_k^t \\Sigma v_k = \\lambda_k\\) are the eigenvalues.\nNotice:\n\n\\(V \\Sigma V^t = \\Lambda\\) (i.e. \\(V\\) diagonalizess \\(\\Sigma\\))\n\\(\\mbox{Trace}(\\Sigma) = \\mbox{Trace}(\\Sigma V^t V) = \\mbox{Trace}(V \\Sigma V^t) = \\sum \\lambda_k\\) (i.e. the total variability is the sum of the eigenvalues)\nSince \\(V^t V = I\\), \\(V\\) is a rotation matrix. Thus, \\(V\\) rotates \\(X_i\\) in such a way that to maximize variability in the first dimension, then the second dimensions …\n\\(\\mbox{Cov}(X_i^t v_k, x_i^t v_{k'} )= \\mbox{Cov}(X_i^t v_k, x_i^t v_{k'} ) v_k^t \\mbox{Cov}(x_i, x_i^t) v_{k'} = v_k^t V v_{k'} = 0\\) if \\(k\\neq k'\\)\nAnother representation of \\(\\Sigma\\) is \\(\\sum_{k=1}^p \\lambda_i v_k v_k^t\\) by simply rewriting the matrix algebra of \\(V \\Lambda V^t\\).\nThe variables \\(U_i = V X_i\\) then: have uncorrelated elements (\\(\\mbox{Cov}(U_{ik}, U_{ik'}) = 0\\) for \\(k\\neq k'\\) by property 5), have the same total variability as the elements of \\(X_i\\) (\\(\\sum_k \\mbox{Var}(U_{ik}) = \\sum_k \\lambda_k = \\sum_k \\mbox{Var}(X_{ik})\\) by property 2), are a rotation of the \\(X_i\\), are ordered so that \\(U_{i1}\\) has the greatest amount of variability and so on.\n\nNotation:\n\nThe \\(\\lambda_k\\) are simply called the eigenvalues or principal components variation.\n\\(U_{ik} = X_i^t v_k\\) is called the principal component scores.\nThe \\(v_k\\) are called the principal component loadings or weights, with \\(v_1\\) being called the first principal component and so on.\n\nStatistical properties\n\n\\(E[U_{ik}]=0\\)\n\\(\\mbox{Var}(U_{ik}) = \\lambda_k\\)\n\\(\\mbox{Cov}(U_{ik}, U_{ik'}) = 0\\) if \\(k\\neq k'\\)\n\\(\\sum_{k=1}^p \\mbox{Var}(U_{ik}) = \\mbox{Trace}(\\Sigma)\\).\n\\(\\prod_{k=1}^p \\mbox{Var}(U_{ik}) = \\mbox{Det}(\\Sigma)\\)\n\n\n\nOf course, we’re describing PCA as a conceptual process. We realize \\(n\\) \\(p\\) dimensional vectors \\(x_1\\) to \\(x_n\\), typically organized in \\(X\\) a \\(n\\times p\\) matrix. If \\(X\\) is not mean 0, we typically demean it by calculating \\((I- J(J^t J)^{-1} J') X\\) where \\(J\\) is a vector of ones. Assume this is done. Then \\(\\frac{1}{n-1} X^t X = \\hat \\Sigma\\). Thus, our sample PCA is obtained via the eigenvalue decomposition \\(\\hat \\Sigma = \\hat V \\hat \\Lambda \\hat V^t\\) and our principal components obtained as $ X V$.\nWe can relate PCA to the SVD as follows. Let \\(\\frac{1}{\\sqrt{n-1}} X = \\hat U \\sqrt{\\hat \\Lambda} \\hat V^t\\) be the SVD of the scaled version of \\(X\\). Then note that \\[\n\\hat \\Sigma = \\frac{1}{n-1} X^t X = \\hat V \\hat \\Lambda \\hat V^t\n\\] yields the sample covariance matrix eigenvalue decomposition.\n\n\n\nConsider the case where one of \\(n\\) or \\(p\\) is large. Let’s assume \\(n\\) is large. Then \\[\n\\frac{1}{n-1} X^t X = \\frac{1}{n-1} \\sum_i x_i x_i^t\n\\] As we learned in the chapter on HDF5, we can do sums like these without loading the entirety of \\(X\\) into memory. Thus, in this case, we can calculate the eigenvectors using only the small dimension. If, on the other hand, \\(p\\) is large and \\(n\\) is smaller, then we can calculate the eigenvalue decomposition of \\[\n\\frac{1}{n-1} X X^t = \\hat U \\hat \\Lambda \\hat U^t.\n\\] In either case, whether \\(U\\) or \\(V\\) is easier to get, we can then obtain the other via vectorized multiplication.\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport numpy.linalg as la\nfrom sklearn.decomposition import PCA\nimport urllib.request\nimport PIL\nimport numpy as np\nimport torch \nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset, DataLoader\nimport torchvision\nimport torchvision.transforms as transforms\n\n\nn = 1000\nmu = (0, 0)\nSigma = np.matrix([[1, .5], [.5, 1]])\nX = np.random.multivariate_normal( mean = mu, cov = Sigma, size = n)\n\nplt.scatter(X[:,0], X[:,1])\n\n<matplotlib.collections.PathCollection at 0x7c35a4745d80>\n\n\n\n\n\n\nX = X - X.mean(0)\nprint(X.mean(0))\nSigma_hat = np.matmul(np.transpose(X), X) / (n-1) \nSigma_hat\n\n[ 2.73669976e-17 -2.66453526e-18]\n\n\narray([[1.09203671, 0.56675734],\n       [0.56675734, 1.06592638]])\n\n\n\nevd = la.eig(Sigma_hat)\nlambda_ = evd[0]\nv_hat = evd[1]\nu_hat = np.matmul(X, np.transpose(v_hat))\nplt.scatter(u_hat[:,0], u_hat[:,1])\n\n<matplotlib.collections.PathCollection at 0x7c35a49950f0>\n\n\n\n\n\nFit using scikitlearn’s function\n\npca = PCA(n_components = 2).fit(X)\nprint(pca.explained_variance_)\nprint(lambda_ )\n\n[1.64588923 0.51207386]\n[1.64588923 0.51207386]\n\n\n\nu_hat2 = pca.transform(X)\nplt.subplot(2, 2, 1)\nplt.scatter(u_hat2[:,0], u_hat2[:,1])\nplt.subplot(2, 2, 2)\nplt.scatter(u_hat2[:,0], u_hat[:,0])\nplt.subplot(2, 2, 3)\nplt.scatter(u_hat2[:,1], u_hat[:,1])\n\n<matplotlib.collections.PathCollection at 0x7c35a4877670>\n\n\n\n\n\n\n\n\nLet’s consider the melanoma dataset that we looked at before. First we read in the data as we have done before so we don’t show that code.\n\n\nUsing downloaded and verified file: /home/bcaffo/.medmnist/dermamnist.npz\nUsing downloaded and verified file: /home/bcaffo/.medmnist/dermamnist.npz\nUsing downloaded and verified file: /home/bcaffo/.medmnist/dermamnist.npz\n\n\nNext, let’s get the data from the torch dataloader format back into an image array and a matrix with the image part (28, 28, 3) vectorized.\n\ndef loader_to_array(dataloader):\n  ## Read one iteration to get data\n  test_input, test_target = iter(dataloader).next()\n  ## Total number of training images\n  n = np.sum([inputs.shape[0] for inputs, targets in dataloader])\n  ## The dimensions of the images\n  imgdim = (test_input.shape[2], test_input.shape[3])\n  images = np.empty( (n, imgdim[0], imgdim[1], 3))\n\n  ## Read the data from the data loader into our numpy array\n  idx = 0\n  for inputs, targets in dataloader:\n    inputs = inputs.detach().numpy()\n    for j in range(inputs.shape[0]):\n      img = inputs[j,:,:,:]\n      ## get it out of pytorch format\n      img = np.transpose(img, (1, 2, 0))\n      images[idx,:,:,:] = img\n      idx += 1\n  matrix = images.reshape(n, 3 * np.prod(imgdim))\n  return images, matrix\n\ntrain_images, train_matrix = loader_to_array(train_loader)\ntest_images, test_matrix = loader_to_array(test_loader)\n\n## Demean the matrices\ntrain_mean = train_matrix.mean(0)\ntrain_matrix = train_matrix - train_mean\ntest_mean = test_matrix.mean(0)\ntest_matrix = test_matrix - test_mean\n\nNow let’s actually perform PCA using scikitlearn. We’ll plot the eigenvalues divided by their sums, \\(\\lambda_k / \\sum_{k'} \\lambda_{k'}\\). This is called a scree plot.\n\nfrom sklearn.decomposition import PCA\nn_comp = 10\npca = PCA(n_components = n_comp).fit(train_matrix)\nplt.plot(pca.explained_variance_ratio_)\n\n\n\n\nOften this is done by plotting the cummulative sum so that you can visualize how much variance is explained by including the top \\(k\\) components. Here I fit 10 components and they explain 85% of the variation.\n\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\n\n\n\n\nLet’s project our testing data onto the principal component basis created by our training data and see how it does. Let \\(X_{training} = U \\Lambda^{1/2} V^t\\) is the SVD of our training data. Then, we can convert ths scores, \\(U\\) back to \\(X_{training}\\) with the map \\(U \\rightarrow U \\lambda^{1/2} V\\). Or, if our scores are normalized, \\(U \\Lambda^{1/2}\\) then we simply multiply by \\(V^t\\). If we want to represent \\(X_{training}\\) by a lower dimensional summary, we just keep fewer columns of scores, then multiply by the same columns of \\(V\\). We could write this as \\(U_s = X_{training} V_S \\lambda^{-1/2}_S\\), where \\(S\\) refers to a subset of values of \\(k\\).\nNotice that \\(\\hat X_{training} = U_{S} V^t_S \\Lambda^{-1/2}_S = X_{training} V_S V_S^t\\) , \\(\\Lambda\\) and \\(V\\). Consider then an approximation to \\(X_{test}\\) as \\(\\hat X_{test} = X_{test} V_s V_S^t\\). Written otherwise \\[\n\\hat X_{i,test} = \\sum_{k \\in S} <x_{i,test}, v_k> v_k\n\\] which is the projection of subject \\(i\\)’s features into the linear space spanned by the basis defined by the principal component loadings.\nLet’s try this on our mole data.\n\ntest_matrix_fit = pca.inverse_transform(pca.transform(test_matrix))\nnp.mean(np.abs( test_matrix - test_matrix_fit))\n\n0.03792390855153157\n\n\n\n\n\n\n\n\nfrom sklearn.decomposition import FastICA\n\ntransformer = FastICA(n_components=10, random_state=0, whiten='unit-variance')\n\nicafit = transformer.fit_transform(train_matrix)\ntrain_matrix_fit_ica = transformer.inverse_transform(icafit)\n\n\ntrain_matrix_remeaned = train_matrix + train_mean\n\n#| echo: false\nplt.figure(figsize=(10,5))\nfor i in range(5): \n  plt.subplot(2, 5,i+1)\n  plt.xticks([])\n  plt.yticks([])\n  img = train_matrix_remeaned[i,:].reshape(28, 28, 3)\n  img = (img - img.min())\n  img = img / img.max()\n  img = img * 255 \n  img = img.astype(np.uint8)\n  plt.imshow(img)\n\n  plt.subplot(2, 5,i+6)\n  plt.xticks([])\n  plt.yticks([])\n  img = train_matrix_fit_ica[i,:].reshape(28, 28, 3)\n  img = (img - img.min())\n  img = img / img.max()\n  img = img * 255 \n  img = img.astype(np.uint8)\n  plt.imshow(img)"
  },
  {
    "objectID": "data_analysis_theory.html",
    "href": "data_analysis_theory.html",
    "title": "12  Data science, conceptually",
    "section": "",
    "text": "In this chapter, we’ll focus on data science theory, thinking and philosophy. We’re going to omit any standard treatment of statistical theory and inference, like maximum likelihood optimality and asymptotics, since those are well covered elsewhere. However, it’s worth mentioning that those topics are obviously part of data science theory.\nInstead, we’ll focus on meta-inferential and meta-empirical science questions, as well as some of the conceptual and theoretical language that’s worth knowing."
  },
  {
    "objectID": "data_analysis_theory.html#all-models-are-wrong",
    "href": "data_analysis_theory.html#all-models-are-wrong",
    "title": "12  Data science, conceptually",
    "section": "12.2 All models are wrong",
    "text": "12.2 All models are wrong\n\n12.2.1 Wrong means wrong\n“All models are wrong, but some are useful”, or some variant, is a quote from statistician George Box that is so well known and used that it has a lengthy wikipedia page. Restricting our attention to probabilistic models, it is interesting to note that this quote, which is near universally agreed upon, has implications that are often not. For example, the quote suggests that there is not, and never has been: an IID sample, normally distributed data (as in having been generated from a normal distribution), a true population stochastic model … In other words, there is no correct probabilistic model, ever, ever, ever (according to the quote).\nOne way to interpret this is that there are correct probability models, we just haven’t found them yet, or maybe can’t find them via some incompleteness law. If we ever find one, I guess we’d have to change the quote to “All models were wrong …”. But, I don’t think the quote is implying the existence of true probabilistic models that we don’t, or can’t, know. I tend to think it is suggesting that, by in large, randomness doesn’t exist and hence probabilitity models are, like Newtonian mechanics, just models, not truth.\nThis is a well discussed topic in philosophy. On some meaningful level, the quote is obviously true. Most things we’re interested in are clearly purely functionally caused by antecedent variables, some of which we know and can measure and some of which we can’t. This is obviously true of of things like die rolls or casino games and, especially, random number generators, where we know the actual deterministic formula generating the numbers.0\nBut does ranomness exist for some weird quantum setting, or is it just a useful model? The best answer for this question came from a famous results called Bell’s theorem, which suggests that hidden variables are not the missing ingridient to explain quantum phenomena. This theory has since been corraborated via experiments. To explain this theory using pure determinism (i.e. hidden variables) requires giving up some important notions in this area, such as locality. But, I’ll stop here, since I don’t understand this stuff at all.\nFor our purposes, this seems irrelevant. Even if randomness did exist at the scale of the extremely small, our data generally references systems where we believe that determinism holds. There is no Bell’s theorem of disease. Typically, we think observed and hidden variables explain the relevant majority of an individual or population’s state of health. Moreover, regardless if some models are right or it’s just true that all stochastic models are wrong, many models are extremely useful. It is more important to be able to accurately represent one’s assumptions and how they connect to the experimental setting than it is to argue that one’s assumptions are true on some bizarre abstract level. So, my recommendation is to ignore this line of thinking entirely, and instead focus on being able to articulate your assumptions and describe what it is you’re treating as random, regardless of whether or not it actually is."
  },
  {
    "objectID": "data_analysis_theory.html#some-models-are-useful",
    "href": "data_analysis_theory.html#some-models-are-useful",
    "title": "12  Data science, conceptually",
    "section": "12.3 Some models are useful",
    "text": "12.3 Some models are useful\nDo we even care if models are ultimately correct? The quote ends with, “some models are useful”. How are they useful?"
  },
  {
    "objectID": "data_science_as_a_science.html",
    "href": "data_science_as_a_science.html",
    "title": "13  Data science as an applied science",
    "section": "",
    "text": "The term “Data science” is typically used to refer to a set of tools, techniques and thought processes used to perform scientific inquiries using data. But is that a scientific topic worthy of study in its own right? It is. And, from a practical, theoretical and philosophical level, it’s already extremely well studied in statistics, computer science, engineering and philosophy departments.\nDespite these fields, at the Data Science Lab at JHU, we’ve had lengthy discussions about data science as an inductive, empirical applied science, like biology or medicine, rather than an deductive discipline, like mathematics or statistical theory, or rather than a set of heuristics, like rules of thumb and agreed upon best practices. An inductive, empirical, applied science itself needs data science, since it’s empirical and thus depends on data. So, maybe there’s an infinite regress of some sort making this an ultimately doomed endeavor. But, nevertheless, we persist.\nThere are some fields of data analysis that are well covered, let’s talk about them first.\n\n\nGeneral EDA and visualization is covered in the next chapter. There is a vast literature of experiments to understand perception of visual data. This is a more neatly circumscribed area of data science as a science. This is possibly because the general field of visual perception, from a variety of angles, is well developed. Let’s cover a specific example, observer studies in radiology.\n\n\nIn diagnostic radiology, new technology in the form of new machines for imaging or new ways of processing images, are being invented constantly. To evaluate these new technologies, a gold standard is to have randomized studies where the underlying truth is known. The images from the new technology and images from a control technology are then randomized to observers. Several issues come about in observer studies. First, establishing truth can be hard. This is often done by digital or physical phantoms or in instances where patients are longitudinally followed up and the disease status is made apparent. A second issue is in the cost associated with observers. Often, instead of attending radiologists, the studies are done with residents or fellows, or students who have received specific training to qualify as reasonable proxies. An interesting alternative approach is to have digital observers.\nMy friends at the JHU Division of Medical Imaging Physics do this quite well. In one process, they first create highly accurate models of the human/non-human animal being studied (so called pantoms, see here). Next they create accurate models of the imaging system, say X-Ray CT or positron imaging. Suppose that they want to study two different ways of performing tomography, say a Bayes algorithm or an EM algorithm. They take generated images from their digital phantom and process them using the two candidate algorithms. Then they use human or mathematical observers to try to diagnose the disease using the processed images. Here’s some examples He et al. (2004).\n\n\n\n\n\n\n\n\nGilland, Karen L, Benjamin MW Tsui, Yujin Qi, and Grant T Gullberg. 2006. “Comparison of Channelized Hotelling and Human Observers in Determining Optimum OS-EM Reconstruction Parameters for Myocardial SPECT.” IEEE Transactions on Nuclear Science 53 (3): 1200–1204.\n\n\nHe, Xin, Eric C Frey, Jonathan M Links, Karen L Gilland, William P Segars, and Benjamin MW Tsui. 2004. “A Mathematical Observer Study for the Evaluation and Optimization of Compensation Methods for Myocardial SPECT Using a Phantom Population That Realistically Models Patient Variability.” IEEE Transactions on Nuclear Science 51 (1): 218–24."
  },
  {
    "objectID": "graphics.html",
    "href": "graphics.html",
    "title": "14  Theory of graphical display",
    "section": "",
    "text": "One of the main design arguments for the graphical display of information is data / ink maximization Tufte (1990). This is the idea that idea that as much of the “ink” (non-background pixels) of the plot as possible should be displaying data.\nData/ink maximalization has been criticized empirically. For example, Inbar, Tractinsky, and Meyer (2007) conducted a study with 87 undergraduates and found a clear preference for the non-maximized variations. Another line of argument discusses the “paradox of simplicity” Norman (2007), Eytam, Tractinsky, and Lowengart (2017), whereby we have a strong aesthetic preference for simplicity, but also want flexibility and maximum utility.\n\nBertin (1983)\n\n\n\n\nWickham et al. (2010)\n\n\n\n\n\nCleveland (1987)\nCleveland and McGill (1984)\nCleveland and Devlin (1980)\nCarswell (1992)\nCleveland and McGill (1986)\nMagical thinking Diaconis (2006)"
  },
  {
    "objectID": "graphics.html#implementation",
    "href": "graphics.html#implementation",
    "title": "14  Theory of graphical display",
    "section": "14.2 Implementation",
    "text": "14.2 Implementation\n\n14.2.1 Grammar of graphics\n\nWilkinson (2012)\nWilkinson (2013)\nWickham (2010)\n\n\n\n14.2.2 Narative storytelling\nEdward and Jeffrey (Segel and Heer (2010)) argue regarding the use of modern interactive tools in data narrative storytelling. They give seven canonical genres of narrative visulation."
  },
  {
    "objectID": "graphics.html#historical-graphics",
    "href": "graphics.html#historical-graphics",
    "title": "14  Theory of graphical display",
    "section": "14.3 Historical graphics",
    "text": "14.3 Historical graphics"
  },
  {
    "objectID": "graphics.html#graph-galleries",
    "href": "graphics.html#graph-galleries",
    "title": "14  Theory of graphical display",
    "section": "14.4 Graph galleries",
    "text": "14.4 Graph galleries\n\n\n\n\nBertin, Jacques. 1983. Semiology of Graphics. University of Wisconsin press.\n\n\nCarswell, C Melody. 1992. “Choosing Specifiers: An Evaluation of the Basic Tasks Model of Graphical Perception.” Human Factors 34 (5): 535–54.\n\n\nCleveland, William S. 1987. “Research in Statistical Graphics.” Journal of the American Statistical Association 82 (398): 419–23.\n\n\nCleveland, William S, and Susan J Devlin. 1980. “Calendar Effects in Monthly Time Series: Detection by Spectrum Analysis and Graphical Methods.” Journal of the American Statistical Association 75 (371): 487–96.\n\n\nCleveland, William S, and Robert McGill. 1984. “The Many Faces of a Scatterplot.” Journal of the American Statistical Association 79 (388): 807–22.\n\n\n———. 1986. “An Experiment in Graphical Perception.” International Journal of Man-Machine Studies 25 (5): 491–500.\n\n\nDiaconis, Persi. 2006. “Theories of Data Analysis: From Magical Thinking Through Classical Statistics.” Exploring Data Tables, Trends, and Shapes, 1–36.\n\n\nEytam, Eleanor, Noam Tractinsky, and Oded Lowengart. 2017. “The Paradox of Simplicity: Effects of Role on the Preference and Choice of Product Visual Simplicity Level.” International Journal of Human-Computer Studies 105: 43–55.\n\n\nInbar, Ohad, Noam Tractinsky, and Joachim Meyer. 2007. “Minimalism in Information Visualization: Attitudes Towards Maximizing the Data-Ink Ratio.” In Proceedings of the 14th European Conference on Cognitive Ergonomics: Invent! Explore!, 185–88.\n\n\nNorman, Donald A. 2007. “Simplicity Is Highly Overrated.” Interactions 14 (2): 40–41.\n\n\nSegel, Edward, and Jeffrey Heer. 2010. “Narrative Visualization: Telling Stories with Data.” IEEE Transactions on Visualization and Computer Graphics 16 (6): 1139–48.\n\n\nTufte, ER. 1990. “Data-Ink Maximization and Graphical Design.” Oikos, 130–44.\n\n\nWickham, Hadley. 2010. “A Layered Grammar of Graphics.” Journal of Computational and Graphical Statistics 19 (1): 3–28.\n\n\nWickham, Hadley, Dianne Cook, Heike Hofmann, and Andreas Buja. 2010. “Graphical Inference for Infovis.” IEEE Transactions on Visualization and Computer Graphics 16 (6): 973–79.\n\n\nWilkinson, Leland. 2012. “The Grammar of Graphics.” In Handbook of Computational Statistics, 375–414. Springer.\n\n\n———. 2013. The Grammar of Graphics. Springer Science & Business Media."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Bertin, Jacques. 1983. Semiology of Graphics. University of\nWisconsin press.\n\n\nCarswell, C Melody. 1992. “Choosing Specifiers: An Evaluation of\nthe Basic Tasks Model of Graphical Perception.” Human\nFactors 34 (5): 535–54.\n\n\nCleveland, William S. 1987. “Research in Statistical\nGraphics.” Journal of the American Statistical\nAssociation 82 (398): 419–23.\n\n\nCleveland, William S, and Susan J Devlin. 1980. “Calendar Effects\nin Monthly Time Series: Detection by Spectrum Analysis and Graphical\nMethods.” Journal of the American Statistical\nAssociation 75 (371): 487–96.\n\n\nCleveland, William S, and Robert McGill. 1984. “The Many Faces of\na Scatterplot.” Journal of the American Statistical\nAssociation 79 (388): 807–22.\n\n\n———. 1986. “An Experiment in Graphical Perception.”\nInternational Journal of Man-Machine Studies 25 (5): 491–500.\n\n\nDiaconis, Persi. 2006. “Theories of Data Analysis: From Magical\nThinking Through Classical Statistics.” Exploring Data\nTables, Trends, and Shapes, 1–36.\n\n\nEytam, Eleanor, Noam Tractinsky, and Oded Lowengart. 2017. “The\nParadox of Simplicity: Effects of Role on the Preference and Choice of\nProduct Visual Simplicity Level.” International Journal of\nHuman-Computer Studies 105: 43–55.\n\n\nGilland, Karen L, Benjamin MW Tsui, Yujin Qi, and Grant T Gullberg.\n2006. “Comparison of Channelized Hotelling and Human Observers in\nDetermining Optimum OS-EM Reconstruction Parameters for Myocardial\nSPECT.” IEEE Transactions on Nuclear Science 53 (3):\n1200–1204.\n\n\nInbar, Ohad, Noam Tractinsky, and Joachim Meyer. 2007. “Minimalism\nin Information Visualization: Attitudes Towards Maximizing the Data-Ink\nRatio.” In Proceedings of the 14th European Conference on\nCognitive Ergonomics: Invent! Explore!, 185–88.\n\n\nNorman, Donald A. 2007. “Simplicity Is Highly Overrated.”\nInteractions 14 (2): 40–41.\n\n\nSegel, Edward, and Jeffrey Heer. 2010. “Narrative Visualization:\nTelling Stories with Data.” IEEE Transactions on\nVisualization and Computer Graphics 16 (6): 1139–48.\n\n\nTufte, ER. 1990. “Data-Ink Maximization and Graphical\nDesign.” Oikos, 130–44.\n\n\nWickham, Hadley. 2010. “A Layered Grammar of Graphics.”\nJournal of Computational and Graphical Statistics 19 (1): 3–28.\n\n\nWickham, Hadley, Dianne Cook, Heike Hofmann, and Andreas Buja. 2010.\n“Graphical Inference for Infovis.” IEEE Transactions on\nVisualization and Computer Graphics 16 (6): 973–79.\n\n\nWilkinson, Leland. 2012. “The Grammar of Graphics.” In\nHandbook of Computational Statistics, 375–414. Springer.\n\n\n———. 2013. The Grammar of Graphics. Springer Science &\nBusiness Media.\n\n\nYang, Jiancheng, Rui Shi, Donglai Wei, Zequan Liu, Lin Zhao, Bilian Ke,\nHanspeter Pfister, and Bingbing Ni. 2021. “MedMNIST V2: A\nLarge-Scale Lightweight Benchmark for 2D and 3D Biomedical Image\nClassification.” arXiv Preprint arXiv:2110.14795."
  }
]