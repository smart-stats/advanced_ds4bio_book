[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Advanced Data Science for Public Health",
    "section": "",
    "text": "This is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book for the Advanced Data Science for Bio/Public Health/medical classes. Since data science isn’t super well defined, advanced data science is even less so. My opinion is that we needed the umbrella term data science because there was a lot in the processes of analyzing data that got ignored in traditional disciplines and training programs. (Of course, what gets ignored is different depeneding on which discipline or program.) I’m mostly going to focus on concepts and implementation that were historically ignored in our (JHU Biostat) program, which is heavily focused on biostatistical inference, probability modeling, public health/bio/medical data analyses and ML.\n\n\nI’m going to assume that you have a lot of basic data science tools down already. If not, here’s some notes. For this book you’ll need: prior programming experience, calculus, linear algebra, unix, python, R and some basic statistics.\n\n\n\nThis is a two quarter course. The first quater is devoted to tools and the second is deveoted to theory. So the book is divided in half that way.\n\n\n\nRead these papers first\n\nTukey (1962)\nDonoho (2017)\nLeek and Peng (2015)\nLeek and Peng (2015)\nKass et al. (2016)\nHicks and Irizarry (2018)\nHardin et al. (2015)\n\n\n\n\n\nDonoho, David. 2017. “50 Years of Data Science.” Journal of Computational and Graphical Statistics 26 (4): 745–66.\n\n\nHardin, Johanna, Roger Hoerl, Nicholas J Horton, Deborah Nolan, Ben Baumer, Olaf Hall-Holt, Paul Murrell, et al. 2015. “Data Science in Statistics Curricula: Preparing Students to ‘Think with Data’.” The American Statistician 69 (4): 343–53.\n\n\nHicks, Stephanie C, and Rafael A Irizarry. 2018. “A Guide to Teaching Data Science.” The American Statistician 72 (4): 382–91.\n\n\nKass, Robert E, Brian S Caffo, Marie Davidian, Xiao-Li Meng, Bin Yu, and Nancy Reid. 2016. “Ten Simple Rules for Effective Statistical Practice.” PLoS Computational Biology. Public Library of Science.\n\n\nLeek, Jeffery T, and Roger D Peng. 2015. “What Is the Question?” Science 347 (6228): 1314–15.\n\n\nTukey, John W. 1962. “The Future of Data Analysis.” The Annals of Mathematical Statistics 33 (1): 1–67."
  },
  {
    "objectID": "interactive.html",
    "href": "interactive.html",
    "title": "2  Interactive graphics",
    "section": "",
    "text": "In your other DS courses, you’ve learned how to create static graphics uses R, ggplot, matplotlib, seaborn … You’ve probably also learned how to create client side interactive graphics using libraries like plotly and maybe also learned client-server interactivity with shiny, dash …\nIn this section we’re going to dig deeper into client side graphics, which are almost always done via html, css, javascript and a javascript plotting library. We’re going to focus on d3.js, a well known javascript library for creating interactive data visulalizations.\nTools like d3 are mostly for creating professional data web graphics. So, most of our daily graphics use will just use python/R/julia/matlab … or plotting libraries like plotly. Usually, you want to prototype graphics outside of d3. Here, we’ll give you a smidge of using d3 to get you started if your goal is to become a graphics expert."
  },
  {
    "objectID": "interactive.html#introduction-to-d3",
    "href": "interactive.html#introduction-to-d3",
    "title": "2  Interactive graphics",
    "section": "2.1 Introduction to D3",
    "text": "2.1 Introduction to D3\nLet’s get started. I’m going to assume that you have a basic knowledge of html, css and a little bit of javascript. D3 works by manipulating html elements. Let’s select every paragraph element in a document.\n<!DOCTYPE html>\n<html lang=\"en\">\n\n<head>\n    <script src=\"https://d3js.org/d3.v5.min.js\"></script>\n</head>\n\n<body>\n    <p> Advanced </p>\n    <p> Data science </p> \n        <script>\n            let pselect = d3.selectAll(\"p\")\n            //let pselect = d3.select(\"p\").style(\"color\", \"green\");\n            //let pselect = d3.selectAll(\"p\").style(\"color\", \"green\");\n        </script>\n    </body>\n</html>\nGoing forward, we’ll omit most of the html commands.\n\nThe command <script src=\"https://d3js.org/d3.v5.min.js\"></script> loads d3 from a CDN. You could also download it locally if you’d like.\nThe script let pselect = d3.selectAll(\"p\").style(\"color\", \"green\"); creates a variable pselect that is all of the html paragraph elements\nTry doing this, loading the web page, then try uncommenting each other script line in turn and refreshing\nIn chrome do Ctrl-shift-i to get the developer console and inspect the variable pselect.\nNesting select or selectAll will select elements within the selected elements.\nYou can also select by id or class."
  },
  {
    "objectID": "interactive.html#a-simple-example",
    "href": "interactive.html#a-simple-example",
    "title": "2  Interactive graphics",
    "section": "2.2 A simple example",
    "text": "2.2 A simple example\nLet’s go through an example where we plot brain volumetric ROI data on the log scale using D3.\n<style>\n    .bar {\n        background: #f5b634;\n        border: 4px solid #0769ad;\n        height: 20px;\n    }\n</style>\n<body>\n        <script>\n            let roiData = [\n                {\"roi\": \"Telencephalon_L\", \"volume\" : 531111},\n                {\"roi\": \"Telencephalon_R\", \"volume\" : 543404},\n                {\"roi\": \"Diencephalon_L\",  \"volume\" : 9683  },\n                {\"roi\": \"Diencephalon_R\",  \"volume\" : 9678  },\n                {\"roi\": \"Mesencephalon\",   \"volume\" : 10268 },\n                {\"roi\": \"Metencephalon\",   \"volume\" : 159402},\n                {\"roi\": \"Myelencephalon\",  \"volume\" : 4973  },\n                {\"roi\": \"CSF\",             \"volume\" : 109776}\n            ];\n    \n            let divSelection = d3.select(\"body\") \n                    .selectAll(\"div\")\n                    .data(roiData)\n                    .enter()\n                    .append('div')\n                    .attr(\"class\", \"bar\")\n                    .style(\"width\", (d) => {return Math.log(d.volume) * 20 + \"px\"; })\n                    .text(d => d.roi)\n                    .on(\"mouseover\", function(){\n                        d3.select(this)\n                        .style(\"background-color\", \"orange\");\n                    })\n                    .on(\"mouseout\", function(){\n                        d3.select(this)\n                        .style(\"background-color\",\"#33A2FF\" )\n                    })        </script>\n    </body>\n\nThe data(roiDat) selects our dataset\nThe enter() and append('div') commands add div elements to the html document, one per data element.\nThe attr method considers our bar stylesheet style\nThe style method changes the style so that the bars have the width of our data. The notation (d) => {return d.volume * .001 + \"px\"} is a function that selects the ROI element of the data, multiplies it by .001 then converts it to text with px at the end.\nThe text method at the end appends the text to our plot\nThe on methods say what to do when one mouses over and off the bars. You can see now that they turn orange then back. Remove the mouseout .on call and see what happens.\n\nThe output looks like this. Hover over a bar to test. (Look at the file in d3/roi1.html)"
  },
  {
    "objectID": "interactive.html#working-through-a-realistic-example",
    "href": "interactive.html#working-through-a-realistic-example",
    "title": "2  Interactive graphics",
    "section": "2.3 Working through a realistic example",
    "text": "2.3 Working through a realistic example\nUnder assets/kirby_pivot.csv is a dataset with the kirby 21 data pivoted to have regions as columns. Let’s work through a d3 example of ploting right versus left asymmetry in the telencephalon (the largest area of the brain including the cortex and central white matter).\nHere’s the scatterplot that I’ve got so far. For HW, add text labels to the point, or a tooltip that gives point information when you hover over it.\n\nThe code for the plot is in d3/roi2.html. Let’s go over some of the main parts of the d3 code here. First, we set up the graphic\nconst h = 500\nconst w = 500\n\n// create the background\nlet svg = d3.select(\"body\")\n    .append(\"svg\")\n    .attr(\"width\" , h)\n    .attr(\"height\", w);\nNext we load in the data. First, we create a function that does a little row processing for us. Honestly, it’s probably better to just do this in python/R/julia … beforehand, but it’s worth showing here. We create variables for the log ratio between the right and left hemispheres and the log of the geometric mean. We’ll use this to create a Tukey mean/difference plot of the log of the volumes.\n//create the variables we're interested in\nlet rowConverter = function(d) {\n    return {\n        id : d.id,\n        //y is going to be the log difference R-L\n        logratio : Math.log(parseFloat(d.Telencephalon_R)) - Math.log(parseFloat(d.Telencephalon_L)),\n        //x is going to be the average log \n        loggm : (Math.log(parseFloat(d.Telencephalon_L)) + Math.log(parseFloat(d.Telencephalon_R))) * .5\n    };\n    }\n\n//the location where I'm pulling the csv from\nlet dataloc = \"https://raw.githubusercontent.com/smart-stats/advanced_ds4bio_book/main/qbook/assets/kirby_pivot.csv\"\n\n//read in the data and parse the rows \nkirby_pivot = d3.csv(dataloc, rowConverter)\nModern js uses something called ‘promises’, which alllows for asynchronous evaluation. When we read in our csv file, it gets created as a promise and not an array like we need. The result is that our plotting commands need to then be called as a method from the promise object. The reason for this is so that it only uses the data when the data is actually loaded (i.e. promise fulfilled.) So, the plotting commmands for us look like this.\nkirby_pivot.then(dat => {\n    PLOTTING COMMANDS\n})\nJust a reminder that the notation d => g(d) is JS shorthand for function(d) {return g(d);} and is used heavily in d3 coding. Now let’s fill in PLOTTING COMMANDS. First, let’s fill in some utility functions. We get the range of our x and y values to help set up our axes. d3 scales map our function values to a range we want. So let’s create scale maps for x, y and color and then also set up axes using those scales. We’ll also go ahead on plot our axes so they’re on the bottom.\nmaxx = d3.max(dat, d => d.loggm)\nminx = d3.min(dat, d => d.loggm)\nmaxy = d3.max(dat, d => d.logratio)\nminy = d3.min(dat, d => d.logratio)\n\n//fudge is the boundary otherwise points get chopped off\nlet fudge = 50\n\nlet yScale = d3.scaleLinear()\n    .domain([miny, maxy])\n    .range([h-fudge, fudge])\n\nlet pointScale = d3.scaleLinear()\n    .domain([miny, maxy])\n    .range([5, 10])\n\nlet colorScale = d3.scaleLinear()\n    .domain([miny, maxy])\n    .range([0, 1])\n\n\nlet xScale = d3.scaleLinear()\n    .domain([minx, maxx])\n    .range([w-fudge, fudge]);\n\n// define the axes\nlet xaxis = d3.axisBottom().scale(xScale)\nlet yaxis = d3.axisLeft().scale(yScale)\nsvg.append(\"g\")\n    .attr(\"class\", \"axis\")\n    .attr(\"transform\", \"translate(0,\" + (h - fudge) + \")\")\n    .call(xaxis)\n\nsvg.append(\"g\")\n    .attr(\"class\", \"axis\")\n    .attr(\"transform\", \"translate(\" + fudge + \",0)\")\n    .call(yaxis)\nNow let’s create the plot. We’re going to add circles at each location, which is attributes cx and cy. Notice we use our previous defined scales to give their locations. Also, we’ll set the color and size relative to the logratio. Finally, when we mouseover a point, let’s change the radius then change it back when we mouseoff.\nsvg.selectAll(\"circle\")\n    .data(dat)\n    .enter()\n    .append(\"circle\")\n    .attr(\"cy\", d => yScale(d.logratio))\n    .attr(\"cx\", d => xScale(d.loggm))\n    .attr(\"r\",  d => pointScale(d.logratio))\n    .attr(\"fill\", d => d3.interpolateWarm(colorScale(d.logratio)))\n    .attr(\"stroke\", \"black\")\n    .on(\"mouseover\", function() {\n        d3.select(this)\n            .attr(\"r\", 30)\n        })\n    .on(\"mouseout\", function() {\n        d3.select(this)\n        .attr(\"r\",  d => pointScale(d.logratio))\n    })\nObviously, this is a lot of work for a simple scatterplot. The difference is that here you have total control over plotting and interactivity elements."
  },
  {
    "objectID": "interactive.html#observable-and-observable-plot",
    "href": "interactive.html#observable-and-observable-plot",
    "title": "2  Interactive graphics",
    "section": "2.4 Observable and Observable Plot",
    "text": "2.4 Observable and Observable Plot\nObserverable is a notebook for working with d3. It’s quite neat since mixing javascript coding in a web notebook, which itself is written in javascript, makes for an interesting setup. Typically, one would do the data preprocessing in R, python, julia … then do the advanced graphing in d3. In addition to accepting d3 as inputs, observable has a slightly higher set of utility functions called observable plot. (Quarto, which this document is in, allows for observable cells.) So, let’s read in some ROI data and plot it in observable plot. Note this is the average of the Type I Level I ROIs. Notice this is much easier than using d3 directly.\n\ndata = FileAttachment(\"assets/kirby_avg.csv\").csv();\nPlot.plot({\nmarks: [Plot.barY(data, {x: \"roi\", y: \"volume\", fill : 'roi'})],\n    x: {tickRotate: 45},\n    color: {scheme: \"spectral\"},    \n    height: 400,\n    width: 400,\n    marginBottom: 100\n\n})"
  },
  {
    "objectID": "interactive.html#links",
    "href": "interactive.html#links",
    "title": "2  Interactive graphics",
    "section": "2.5 Links",
    "text": "2.5 Links\n\nObservable is not javascript\nd3 tutorial.\nd3 gallery"
  },
  {
    "objectID": "interactive.html#homework",
    "href": "interactive.html#homework",
    "title": "2  Interactive graphics",
    "section": "2.6 Homework",
    "text": "2.6 Homework\n\nCreate a D3 graphic web page that displays a scatterplot of your chosing. Show point information on hover.\nOn the same web page, create a D3 graphic web page that displays a stacked bar chart for the Kirby 21 data. Hover data should show subject information and increase the size of the bar. Here’s a plotly version to get a sense.\n\n\nimport pandas as pd\nimport plotly.express as px\nimport numpy as np\ndat = pd.read_csv(\"https://raw.githubusercontent.com/smart-stats/ds4bio_book/main/book/assetts/kirby21.csv\").drop(['Unnamed: 0'], axis = 1)\ndat = dat.assign(id_char = dat.id.astype(str))\nfig = px.bar(dat, x = \"id_char\", y = \"volume\", color = \"roi\")\nfig.show()\n\n\n                                                \n\n\n\nSubmit your webpages and all supporting code to your assignment repo\nHere’s a hint to the HW in d3/hwHint.html"
  },
  {
    "objectID": "webscraping.html",
    "href": "webscraping.html",
    "title": "3  Advanced web scrapping",
    "section": "",
    "text": "Before you start webscraping make sure to consider what you’re doing. Does your scraping violate TOS? Will it inconvenience the site, other users? Per Uncle Ben: WGPCGR.\nAlso, before you begin web scraping, look for a download data option or existing solution. Probably someone has run up against the same problem and worked it out. For example, we’re going to scrape some wikipedia tables, which there’s a million other solutions for, including a wikipedia api."
  },
  {
    "objectID": "webscraping.html#basic-web-scraping",
    "href": "webscraping.html#basic-web-scraping",
    "title": "3  Advanced web scrapping",
    "section": "3.2 Basic web scraping",
    "text": "3.2 Basic web scraping\nLet’s show an example of static page parsing. Consider scraping the table of top 10 heat waves from wikipedia. First, we open the url, then parse it using BeautifulSoup, then load it into a pandas dataframe.\n\nfrom urllib.request import urlopen\nfrom bs4 import BeautifulSoup as bs\nimport pandas as pd\nurl = \"https://en.wikipedia.org/wiki/List_of_natural_disasters_by_death_toll\"\nhtml = urlopen(url)\nparsed = bs(html, 'html.parser').findAll(\"table\")\npd.read_html(str(parsed))[11]\n\n\n\n\n\n  \n    \n      \n      Rank\n      Death toll\n      Event\n      Location\n      Date\n    \n  \n  \n    \n      0\n      1.0\n      72000\n      2003 European heat wave\n      Europe\n      2003\n    \n    \n      1\n      2.0\n      56000\n      2010 Russian heat wave\n      Russia\n      2010\n    \n    \n      2\n      3.0\n      53000\n      2022 European heat waves\n      Europe\n      2022\n    \n    \n      3\n      4.0\n      41,072[42]\n      1911 French heat wave\n      France\n      1911\n    \n    \n      4\n      5.0\n      9500\n      1901 eastern United States heat wave\n      United States\n      1901\n    \n    \n      5\n      6.0\n      5,000–10,000\n      1988–1990 North American drought\n      United States\n      1988\n    \n    \n      6\n      7.0\n      3951\n      2019 European heat waves\n      Europe\n      2019\n    \n    \n      7\n      8.0\n      3,418[43]\n      2006 European heat wave\n      Europe\n      2006\n    \n    \n      8\n      9.0\n      2,541[43]\n      1998 Indian heat wave\n      India\n      1998\n    \n    \n      9\n      10.0\n      2500\n      2015 Indian heat wave\n      India\n      2015\n    \n  \n\n\n\n\nThe workflow as as follows:\n\nWe used the developer console on the webpage to inspect the page and its properties.\nWe opened the url with urlopen\nWe parsed the webpage with BeautifulSoup then used the method findAll on that to search for every table\nPandas has a utility that converts a html tables into a dataframe. In this case it creates a list of tables, where the 12th one is the heatwaves. Note it needs the data to be converted to a string before proceeding.\n\nThis variation of web scraping couldn’t be easier. However, what if the content we’re interested in only exists after interacting with the page? Then we need a more sophisticated solution."
  },
  {
    "objectID": "webscraping.html#form-filling",
    "href": "webscraping.html#form-filling",
    "title": "3  Advanced web scrapping",
    "section": "3.3 Form filling",
    "text": "3.3 Form filling\nWeb scraping can require posting to forms, such as logins. This can be done directly with python / R without elaborate programming, for example using the requests library. However, make sure you aren’t violating a web site’s TOS and also make sure you’re not posting your password to github as you commit scraping code. In general, don’t create a security hole for your account by web scraping it. Again, also check to make sure that the site doesn’t have an API with an authentication solution already before writing the code to post authentication. Many websites that want you to programmatically grab the data build an API."
  },
  {
    "objectID": "webscraping.html#programmatically-web-browsing",
    "href": "webscraping.html#programmatically-web-browsing",
    "title": "3  Advanced web scrapping",
    "section": "3.4 Programmatically web browsing",
    "text": "3.4 Programmatically web browsing\nSome web scraping requires us to interact with the webpage. This requires a much more advanced solution where we programmatically use a web browser to interact with the page. I’m using selenium and chromedriver. To do this, I had to download chromedriver and set it so that it was in my unix PATH.\n\nfrom selenium import webdriver\ndriver = webdriver.Chrome()\ndriver.quit()\n\nIf all went well, a chrome window appeared then closed. That’s the browser we’re going to program. If you look closely at the browser before you close it, there’s a banner up to that says “Chrome is being controlled by automated test software.” Let’s go through the example on the selenium docs here. First let’s vist a few pages. We’ll go to my totally awesome web page that I meticulously maintain every day then duckduckgo. We’ll wait a few seconds in between. My site is created and hosted by google sites, which seems reasonable that they would store a cookie so that I can log in and edit my site (which I almost never do). Duckduckgo is a privacy browser, so let’s check to see if they create a cookie. (Hint, I noticed that selenium doesn’t like redirects, so use the actual page url.)\n\ndriver = webdriver.Chrome()\ndriver.get(\"https://sites.google.com/view/bcaffo/home\")\nprint(driver.get_cookies())\ndriver.implicitly_wait(5)\n## Let's get rid of all cookies before we visit duckduckgo\ndriver.delete_all_cookies()\ndriver.get(\"https://duckduckgo.com/\")\nprint(driver.get_cookies())\n\nFor me, at least, this prints out the cookie info for my google site then nothing for ddg. (I’m not evaluating the code in quarto since I don’t want to bring up the browser when I compile the document.)\nNow let’s find the page elements that we’d like to interact with. There’s a text box that we want to submit a search command into and a button that we’ll need to press. When I go to ddg and press CTRL-I I find that the search box is:\n<input id=\"search_form_input_homepage\" class=\"js-search-input search__input--adv\" type=\"text\" autocomplete=\"off\" name=\"q\" tabindex=\"1\" value=\"\" autocapitalize=\"off\" autocorrect=\"off\" placeholder=\"Search the web without being tracked\">\nNotice, the name=\"q\" html name for the search form. When I dig around and find the submit button, it’s code is:\n<input id=\"search_button_homepage\" class=\"search__button  js-search-button\" type=\"submit\" tabindex=\"2\" value=\"S\">\nNotice its id is search_button_homepage. Let’s find these elements.\n\nsearch_box = driver.find_element(by=By.NAME, value=\"q\")\nsearch_button = driver.find_element(by=By.ID, value=\"search_button_homepage\")\n\nNow let’s send the info and press submit\n\nsearch_box.send_keys(\"Selenium\")\nsearch_button.click()\ndriver.implicitly_wait(10)\ndriver.save_screenshot(\"assets/images/webscraping.png\")\npage_source = driver.page_source\ndriver.close()\n\nHere, we saved the page_source as a variable that then can be parsed with other html parses (like bs4). Play around with the methods associated with driver and navigate the web. You’ll see that selenium is pretty incredible. Here’s the screenshot that we took:\n\n\n\nScreenshot of webscraping"
  },
  {
    "objectID": "webscraping.html#homework",
    "href": "webscraping.html#homework",
    "title": "3  Advanced web scrapping",
    "section": "3.5 Homework",
    "text": "3.5 Homework\n\nWrite a function that takes a search term, enters it into this link and returns the number of characters from the output.\nWrite a function that solves THE MAZE and returns your current location at its solution"
  },
  {
    "objectID": "images.html",
    "href": "images.html",
    "title": "4  Working with images",
    "section": "",
    "text": "Images broadly come in two types, vector and raster. Vector graphics are in formats like pdf, eps, svg and raster graphics are like jpeg, gif, png. Vector graphics store the image constructs and shapes. So, a vector graphics renderer can zoom in indefinitely on a shape and its edges will appear sharp. Vector fonts work this way. Raster graphics basically store a matrix and the pixels on the screen show the values of that matrix. Bitmapped fonts work this way. Of course, vector graphics have to be converted to raster to be actually displayed by the computer. Finally, some rater graphics formats have compression, which we won’t really discuss."
  },
  {
    "objectID": "images.html#working-with-raster-graphics",
    "href": "images.html#working-with-raster-graphics",
    "title": "4  Working with images",
    "section": "4.2 Working with raster graphics",
    "text": "4.2 Working with raster graphics\nRaster images are typically stored as an array. Grayscale images are matrices with the image intensity as the value and color pictures are stored as 3D arrays with the two main dimensions and color channels. A library for working with regular images in python is called PIL.\nThere are different raster specifications. RGB has 3 color channels, red, green and blue. CMYK has four: cyan, magenta, yellow and black. It’s interesting to note that the use of color channels existed before color cameras, when photographers would use different filters and additive and subtractive processes. The photograph below was created in 1877 by Louis Ducos du Hauron.\n\n\n\nColor image\n\n\nReading and working with images in python is quite easy because of the Python Image Library (PIL).\n\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nimg = Image.open(\"assets/images/Duhauron1877.jpg\")\n## You can see it with this, or img.show()\nimg\n\n\n\n\nPIL objects come with a ton of methods. For example, if we want to know whether we have an RGB or CMYK image, just print its mode.\n\nprint(img.mode)\n\nRGB\n\n\n\nr, g, b = img.split()\n\nplt.figure(figsize=(10,4));\nplt.subplot(1, 3, 1);\nplt.axis('off');\nplt.imshow(r);\n\nplt.subplot(1, 3, 2);\nplt.axis('off');\nplt.imshow(g);\n\nplt.subplot(1, 3, 3);\nplt.axis('off');\nplt.imshow(b);\n\n\n\n\nIf you’re tired of working with the image as a PIL object, it’s easy to convert to a np array.\n\nimg_array = np.array(img)\nimg_array.shape\n\n(1132, 1548, 3)\n\n\nBefore we leave PIL, it should be said that most image operations can be done in it. For example, cropping.\n\nbbox = [500, 630, 700, 760]\ncropped = img.crop(bbox)\ncropped\n\n\n\n\nWe can rotate the house and put it back\n\nrot = cropped.transpose(Image.Transpose.ROTATE_180)\nrot\n\n\n\n\n\n##Note this overwrites the image\nimg.paste(rot, bbox)\nimg"
  },
  {
    "objectID": "images.html#image-mathematics",
    "href": "images.html#image-mathematics",
    "title": "4  Working with images",
    "section": "4.3 Image mathematics",
    "text": "4.3 Image mathematics\n\n4.3.1 Convolutions\n\n4.3.1.1 1D transforms\nConvolutions are an important topic in mathematics, statistics, signal processing … Let’s discuss 1D convolutions first. A real valued convolution of two continuous signals, \\(X(t)\\) and \\(K(t)\\) is defined as \\(X* K\\)\n\\[\n(X* K)(t) = \\int_{-\\infty}^{\\infty} X(u) K(t-u) du\n= \\int_{-\\infty}^{\\infty} X(t-v) K(v) dv,\n\\]\nwhere the equality is determined by a simple change of variable argument. The discrete analog is\n\\[\n(X* K)(t) = \\sum_{u = -\\infty}^{\\infty} X(u) K(t-u)\n= \\sum_{v = -\\infty}^{\\infty} X(t-v) K(v)\n\\]\nThe convolution has many, many uses in data science and statistics. For example, the convolution of densities or mass functions is the respective density or mass function for the sum of random variables from those distributions. In applied data analysis, you can think of the convolution between \\(X\\) and \\(K\\) as smearing the function \\(K\\) over the function \\(X\\). Thus, it plays a key role in smoothing. Let’s try an example using the covid data and a box kernel. We take \\(K(t) = I\\{0 \\leq t < M\\} / M\\) (i.e. is 1 for times 0 to \\(M-1\\), then rescaled so it sums to 1). Assume that \\(N\\geq M\\) and that \\(X(t)\\) and \\(K(t)\\) are \\(0\\) and for \\(t < 0\\) or \\(t > N\\). Then, our convolution works out to be\n\\[\n(X* K)(t)\n= \\sum_{u = -\\infty}^{\\infty} X(u) K(t-u)\n= \\sum_{u = 0}^{N} X(u) K(t-u)\n= \\sum_{u = t}^{t + M - 1} X(u) K(t -u)\n= \\sum_{u = t}^{t + M - 1} X(u) / M\n\\]\nThat is, our convolution is a moving average of \\(X\\) where the convolution at point \\(t\\) is the average of the points between \\(t\\) and \\(t + M - 1\\). So, the convolution, as we’ve defined it, at point \\(t\\) is the moving average at point \\(t + (M-1)/2\\) (ie. it’s shifted by \\((M-1)/2\\)). Also, at the end (\\(t \\geq N - M + 1\\)), we’re averaging in the assumed zero values of the \\(X\\). This might be reasonable to do, or maybe not. The fact that we’re padding the end and not the beginning is just because of the range of index values we defined the kernel on. We’d have the same problem only on the other end if \\(K(t) = I(-M < t \\leq 0)/M\\). Of course, the computer will start summing things at index 0 regardless. However, it can shift the kernel relative to the signal arbitrarily by zero padding one end or the other or both. A reasonable strategy is to set it so that it averages in \\((M-1)/2\\) on both ends. Numpy allows you to only look at the range of \\(N - M\\) middle values where this isn’t an issue (argument mode = \"valid\").\nNote we could make the kernel weight points differently than just a box kernel. A popular choice is a Gaussian distribution.\nAlso, the convolution has \\(N+M-1\\) points. So, it has more time points than the original signal. Numpy has options to shift the convolution back into the same space as the original signal for you (i.e. has \\(N\\) points, mode = \"same\"). Or, you can just do it yourself if you do mode = \"full\", just shift by \\((M-1)/2\\). Similarly shift for mode = \"valid\" (but the convolution has fewer points in this case, so it won’t have corresponding points with \\(X\\) at the very beginning and end).\nHere’s an example using Italy’s daily covid case count data. We plot the data and the convolution smoothed data. In the bottom panels, we show the residuals to highlight the difference.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\ndat = pd.read_csv('https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv')\n## Get Italy, drop everyrthing except dates, convert to long (unstack converts to tuple)\nX = dat[dat['Country/Region'] == 'Italy'].drop([\"Province/State\", \"Country/Region\", \"Lat\", \"Long\"], axis=1).unstack()\n## convert from tuple to array\nX = np.asarray(X)  \n## get case counts instead of cumulative counts\nX = X[1 : X.size] - X[0 : (X.size - 1)]\n## get the first non zero entry\nX =  X[np.min(np.where(X !=  0)) : X.size]\nplt.plot(X)\n\n\n\n\nNow let’s plot the convolutions with different options in np.convolve.\n\n## 41 day moving average\nN = len(X)\nM = 41\n\nfig, axes = plt.subplots(2, 3, figsize = [12.4, 12.4])\naxes[0,0].plot(X)\naxes[0,1].plot(X)\naxes[0,2].plot(X)\n\nK = np.ones(M) / M\n\n## Plot the convolution with the argument 'same'\n## this gives N (assumed greater than M) points\nXC = np.convolve(X, K, 'same')\naxes[0,0].plot(XC)\naxes[1,0].plot(X - XC)\n\n## Plot the convolution with the argument 'full'\n## which gives N+M-1 total pionts\nXC = np.convolve(X, K, 'full')\ntemp = np.pad(X, (M-1, 0), 'constant') \naxes[0,1].plot(XC)\naxes[1,1].plot(temp- XC)\n\n\n## Plot the convolution with the convolution shifted back by (M-1)/2\nXCshifted = XC[ (int((M - 1)/2)) : int(len(XC) - (M - 1)/2) ]\naxes[0, 2].plot(XCshifted)\naxes[1, 2].plot(X - XCshifted)\n## 41 day moving average\nN = len(X)\nM = 41\n\nfig, axes = plt.subplots(2, 3, figsize = [12.4, 12.4])\naxes[0,0].plot(X)\naxes[0,1].plot(X)\naxes[0,2].plot(X)\n\nK = np.ones(M) / M\n\n## Plot the convolution with the argument 'same'\n## this gives N (assumed greater than M) points\nXC = np.convolve(X, K, 'same')\naxes[0,0].plot(XC)\naxes[1,0].plot(X - XC)\n\n## Plot the convolution with the argument 'full'\n## which gives N+M-1 total pionts\nXC = np.convolve(X, K, 'full')\ntemp = np.pad(X, (M-1, 0), 'constant') \naxes[0,1].plot(XC)\naxes[1,1].plot(temp- XC)\n\n\n## Plot the convolution with the convolution shifted back by (M-1)/2\nXCshifted = XC[ (int((M - 1)/2)) : int(len(XC) - (M - 1)/2) ]\naxes[0, 2].plot(XCshifted)\naxes[1, 2].plot(X - XCshifted)\n\n\n\n\n\n\n\nLet’s show that the first point and end point of the convolution are the averages of \\((M-1)/2\\) points and and \\((M-1)/2+1\\) zeros at the beginning or end of the original signal just to show that our intuition is correct.\n\ntemp = np.convolve(X, K, 'same')\n[\n  # the first convolution point (temp[0]) and the average of the\n  # the first (M-1) / 2 X points and (M-1)/2 + 1 zeros\n  [temp[0],     X[0 : int(    (M - 1) / 2)].sum() / M],\n  # the last convolution point (temp[N-1]) and the average of the\n  # the last (M-1) / 2 X points and (M-1)/2 + 1 zeros\n  [temp[N - 1], X[int(N - (M - 1) / 2 - 1)  : N].sum() / M]\n \n]\n\n[[0.07317073170731708, 0.07317073170731707],\n [11588.219512195123, 11588.219512195123]]\n\n\nAlso, I averaged a lot (41 days) in order to make the shift very apparent. Let’s look at the performance for less wide of a kernel.\n\n## 21 day moving average\nM = 21\nK = np.ones(M) / M\n\nfig, axes = plt.subplots(1, 2, figsize = [12.4, 6.2])\nXC = np.convolve(X, K, 'same')\naxes[0].plot(X)\naxes[0].plot(XC)\naxes[1].plot(X - XC)\n\n\n\n\nIt should be stated that the convolution operation is multiplication in Fourier space. So, functions like np.convolve are performing FFTs in the background. However, if you’re going to do this yourself, make sure to keep track of indices and zero padding. (I.e. the bookkeeping.) Otherwise, the FFT wraps around and you get a little of the end averaged in with the beginning and vice versa. I work out getting the same answer as mode = “same” below.\n\nfig, axes = plt.subplots(1, 2, figsize = [12.4, 6.2])\n\n## Pad the X with zeros in the back, need at least M-1 \npad_width = (0, M - 1)\nXpadded = np.pad(X, pad_width, \"constant\")\n## Pad the kernel in the back with N-1, so both the kernel\n## and the X are of length, N+M-1\nKpadded = np.pad(K, (0, N - 1))\n\n## Note we take the real part b/c the complex part is all effectively \n## machine 0\nconvolution = np.fft.ifft(np.fft.fft(Xpadded) * np.fft.fft(Kpadded)).real\n\n## At this point the convolution is of length N + M - 1\n## To get it comparable with the original X, subtract (M-1)/2 indices\n## from each end\nconvolution = convolution[ int((M-1)/2) : int(N+(M-1)/2)]\n\n## Let's see how we did\naxes[0].plot(X)\naxes[0].plot(convolution)\n\n#Show they're the same by plotting the subtraction\naxes[1].plot(convolution - XC)\n\n\n\n\n\n\n4.3.1.2 2D transforms\nFor two dimensions, the convolution is similar\n\\[\n(X ** K)(i,j) = \\sum_{u=-\\infty}^{\\infty} \\sum_{v=-\\infty}^{\\infty}\nX(u, v)  K(i -u, k - v) = \\sum_{u=-\\infty}^{\\infty} \\sum_{v=-\\infty}^{\\infty}\nK(u, v)  X(i -u, k - v)  \n\\]\nOnce again, let’s think where \\(X\\) is of dimension \\((N_1, N_2)\\) and 0 outside of that range, and\n\\[\nK(u, v) = I(0 \\leq u < M_1, 0 \\leq v < M_2) / (M_1 M_2)\n\\]\n(i.e. \\(K\\) is a box on \\(M_1 \\leq N_1\\), \\(M_2 < N_2\\)). Then, applying the exact same argument as before, the convolution is:\n\\[\n(X ** K)(i,j) = \\sum_{u=i}^{M_1 + i - 1} \\sum_{v=j}^{M_2 + j - 1}\nX(u, v) / (M_1 M_2)\n\\]\nThat is, the convolution at point \\((i,j)\\) is the average of the neighboring points. Also, all of the same bookkeeping, zero padding and Fourier transform stuff apply (using the 2D FFT).\nFor regular kernels (box kernels, 2D Gaussians), convolution smooths the image, which has the efffect of making it blurrier. The kernel width determines how blurry the image will then be. This is typically done to denoise an image (to blur out the noise). Let’s try it on a cartoon image of Brian. We’ll just stick to a black and white image so that it’s 2D. A color image has 3 color channels, so is a 3D array. (However, you see the patten; you should be able to extend this to 3D with little problem.)\n\nimport PIL\nimport scipy.signal as sp\nimport urllib.request\n\n\nimgURL = \"https://github.com/smart-stats/ds4bio_book/raw/main/book/bcCartoon.png\"\nurllib.request.urlretrieve(imgURL, \"bcCartoon.png\")\nimg = np.asarray(PIL.Image.open(\"bcCartoon.png\").convert(\"L\"))\n\nplt.xticks([])\nplt.yticks([])\nplt.imshow(img, cmap='gray', vmin=0, vmax=255)\n\n<matplotlib.image.AxesImage at 0x7b75f8fb5e70>\n\n\n\n\n\nNow let’s take this image and convolve it with different kernels of different window lengths.\n\ndef kernel(i, j):\n  return np.ones((i, j)) / np.prod([i, j])\n\nplt.figure(figsize=[12.4, 12.4])\nimgC = sp.convolve2d(img, kernel(4, 4))\nplt.subplot(2, 2, 1)\nplt.xticks([])\nplt.yticks([])\nplt.imshow(imgC, cmap='gray', vmin=0, vmax=255)\nplt.title(\"4x4\")\n\nimgC = sp.convolve2d(img, kernel(8, 8))\nplt.subplot(2, 2, 2)\nplt.xticks([])\nplt.yticks([])\nplt.imshow(imgC, cmap='gray', vmin=0, vmax=255)\nplt.title(\"8x8\")\n\nimgC = sp.convolve2d(img, kernel(16, 16))\nplt.subplot(2, 2, 3)\nplt.xticks([])\nplt.yticks([])\nplt.imshow(imgC, cmap='gray', vmin=0, vmax=255)\nplt.title(\"16x16\")\n\nboxsize = (5, 5)\nimgC = sp.convolve2d(img, kernel(32,32))\nplt.subplot(2, 2, 4)\nplt.xticks([])\nplt.yticks([])\nplt.imshow(imgC, cmap='gray', vmin=0, vmax=255)\nplt.title(\"32x32\")\n\nText(0.5, 1.0, '32x32')\n\n\n\n\n\n\n\n4.3.1.3 Convolutional neural networks\nOf course, your kernel doesn’t have to be a box, or a truncated, discretized bivariate Gaussian density or even be non-negative. It’s helpful for smoothers to have non-negative kernels, since they’re just taking a generalized variation of a moving average that way. But, we want to use convolutions\nmore generally. Here, let’s take a kernel that is part of the image (left eye) and convolve it. I’ll make the kernel super peaked at eye features by extracting the eye and raising it to the 4th power.\nSo a relu activation function plus a bias term would then be able to highlight different thresheld variations of this convolution image. For example, here I add a bias term to the convolution then apply a leaky relu. You can see it just highlights the one area where the eye is. A leaky relu is\n\\[\nlrelu(x, c) = \\left\\{\n  \\begin{array}{ll}\n  x & \\text{if $x > 0$} \\\\\n  x * c & \\text{otherwise}\n  \\end{array}\n  \\right.\n\\]\nwhere \\(c\\) is usually set to a small value. If \\(c=0\\) the leaky relu is just the relu. I set \\(c\\) to be 0.05 so that we can see the background image.\n\nplt.figure(figsize=[12.4, 6.2])\n\nK = img[200 : 270,225 : 322]\nplt.subplot(1, 3, 1)\nplt.xticks([])\nplt.yticks([])\nplt.imshow(K,  cmap='gray', vmin=0, vmax=255)\n## I normalized it this way so that the convolution\n## numbers wouldn't be so big\n## Also, I put it to the 4th power, so it exactly finds \n## the eye.\nK = K ** 4\nK = K / K.sum()\nK = K - K.mean()\n\nimgC = sp.convolve2d(img, K)\nplt.subplot(1, 3, 2)\nplt.xticks([])\nplt.yticks([])\nplt.imshow(imgC)\nplt.title(\"Convolution\")\n\ntemp = imgC.copy()\n## Add a bias term of -15\ntemp -= 15\n## Perform a leaky relu\ntemp[np.where(temp < 0)] = temp[np.where(temp < 0)] * .05\n\nplt.subplot(1, 3, 3)\nplt.imshow(temp)\nplt.xticks([])\nplt.yticks([])\nplt.title(\"LRELU of convolution + bias\")\n\nText(0.5, 1.0, 'LRELU of convolution + bias')\n\n\n\n\n\nBecause of how convolutions work, this will find this eye anywhere in the image. Here we just add another eye somewhere else and repeat the convolution.\n\nplt.figure(figsize=[12.4, 6.2])\n\n#put another eye in the image\nimgCopy = img.copy()\nimgCopy[60 : 130, 85 : 182] = img[200 : 270,225 : 322]\nplt.subplot(1, 2, 1)\nplt.imshow(imgCopy,  cmap='gray', vmin=0, vmax=255)\nplt.xticks([])\nplt.yticks([])\n\nimgC = sp.convolve2d(imgCopy, K)\n\nplt.subplot(1, 2, 2)\ntemp = imgC.copy()\n## Add a bias term of -15\ntemp -= 15\n## Perform a leaky relu\ntemp[np.where(temp < 0)] = temp[np.where(temp < 0)] * .05\n\nplt.subplot(1, 2, 2)\nplt.imshow(temp)\nplt.xticks([])\nplt.yticks([])\nplt.title(\"LRELU of convolution + bias\")\n\nText(0.5, 1.0, 'LRELU of convolution + bias')\n\n\n\n\n\nSo, we found a custom kernel that highlights this specific feature in images. Convnets layers learn the kernel. That is, CNNs learn the image that gets convolved with the previous layer to produce the next one. Here’s a really great pictorial guide by Sumit Saha.\nNow, let’s discuss some specific vocabulary used in CNNs.\n\nPadding zero padding just like we discussed for 1D transformations\nPooling pooling, often max pooling, is a dimension reduction technique, taking the max in little blocks.\nstride length instead of sliding the kernel by moving it one pixel at a time, move it more to increase computational efficiency and reduce the size of the output convolution."
  },
  {
    "objectID": "databases.html",
    "href": "databases.html",
    "title": "5  Databases",
    "section": "",
    "text": "You’ve probably already learned about some variation of databases, either sql, nosql, spark, a cloud db, … Often, the backend of these databases can be quite complicated, while the front end requires SQL querries or something similar. We’ll look at a non-relational database format that is specifically useful for scientific computing called hdf5. HDF5 has implementations in many languages, but we’ll look at python. This is a hierarchical data format specifically useful for large array calculations.\nLet’s create a basic h5py file. First, let’s load our stuff.\nNow, let’s create an empty hdf5 file. Here’s the basic code; the option w is open for writing. There’s also w-, r, r+, a for write protected, read only, read/write, read/write and create. The first time I ran it I used:\nThen, subsequently\nNow let’s populate it with some data. The hdf5 file works almost like a directory where we can store hierarchical data. For example, suppose that we want sensors stored in a superstructure called sensors and want to fill in the data for sensor1 and sensor1.\nNow we can do normal np stuff on this sensor. However, hdf5 is only bringing in the part that we are using into memory. This allows us to work with very large files. Also, as we show here, you can name the data to a variable since that’s more convenient."
  },
  {
    "objectID": "databases.html#blockwise-basic-statistical-calculations",
    "href": "databases.html#blockwise-basic-statistical-calculations",
    "title": "5  Databases",
    "section": "5.1 Blockwise basic statistical calculations",
    "text": "5.1 Blockwise basic statistical calculations\nNow, consider taking the mean of both variables. Imagine that the time series is so long it’s not feasible to load into memory. So, we want to read it in blocks. You want your blocks to be as big as possible, since that’s fastest. In our case, of course, none of this is necessary.\nOur goal in this section is to do the following: calculate the empirical mean and variance for each sensor, center and scale each sensor, and write those changes to those variables, calculate the sample correlation then calculate the residual for sensor1 given sensor2. (I think typically you wouldn’t want to overwrite the original data; but, this is for pedagogical purposes.) We want our data organized so sensors are stored in a hierarchical “folder” called sensors and processed data is in a different folder.\nWe’re just simulating iid standard normals. So, we have a rough idea of the answers we should get, since the the data are theoretically mean 0, variance 1 and uncorrelated. After our calculations, they will have empirical mean 0 and variance 1 and the empirical correlation between the residual and sensor 2 will be 0.\nLet’s consider a block variation of the inner product. \\[\n<a, b> = \\sum_{i=0}^{n-1} a_i b_i = \\sum_{i=0}^{n/B} \\sum_{j=0}^{B-1} a_{j + i B} b_{j + i B}\n\\] (if \\(n\\) is divisible by \\(B\\). Otherwise you have to figure out what to do with the final block, which isn’t hard but makes the notation messier.) So, for example, the (sample) mean is then \\(<x, J>/n\\) where \\(J\\) is a vector of ones.\nLet’s calculate the mean using blockwise calculations.\n\nn = s1.shape[0]\nB = 32\n## mean center the blocks\nmean1 = 0\nmean2 = 0\nfor i in range(int(n/B)):\n    block_indices = np.array(range(B)) + i * B\n    mean1 += s1[block_indices].sum() / n \n    mean2 += s2[block_indices].sum() / n\n\n[mean1, mean2]\n\n[-0.004707482011070242, 0.02597874768738295]\n\n\nLet’s now center our time series.\n\nfor i in range(int(n/B)):\n    block_indices = np.array(range(B)) + i * B\n    s1[block_indices] -= mean1  \n    s2[block_indices] -= mean2\n\nNow the (unbiased, sample) variance of centered vector \\(a\\) is simply \\(<a, a>/(n-1)\\).\n\nv1, v2 = 0, 0\nfor i in range(int(n/B)):\n    block_indices = np.array(range(B)) + i * B\n    v1 += np.sum(s1[block_indices] ** 2) / (n - 1)\n    v2 += np.sum(s2[block_indices] ** 2) / (n - 1)\n[v1, v2]\n\n[1.000799527252137, 1.0516175030507937]\n\n\nNow let’s scale our vectors as\n\nsd1 = np.sqrt(v1)\nsd2 = np.sqrt(v2)\nfor i in range(int(n/B)):\n    block_indices = np.array(range(B)) + i * B\n    s1[block_indices] /= v1  \n    s2[block_indices] /= v2\n\nNow that our vectors are centered and scaled, the empirical correlation is simply \\(<a, b>/(n-1)\\). Let’s do that\n\ncor = 0\nfor i in range(int(n/B)):\n    block_indices = np.array(range(B)) + i * B\n    cor += np.sum(s1[block_indices] * s2[block_indices]) / (n-1) \ncor\n\n-0.02051175073471629\n\n\nFinally, we want to “regress out” s2 from s1. Since we normalized our series, the correlation is slope coefficient from linear regression (regardless of the outcome and dependent variable) and the intercept is zero (since we centered). Thus, the residual we want is \\(e_{12} = s_1 - \\rho s_2\\) where \\(\\rho\\) is the correlation.\n\nf['processed/resid_s1_s2'] = np.empty(n)\ne12 = f['processed/resid_s1_s2']\nfor i in range(int(n/B)):\n    block_indices = np.array(range(B)) + i * B\n    e12[block_indices] += s1[block_indices] - cor * s2[block_indices] \n\nNow we have our new processed data stored in a vector. To close our database simply do:\n\nf.close()\n\nNow our processed data is stored on disk.\n\nf = h5py.File('sensor.hdf5', 'r')\nf['processed/resid_s1_s2']\n\n<HDF5 dataset \"resid_s1_s2\": shape (1024,), type \"<f8\">\n\n\n\nf.close()"
  },
  {
    "objectID": "databases.html#homework",
    "href": "databases.html#homework",
    "title": "5  Databases",
    "section": "5.2 Homework",
    "text": "5.2 Homework\n\nPerform lots of regressions. Suppose that you have a setting where you would like to perform the operation \\[\n(X'X)^{-1} X' Y\n\\] where \\(X\\) is \\(n\\times p\\) and \\(Y\\) is \\(n\\times v\\). Consider the case where \\(Y\\) is very large (so \\(V\\) is large). Simulate some data where you perform this linear model in block calculations.\nWrite a block matrix multiplication program that takes in two matrices with agreeable dimensions stored as HDF5 and multiplies them in block sizes specified by the user."
  },
  {
    "objectID": "pipelines.html",
    "href": "pipelines.html",
    "title": "6  Pipelines",
    "section": "",
    "text": "We’ll cover make here, which is less commonly used for data science, but is ubiquitously used for software development. A makefile simply says what tasks needs to be done to construct a project. Then, it only runs those tasks that need to be updated. Imagine something like the folowing. I have R code that creates a figure, that figure is required by a latex file.\nproject.pdf : rplots.pdf project.tex\n        pdflatex project.tex\n\nrplots.pdf : gen_rplots.R\n       R --no-save < gen_rplots.R\n\nclean :\n    rm rplots.pdf\n    rm project.pdf\n    rm project.log\nTyping make at the command like will make my file project.pdf. I can type make clean to remove my build files. I can also type make rplots.pdf to make just that file. Make uses timestamps to only update what is needed to be updated. So if I only change project.tex it won’t regenerate my rplots.pdf.\nWhy use something like make when we have rmarkdown, quarto …? The main reason is that make is completely general. You can have any command used in building and any type of output. This is why these kind of build utilities are so ubiquitously used in pipelining."
  },
  {
    "objectID": "data_structures.html",
    "href": "data_structures.html",
    "title": "7  Data structures",
    "section": "",
    "text": "The topic of data structures focuses on how we represent, access and store data, ideally in ways efficient for our purpose. This chapter is a low level teaser of data structures geared towards students who haven’t been exposed to the ideas much at all."
  },
  {
    "objectID": "data_structures.html#hash-tables",
    "href": "data_structures.html#hash-tables",
    "title": "7  Data structures",
    "section": "7.2 Hash tables",
    "text": "7.2 Hash tables\nA common starting point for data structures is a hash table. Suppose we want to store a set of values associated with a set of keys. Consider a list of some students and their PhD theses.\n\n\n\n\n\n\n\nkey\nvalue\n\n\n\n\nLeena\nModelling biomedical data and the foundations of bioequivalence\n\n\nXianbin\nModeling composite outcomes and their component parts\n\n\nShu-Chih\nStructure/function relationships in the analysis of anatomical and functional neuroimaging data\n\n\nHaley\nStatistical methods for inter-subject analysis of neuroscience data\n\n\nBruce\nFrom individuals to populations: application and insights concerning the generalized linear mixed model\n\n\n\nIf we stored these in an text array, say, and we wanted to look up Bruce’s thesis title, we’d have to check each key in turn until we arived at Bruce and then looked up his thesis. This has a worst case scenario of n operations. (Question, if we looked in the order of a random permutation, what is the expected number of lookups?)\nHash tables map the keys to a specific lookup number. Thus, when trying to find Bruce’s value, the has function would perform hash(\"Bruce\") to get the hash value, then to straight to that point in the array. Sounds great!\nThere are some details, of course. Most (all) programming languages have hash tables, or libraries that add on hash tables. For example, the dict structure in python. Since that exists, let’s work in R and create our own hash table.\nWe need a hash function. Let’s create one as the sum of its utf8 values\n\nhash = function(string, mod) sum(utf8ToInt(string)) %% mod\nhash(\"Bruce\", 5)\n\n[1] 2\n\nhash(\"Bruce2\", 5)\n\n[1] 2\n\nhash(\"Haley\", 5)\n\n[1] 4\n\n\nHere the mod is used to truncate our integer value so that our it fits in our list. In our case, let’s assume the list is of length no larger than 5. Ideally, you want you hash functions to have as few collisions, instances where different key texts give the same hash value, as possible. For our simple example, we’re not going to stress out over this much and our hash function is going to give lots of collisions. Let’s create an empty hash table\n\nhash_table = vector(mode = \"list\", length = 5)\n\nNow, let’s add an element\n\n##Note this operates on hash_table outside of the function\nadd_pair = function(key, value, hash_table){\n    n = length(hash_table)\n    new_entry = list(c(key, value))\n    hash_value = hash(key, n)\n    hash_entry = hash_table[[hash_value]]\n    if (is.null(hash_entry)){\n        hash_table[[hash_value]] = new_entry\n    }\n    else {\n        hash_table[[hash_value]] = c(hash_entry, new_entry)\n    }\n    return(hash_table)\n}\nhash_table = add_pair(\"Bruce\", \"From individuals to populations\", hash_table)\nhash_table = add_pair(\"Bruce2\", \"From individuals to populations2\", hash_table)\nhash_table = add_pair(\"Haley\", \"Statistical methods\", hash_table)\nhash_table\n\n[[1]]\nNULL\n\n[[2]]\n[[2]][[1]]\n[1] \"Bruce\"                           \"From individuals to populations\"\n\n[[2]][[2]]\n[1] \"Bruce2\"                           \"From individuals to populations2\"\n\n\n[[3]]\nNULL\n\n[[4]]\n[[4]][[1]]\n[1] \"Haley\"               \"Statistical methods\"\n\n\n[[5]]\nNULL\n\n\nA nefarious character named Bruce2 submitted an incremental thesis. But, alas, must go into the hash table. The keys Bruce and Bruce2 result in collisions from our hash function, keys that have the same hash value. We’re adopting a collision strategy where we just add collision keys in turn. We should also write some code that prevents us from adding the same exact key twice. As it stands we could add Bruce twice when keys need to be unique.\nLet’s write a retrieval function. It needs to find the appropriate hash value, then search within that list for the appropriate key.\n\nretrieve = function(key, hash_table){\n    n = length(hash_table)\n    hash_value = hash(key, n)\n    hash_entry = hash_table[[hash_value]]\n    ## If there's nothing there return null\n    if (is.null(hash_entry)){\n        return(NULL)\n    }\n    else {\n        keys = sapply(hash_entry, function(x) x[1])\n        key_test = key == keys\n        if (any(key_test)){\n            key_index = which(key_test) \n            return(hash_entry[[key_index]][2])\n        }\n        ## If the key isn't there return null\n        else return(NULL)\n    }\n}\nretrieve(\"Bruce\", hash_table)\n\n[1] \"From individuals to populations\"\n\nretrieve(\"Bruce2\", hash_table)\n\n[1] \"From individuals to populations2\"\n\nretrieve(\"bruce\", hash_table)\n\nNULL"
  },
  {
    "objectID": "diymlai.html",
    "href": "diymlai.html",
    "title": "8  DIY ML/AI",
    "section": "",
    "text": "Usually, the nodes are added in so called layers. \\((X_1, X_2)\\) is the input layer, \\((H_{11}, H_{12})\\) is the first hidden layer, \\((H_{21}, H_{22})\\) is the second hidden layer and \\(Y\\) is the output layer. Imagine plugging an \\(X_1\\) and \\(X_2\\) into this network. It would feed forward through the network as\n\\[\n\\begin{align}\nH_{11} = & g_1(W_{011} + W_{111} X_1 + W_{211} X_2) \\\\\nH_{12} = & g_1(W_{012} + W_{112} X_1 + W_{212} X_2) \\\\\nH_{21} = & g_2(W_{021} + W_{121} H_{11} + W_{221} H_{12}) \\\\\nH_{22} = & g_2(W_{022} + W_{122} H_{11} + W_{222} H_{12}) \\\\\n\\hat \\eta = & g_3(W_{031} + W_{131} H_{21} + W_{231} H_{22})\n\\end{align}\n\\]\nwhere \\(g_k\\) are specified activation functions and \\(\\eta\\) is our estimate of \\(Y\\). Typically, we would have a different activation function for the output layer than the others, and the other would have the same activation function. So, for example, if \\(Y\\) was binary, like hypertension diagnosis, then \\(g_1=g_2\\) and \\(g_3\\) would be a sigmoid.\nA typical activation function is a rectified linear unit (RELU), defined as \\(g(x) = x I(x > 0)\\). The neural network is typically fit via a gradient based method, such as gradient descent, assuming a loss function. The loss function is usually based on maximum likelihood.\nLet’s consider fitting the network above using gradient descent and obtaining the derivative via the chain rule. Consider the contribution of a row of data to a SE loss function, \\({\\cal L_i}(w) = (y_i - \\eta_i)^2\\), where \\(\\eta_i\\) is the feed forward of our neural network for row \\(i\\). Let’s look at the derivative with respect to \\(w_{111}\\) where we drop the subscript \\(i\\). First note that only these arrows involve \\(w_{111}\\)\n\n\n\n\n\n\\[\n\\frac{\\partial {\\cal L} }{\\partial w_{111}} =\n\\frac{\\partial {\\cal L} }{\\partial \\eta} \\frac{\\partial \\eta}{\\partial H_{2}}\n\\frac{\\partial H_2 }{\\partial H_{11}}\\frac{\\partial H_{11} }{\\partial w_{111}}\n\\]\nwhere \\(H_2 = (H_{21}, H_{22})^t\\). These parts are:\n\\[\n\\begin{aligned}\n\\frac{\\partial {\\cal L} }{\\partial \\eta} & =  -2 (Y - \\eta) \\\\\n\\frac{\\partial \\eta}{\\partial H_{2}} & = g_3'(W_{031} + W_{131} H_{21} + W_{231} H_{22}) (W_{131},  W_{231}) \\\\\n\\frac{\\partial H_2 }{\\partial H_{11}} &= [g_2'(W_{021} + W_{121} H_{11} + W_{221} H_{12}) W_{121},\n                                          g_2'(W_{022} + W_{122} H_{11} + W_{222} H_{12}) W_{122}]^t\\\\\n\\frac{\\partial H_{11} }{\\partial w_{111}} &= g_1'(W_{011} + W_{111} X_1 + W_{211} X_2) x_1\n\\end{aligned}\n\\]\nThese get multiplied together, using matrix multiplication when required, to form the first derivative for \\(W_{111}\\). This is repeated for all of the weight parameters. Notice this requires keeping track of which nodes have \\(w_{111}\\) in its parent chain and that it travels backwards through the network. For this reason, it is called backpropagation\nLet’s try coding it for this parameter. We’re going to create the model just hard coding the network.\n\nimport numpy as np\n\n## Define our activation function and its derivative\ng = lambda x : np.exp(x) / (1 + np.exp(x))\ng_deriv = lambda x: g(x) * (1 - g(x))\n\n## Here's one row of data\nY, X1, X2 = 100, 2, 3\n\n## Creating some random initialized weights\n## Adding to the dims so that the notation agrees\nW = np.random.normal( size = [3, 4, 3] )\n\nH11 = g(W[0,1,1] + W[1,1,1] * X1  + W[2,1,1] * X2)\nH12 = g(W[0,1,2] + W[1,1,2] * X1  + W[2,1,2] * X2) \nH21 = g(W[0,2,1] + W[1,2,1] * H11 + W[2,2,1] * H12) \nH22 = g(W[0,2,2] + W[1,2,2] * H11 + W[2,2,2] * H12) \nETA = g(W[0,3,1] + W[1,3,1] * H21 + W[2,3,1] * H22)\n\n## Our chain rule sequence of derivatives\nL = (Y - ETA) ** 2\n\n## Backprop calculating the derivatives\ndL_dETA  = -2 * (Y - ETA)\n\ndETA_dH2 = g_deriv(W[0,3,1] + W[1,3,1] * H21 + W[2,3,1] * H22) * np.array((W[1,3,1],  W[2,3,1]))\n\ndH2_dH11 = np.array( \n        ( g_deriv(W[0,2,1] + W[1,2,1] * H11 + W[2,2,1] * H12 ) * W[1,2,1], \n          g_deriv(W[0,2,2] + W[1,2,2] * H11 + W[2,2,2] * H12 ) * W[1,2,2] \n        ) \n)\n\ndH11_dW111 = g_deriv(W[0,1,1] + W[1,1,1] * X1 + W[2,1,1] * X2) * X1\n\n## Here's the backrpop in derivative calculation\ndL_dW111 = dL_dETA * np.sum(dETA_dH2 * dH2_dH11) * dH11_dW111\n\nprint(dL_dW111)\n\n## Let's approximate the derivative numerically\ne = 1e-6\n\n## Perturb W111 a little bit\nW[1,1,1] -= e\n\n## Feed forward through the network with the perturbed W111\nH11 = g(W[0,1,1] + W[1,1,1] * X1  + W[2,1,1] * X2)\nH12 = g(W[0,1,2] + W[1,1,2] * X1  + W[2,1,2] * X2) \nH21 = g(W[0,2,1] + W[1,2,1] * H11 + W[2,2,1] * H12) \nH22 = g(W[0,2,2] + W[1,2,2] * H11 + W[2,2,2] * H12) \nETA = g(W[0,3,1] + W[1,3,1] * H21 + W[2,3,1] * H22)\n\n## Calculate the new loss\nLe = (Y - ETA) ** 2\n\n## Here's the approximate derivative\nprint( (L - Le) / e )\n\n0.2528652632516764\n0.2528649929445237\n\n\nNow let’s calculate the derivative"
  },
  {
    "objectID": "convnet_example.html",
    "href": "convnet_example.html",
    "title": "9  Example convnet",
    "section": "",
    "text": "Medical images offer unique challenges for imaging. A common format for medical images is dicom. Most medical images are 3D or 4D grayscale images. To get a sense of working with medical images, let’s consider a set of 2D color images from the medical mnist library (Yang et al. 2021). A tutorial for working with these images can be found here. These are images of moles to determine whether or not they are cancerous. We’ll follow along with the medmnist code then investigate the output.\nHere are the imports\n\nfrom tqdm import tqdm\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.utils.data as data\nimport torchvision\nimport torchvision.transforms as transforms\nimport medmnist\nfrom medmnist import INFO, Evaluator\nimport matplotlib.pyplot as plt\nimport scipy\n\nThe dermamnist code downloads the data and loads it into a pytorch dataloader. It also keeps track of outcomes.\n\n## Using the code from https://github.com/MedMNIST/MedMNIST/blob/main/examples/getting_started.ipynb\ndata_flag = 'dermamnist'\n\n## This defines our NN parameters\nNUM_EPOCHS = 10\nBATCH_SIZE = 128\nlr = 0.001\n\ninfo = INFO[data_flag]\ntask = info['task']\nn_channels = info['n_channels']\nn_classes = len(info['label'])\n\ndata_transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[.5], std=[.5])\n])\n\n\nDataClass = getattr(medmnist, info['python_class'])\n\n\n# load the data\ntrain_dataset = DataClass(split = 'train', transform = data_transform, download = True)\ntest_dataset  = DataClass(split = 'test' , transform = data_transform, download = True)\npil_dataset   = DataClass(split = 'train',                             download = True)\n\ntrain_loader = data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\ntrain_loader_at_eval = data.DataLoader(dataset=train_dataset, batch_size= 2 * BATCH_SIZE,  shuffle=False)\ntest_loader = data.DataLoader(dataset=test_dataset, batch_size=2*BATCH_SIZE, shuffle=False)\n\nUsing downloaded and verified file: /home/bcaffo/.medmnist/dermamnist.npz\nUsing downloaded and verified file: /home/bcaffo/.medmnist/dermamnist.npz\n\n\nUsing downloaded and verified file: /home/bcaffo/.medmnist/dermamnist.npz\n\n\nLet’s look at a montage of images. Here is the Mayo clinic’s page for investigating moles for potential cancer. To quote them, use the ABCDE method, which they define exactly as below\n\nA is for asymmetrical shape. Look for moles with irregular shapes, such as two very different-looking halves.\nB is for irregular border. Look for moles with irregular, notched or scalloped borders — characteristics of melanomas.\nC is for changes in color. Look for growths that have many colors or an uneven distribution of color.\nD is for diameter. Look for new growth in a mole larger than 1/4 inch (about 6 millimeters).\nE is for evolving. Look for changes over time, such as a mole that grows in size or that changes color or shape. Moles may also evolve to develop new signs and symptoms, such as new itchiness or bleeding.\n\nOur data is cross sectional. So parts of C and D and all of E are challenging. We do not have patient retrospective reports.\n\ntrain_dataset.montage(length=20)\n\n/home/bcaffo/miniconda3/envs/ds4bio/lib/python3.10/site-packages/medmnist/utils.py:25: FutureWarning:\n\n`multichannel` is a deprecated argument name for `montage`. It will be removed in version 1.0. Please use `channel_axis` instead.\n\n\n\n\n\n\nHere are the labels\n\ninfo['label']\n\n{'0': 'actinic keratoses and intraepithelial carcinoma',\n '1': 'basal cell carcinoma',\n '2': 'benign keratosis-like lesions',\n '3': 'dermatofibroma',\n '4': 'melanoma',\n '5': 'melanocytic nevi',\n '6': 'vascular lesions'}\n\n\nHere’s a frequency table of the outcomes in the training data. Notice the prevalence of the sixth category (numbered 5), melanocytic nevi. Looking over our data, it’s by far the most prevalent. See below where we see that the this category is 67% of our training data.\n\ntrain_targets = []\nfor i, t in train_loader:\n    train_targets = np.append(train_targets, t.numpy())\n\nimport pandas as pd\npd.DataFrame({'target' : train_targets }).value_counts(normalize = True)\n\ntarget\n5.0       0.669759\n4.0       0.111175\n2.0       0.109747\n1.0       0.051234\n0.0       0.032539\n6.0       0.014129\n3.0       0.011417\ndtype: float64\n\n\nThis is important to note, because 67% trianing data accuracy is obtainable by simply calling every tumor melanocytic nevi.\nNote that the categories are exclusionary. That is, if a mole is of type \\(j\\), it can’t be of type \\(j'\\). This is different than a task where we’re trying to model multiple properties of the mole. For example, imagine if our dermatologist recorded whether a mole satisfied each of the A, B, C or D criteria. Then, the picture could have multiple 1s across different outcomes. Instead, we only have one possible outcome of the 7 for each.\nSo, we use a softmax outcome. If \\(\\eta_j\\) is a final layer of our network corresponding to category \\(j\\), consider defining \\[\nP(Y_i = j) = \\frac{\\exp(\\eta_j)}{\\sum_{j'=1}^J \\exp(\\eta_{j'})}\n\\] where \\(J\\) is the number of categories (7 in our case) and \\(Y_i\\) is an outcome for image \\(i\\). Notice, this sums to 1 over \\(j\\)."
  },
  {
    "objectID": "convnet_example.html#training-the-network",
    "href": "convnet_example.html#training-the-network",
    "title": "9  Example convnet",
    "section": "9.2 Training the network",
    "text": "9.2 Training the network\nHere is the medmnist NN. Note convd has argument in_channels, out_channels, kernel_size. So, in this case the number of channels in is 3 and in the first layer it puts out a 16 channel image obtained by convolving a 3x3x3 kernels with each channel, adding bias terms, then repeating that 15 more times (documentaiton).\n\nclass Net(nn.Module):\n    def __init__(self, in_channels, num_classes):\n        super(Net, self).__init__()\n\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(in_channels, 16, kernel_size=3),\n            nn.BatchNorm2d(16),\n            nn.ReLU())\n\n        self.layer2 = nn.Sequential(\n            nn.Conv2d(16, 16, kernel_size=3),\n            nn.BatchNorm2d(16),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2))\n\n        self.layer3 = nn.Sequential(\n            nn.Conv2d(16, 64, kernel_size=3),\n            nn.BatchNorm2d(64),\n            nn.ReLU())\n        \n        self.layer4 = nn.Sequential(\n            nn.Conv2d(64, 64, kernel_size=3),\n            nn.BatchNorm2d(64),\n            nn.ReLU())\n\n        self.layer5 = nn.Sequential(\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2))\n\n        self.fc = nn.Sequential(\n            nn.Linear(64 * 4 * 4, 128),\n            nn.ReLU(),\n            nn.Linear(128, 128),\n            nn.ReLU(),\n            nn.Linear(128, num_classes))\n\n    def forward(self, x):\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.layer5(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        return x\n\nmodel = Net(in_channels=n_channels, num_classes=n_classes)\n    \ncriterion = nn.CrossEntropyLoss()\n    \noptimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n\nNow we can train the network. Just for the sake of time, we’ve set the number of epochs to be fairly low.\n\nfor epoch in range(NUM_EPOCHS):\n    train_correct = 0\n    train_total = 0\n    test_correct = 0\n    test_total = 0\n    \n    model.train()\n    for inputs, targets in train_loader:\n        # forward + backward + optimize\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        \n        if task == 'multi-label, binary-class':\n            targets = targets.to(torch.float32)\n            loss = criterion(outputs, targets)\n        else:\n            targets = targets.squeeze().long()\n            loss = criterion(outputs, targets)\n        \n        loss.backward()\n        optimizer.step()"
  },
  {
    "objectID": "convnet_example.html#evaluating-the-network",
    "href": "convnet_example.html#evaluating-the-network",
    "title": "9  Example convnet",
    "section": "9.3 Evaluating the network",
    "text": "9.3 Evaluating the network\nThere’s some evaluation methods included at the MedMnist repo. Below this calculates the AUCs and the accuracy.\n\ndef test(split):\n    model.eval()\n    y_true = torch.tensor([])\n    y_score = torch.tensor([])\n    \n    data_loader = train_loader_at_eval if split == 'train' else test_loader\n\n    with torch.no_grad():\n        for inputs, targets in data_loader:\n            outputs = model(inputs)\n            outputs = outputs.softmax(dim=-1)\n\n            if task == 'multi-label, binary-class':\n                targets = targets.to(torch.float32)\n            else:\n                targets = targets.squeeze().long()\n                targets = targets.float().resize_(len(targets), 1)\n\n            y_true = torch.cat((y_true, targets), 0)\n            y_score = torch.cat((y_score, outputs), 0)\n\n        y_true = y_true.numpy()\n        y_score = y_score.detach().numpy()\n        \n        evaluator = Evaluator(data_flag, split)\n        metrics = evaluator.evaluate(y_score)\n    \n        print('%s  auc: %.3f  acc:%.3f' % (split, *metrics))\n\n        \nprint('==> Evaluating ...')\ntest('train')\ntest('test')\n\n==> Evaluating ...\n\n\ntrain  auc: 0.890  acc:0.723\n\n\ntest  auc: 0.878  acc:0.713\n\n\nLet’s look into the levels. First let’s grab a batch and run it through the model. Then we’ll look at the predictions for a batch. Recall there are 7 tumor types, so we plot our predictions as an image (in little bars since it’s too long otherwise).\n\ninputs,  targets = iter(test_loader).next()\n\noutputs = model(inputs)\noutputs = outputs.softmax(dim=-1)\n\nI, J = outputs.shape\n\nfor i in range(8):\n    plt.subplot(1,8,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.imshow(outputs.detach().numpy()[int(i * I / 8 ) : int((i + 1) * I / 8), :]);\n\n\n\n\nAs expected, it’s predicting the sixth category, labeled 5 since we’re counting from 0, very often, since it is the most prevalent category. The roughly 90% accuracy is pretty good. But, recall, we get 67% accuracy for free. Here’s the highest predictory category for 16 randomly selected images from our batch, their highest probability category and what their actual category is.\n\nbatch = outputs.detach().numpy()\n\n\n#grab 16 images to plot\nindices = np.random.permutation(np.array(range(I)))[0 : 16]\n\n## The associated \nactual = targets.squeeze(-1).numpy()[indices]\ntarget_pred = batch.argmax(axis = 1)[indices]\ntarget_prob = np.round(batch.max(axis = 1) * 100)[indices]\nimages = inputs.numpy()[indices,:,:,:]\n\nplt.figure(figsize=(10,10))\n\nfor i in range(16):\n    plt.subplot(4,4,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    img = np.transpose(images[i,:,:,:], (1, 2, 0));\n    img = ((img + 1)* 255/2).astype(np.uint8)\n    plt.title(\"P=\"+str(target_prob[i])+\",Yhat=\"+str(target_pred[i])+\",Y=\"+ str(actual[i]))\n    plt.imshow(img);\n\n\n\n\nLet’s look at a cross tabulation of the model prediction versus the actual value.\n\ntargets_pred, targets_actual = [], []\n\nfor i,t in test_loader:\n    targets_pred.append(model(i).softmax(dim=-1))\n    targets_actual.append(t)\n\ntargets_pred = torch.cat(targets_pred, dim = 0).detach().numpy().argmax(axis = 1)\ntargets_actual = torch.cat(targets_actual, dim=0).numpy().squeeze()\n\n\n## This is the confusion  matrix data\nconfusion = pd.DataFrame({'y' : targets_actual, 'yhat' : targets_pred}).value_counts()\nconfusion.head(10)\n\ny  yhat\n5  5       1255\n4  5        132\n2  2        109\n   5         84\n5  2         55\n1  5         46\n4  4         46\n   2         40\n1  2         35\n5  4         25\ndtype: int64"
  },
  {
    "objectID": "convnet_example.html#visualizing-the-network",
    "href": "convnet_example.html#visualizing-the-network",
    "title": "9  Example convnet",
    "section": "9.4 Visualizing the network",
    "text": "9.4 Visualizing the network\nHere are the convolutions. Note they are color images since there are 3 channels.\n\n### here's a list of the states that the model has kept track of\nstate_dictionary = list(model.state_dict())\nstate_dictionary[1 : 5]        \n### let's get the first round of weights\nlayer1_kernel = model.get_parameter('layer1.0.weight').detach().numpy()\n\nlayer1_kernel = layer1_kernel - layer1_kernel.min()\nlayer1_kernel = layer1_kernel / layer1_kernel.max()\nlayer1_kernel = layer1_kernel * 255\n\nplt.figure(figsize=(5,5))\nfor h in range(16):\n    plt.subplot(4,4,h+1)\n    plt.xticks([])\n    plt.yticks([])\n    img = np.transpose(layer1_kernel[h,:,:,:],(1,2,0)).astype(np.uint8);\n    plt.imshow(img);\n\n\n\n\nLook at the convolutional layers by following the first image (upper left) through the network.\n\nl1 = model.layer1(inputs)\n\n## plot one\nimages = l1.detach().numpy()[indices,:,:,:]\n\ni = 0\n\nplt.figure(figsize=(10,10))\nfor h in range(16):\n    plt.subplot(4,4,h+1)\n    plt.xticks([])\n    plt.yticks([])\n    img = images[i,h,:,:];\n    plt.imshow(img);\n\n\n\n\nLet’s use SHAP to look at our convolutional neural network. SHAP uses Shapley values from game theory to explain NN predictors. Here’s a manuscript (Young et al. 2019) that utilizes SHAP values on this problem.\nThe Shapley value is defined in game theory as the contribution of a player to a teams’ output factoring in their cooperation. In neural networks, a pixel could be a considered a player and the gain function as the output. The goal is to see the contribution of the player/pixel, factoring in the contributions and how they interact with the others. The Shapley values are difficult to compute, and so approximations need to be used. Python has a package shap that can be used to calculate and visualize approximated Shapley values for an input.\nThe following code is taken from here\n\nimport shap\nfrom PIL import Image\n\n\nbatch = next(iter(test_loader))\nimages, _ = batch\n\nbackground = images[:100]\ntest_images = images[100:105]\n\ne = shap.DeepExplainer(model, background)\nshap_values = e.shap_values(test_images)\n\nshap_numpy = [np.swapaxes(np.swapaxes(s, 1, -1), 1, 2) for s in shap_values]\ntest_numpy = np.swapaxes(np.swapaxes(test_images.cpu().numpy(), 1, -1), 1, 2)\nshap.image_plot(shap_numpy, -test_numpy)\n\nUsing a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\n\n\n\n\nYang, Jiancheng, Rui Shi, Donglai Wei, Zequan Liu, Lin Zhao, Bilian Ke, Hanspeter Pfister, and Bingbing Ni. 2021. “MedMNIST V2: A Large-Scale Lightweight Benchmark for 2D and 3D Biomedical Image Classification.” arXiv Preprint arXiv:2110.14795.\n\n\nYoung, Kyle, Gareth Booth, Becks Simpson, Reuben Dutton, and Sally Shrapnel. 2019. “Deep Neural Network or Dermatologist?” In Interpretability of Machine Intelligence in Medical Image Computing and Multimodal Learning for Clinical Decision Support, 48–55. Springer."
  },
  {
    "objectID": "unsupervised.html",
    "href": "unsupervised.html",
    "title": "10  Unsupervised learning",
    "section": "",
    "text": "Let \\(\\{X_i\\}\\) for \\(i=1,\\ldots,n\\) be \\(p\\) random vectors with means \\((0,\\ldots,0)^t\\) and variance matrix \\(\\Sigma\\). Consider finding \\(v_1\\), a \\(p\\) dimensional vector with \\(||v_1|| = 1\\) so that \\(v_1^t \\Sigma v_1\\) is maximized. Notice this is equivalent to saying we want to maximize \\(\\mathrm{Var}( X_i^t V_1)\\). The well known solution to this equation is that \\(v_1\\) is the first eigenvector of \\(\\Sigma\\) and \\(\\lambda_1 = \\mathrm{Var}( X_i^t V_1)\\) is the associated eigenvalue. If \\(\\Sigma = V^t \\Lambda V\\) is the eigenvalue decomposition of where \\(V\\) are the eigenvectors and \\(\\Lambda\\) is a diagonal matrix of the eigenvalues ordered from greatest to least, then \\(v_1\\) corresponds to the first column of \\(V\\) and \\(\\lambda_1\\) corresponds to the first element of \\(\\Lambda\\). If one then finds \\(v_k\\) as the vector maximizing \\(v_k^t \\Sigma v_k\\) so that \\(v_k^t v_{k'} = I(k=k')\\), then the \\(v_k\\) are the columns of \\(V\\) and \\(v_k^t \\Sigma v_k = \\lambda_k\\) are the eigenvalues.\nNotice:\n\n\\(V \\Sigma V^t = \\Lambda\\) (i.e. \\(V\\) diagonalizess \\(\\Sigma\\))\n\\(\\mbox{Trace}(\\Sigma) = \\mbox{Trace}(\\Sigma V^t V) = \\mbox{Trace}(V \\Sigma V^t) = \\sum \\lambda_k\\) (i.e. the total variability is the sum of the eigenvalues)\nSince \\(V^t V = I\\), \\(V\\) is a rotation matrix. Thus, \\(V\\) rotates \\(X_i\\) in such a way that to maximize variability in the first dimension, then the second dimensions …\n\\(\\mbox{Cov}(X_i^t v_k, x_i^t v_{k'} )= \\mbox{Cov}(X_i^t v_k, x_i^t v_{k'} ) v_k^t \\mbox{Cov}(x_i, x_i^t) v_{k'} = v_k^t V v_{k'} = 0\\) if \\(k\\neq k'\\)\nAnother representation of \\(\\Sigma\\) is \\(\\sum_{k=1}^p \\lambda_i v_k v_k^t\\) by simply rewriting the matrix algebra of \\(V \\Lambda V^t\\).\nThe variables \\(U_i = V X_i\\) then: have uncorrelated elements (\\(\\mbox{Cov}(U_{ik}, U_{ik'}) = 0\\) for \\(k\\neq k'\\) by property 5), have the same total variability as the elements of \\(X_i\\) (\\(\\sum_k \\mbox{Var}(U_{ik}) = \\sum_k \\lambda_k = \\sum_k \\mbox{Var}(X_{ik})\\) by property 2), are a rotation of the \\(X_i\\), are ordered so that \\(U_{i1}\\) has the greatest amount of variability and so on.\n\nNotation:\n\nThe \\(\\lambda_k\\) are simply called the eigenvalues or principal components variation.\n\\(U_{ik} = X_i^t v_k\\) is called the principal component scores.\nThe \\(v_k\\) are called the principal component loadings or weights, with \\(v_1\\) being called the first principal component and so on.\n\nStatistical properties under the assumption that the \\(x_i\\) are iid with mean 0 and variance \\(\\Sigma\\)\n\n\\(E[U_{ik}]=0\\)\n\\(\\mbox{Var}(U_{ik}) = \\lambda_k\\)\n\\(\\mbox{Cov}(U_{ik}, U_{ik'}) = 0\\) if \\(k\\neq k'\\)\n\\(\\sum_{k=1}^p \\mbox{Var}(U_{ik}) = \\mbox{Trace}(\\Sigma)\\).\n\\(\\prod_{k=1}^p \\mbox{Var}(U_{ik}) = \\mbox{Det}(\\Sigma)\\)\n\n\n\nOf course, we’re describing PCA as a conceptual process. We realize \\(n\\) \\(p\\) dimensional vectors \\(x_1\\) to \\(x_n\\), typically organized in \\(X\\) a \\(n\\times p\\) matrix. If \\(X\\) is not mean 0, we typically demean it by calculating \\((I- J(J^t J)^{-1} J') X\\) where \\(J\\) is a vector of ones. Assume this is done. Then \\(\\frac{1}{n-1} X^t X = \\hat \\Sigma\\). Thus, our sample PCA is obtained via the eigenvalue decomposition \\(\\hat \\Sigma = \\hat V \\hat \\Lambda \\hat V^t\\) and our principal components obtained as $ X V$.\nWe can relate PCA to the SVD as follows. Let \\(\\frac{1}{\\sqrt{n-1}} X = \\hat U \\hat \\Lambda^{1/2} \\hat V^t\\) be the SVD of the scaled version of \\(X\\). Then note that \\[\n\\hat \\Sigma = \\frac{1}{n-1} X^t X = \\hat V \\hat \\Lambda \\hat V^t\n\\] yields the sample covariance matrix eigenvalue decomposition.\n\n\n\nConsider the case where one of \\(n\\) or \\(p\\) is large. Let’s assume \\(n\\) is large. Then \\[\n\\frac{1}{n-1} X^t X = \\frac{1}{n-1} \\sum_i x_i x_i^t\n\\] As we learned in the chapter on HDF5, we can do sums like these without loading the entirety of \\(X\\) into memory. Thus, in this case, we can calculate the eigenvectors using only the small dimension. If, on the other hand, \\(p\\) is large and \\(n\\) is smaller, then we can calculate the eigenvalue decomposition of \\[\n\\frac{1}{n-1} X X^t = \\hat U \\hat \\Lambda \\hat U^t.\n\\] In either case, whether \\(U\\) or \\(V\\) is easier to get, we can then obtain the other via vectorized multiplication.\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport numpy.linalg as la\nfrom sklearn.decomposition import PCA\nimport urllib.request\nimport PIL\nimport numpy as np\nimport torch \nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.utils.data as data\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom sklearn.decomposition import FastICA\nfrom tqdm import tqdm\nimport medmnist\nfrom medmnist import INFO, Evaluator\nimport scipy\nimport IPython\n\n\nn = 1000\nmu = (0, 0)\nSigma = np.matrix([[1, .5], [.5, 1]])\nX = np.random.multivariate_normal( mean = mu, cov = Sigma, size = n)\n\nplt.scatter(X[:,0], X[:,1])\n\n<matplotlib.collections.PathCollection at 0x7f048f666d70>\n\n\n\n\n\n\nX = X - X.mean(0)\nprint(X.mean(0))\nSigma_hat = np.matmul(np.transpose(X), X) / (n-1) \nSigma_hat\n\n[-1.16573418e-17 -4.28546088e-17]\n\n\narray([[1.03646674, 0.51893226],\n       [0.51893226, 0.99579207]])\n\n\n\nevd = la.eig(Sigma_hat)\nlambda_ = evd[0]\nv_hat = evd[1]\nu_hat = np.matmul(X, np.transpose(v_hat))\nplt.scatter(u_hat[:,0], u_hat[:,1])\n\n<matplotlib.collections.PathCollection at 0x7f048e998a00>\n\n\n\n\n\nFit using scikitlearn’s function\n\npca = PCA(n_components = 2).fit(X)\nprint(pca.explained_variance_)\nprint(lambda_ )\n\n[1.53546003 0.49679878]\n[1.53546003 0.49679878]\n\n\n\n\n\nLet’s consider the melanoma dataset that we looked at before. First we read in the data as we have done before so we don’t show that code.\n\n\nUsing downloaded and verified file: /home/bcaffo/.medmnist/dermamnist.npz\n\n\nUsing downloaded and verified file: /home/bcaffo/.medmnist/dermamnist.npz\n\n\nUsing downloaded and verified file: /home/bcaffo/.medmnist/dermamnist.npz\n\n\nNext, let’s get the data from the torch dataloader format back into an image array and a matrix with the image part (28, 28, 3) vectorized.\n\ndef loader_to_array(dataloader):\n  ## Read one iteration to get data\n  test_input, test_target = iter(dataloader).next()\n  ## Total number of training images\n  n = np.sum([inputs.shape[0] for inputs, targets in dataloader])\n  ## The dimensions of the images\n  imgdim = (test_input.shape[2], test_input.shape[3])\n  images = np.empty( (n, imgdim[0], imgdim[1], 3))\n\n  ## Read the data from the data loader into our numpy array\n  idx = 0\n  for inputs, targets in dataloader:\n    inputs = inputs.detach().numpy()\n    for j in range(inputs.shape[0]):\n      img = inputs[j,:,:,:]\n      ## get it out of pytorch format\n      img = np.transpose(img, (1, 2, 0))\n      images[idx,:,:,:] = img\n      idx += 1\n  matrix = images.reshape(n, 3 * np.prod(imgdim))\n  return images, matrix\n\ntrain_images, train_matrix = loader_to_array(train_loader)\ntest_images, test_matrix = loader_to_array(test_loader)\n\n## Demean the matrices\ntrain_mean = train_matrix.mean(0)\ntrain_matrix = train_matrix - train_mean\ntest_mean = test_matrix.mean(0)\ntest_matrix = test_matrix - test_mean\n\nNow let’s actually perform PCA using scikitlearn. We’ll plot the eigenvalues divided by their sums, \\(\\lambda_k / \\sum_{k'} \\lambda_{k'}\\). This is called a scree plot.\n\nfrom sklearn.decomposition import PCA\nn_comp = 10\npca = PCA(n_components = n_comp).fit(train_matrix)\nplt.plot(pca.explained_variance_ratio_)\n\n\n\n\nOften this is done by plotting the cummulative sum so that you can visualize how much variance is explained by including the top \\(k\\) components. Here I fit 10 components and they explain 85% of the variation.\n\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\n\n\n\n\nNote that the weights from the eigenvectors, \\(V\\), are images. We can plot these as images.\n\neigen_moles = pca.components_\n\n\n\n\n\n\nLet’s project our testing data onto the principal component basis created by our training data and see how it does. Let \\(X_{training} = U \\Lambda^{1/2} V^t\\) is the SVD of our training data. Then, we can convert ths scores, \\(U\\) back to \\(X_{training}\\) with the map \\(U \\rightarrow U \\lambda^{1/2} V\\). Or, if our scores are normalized, \\(U \\Lambda^{1/2}\\) then we simply multiply by \\(V^t\\). If we want to represent \\(X_{training}\\) by a lower dimensional summary, we just keep fewer columns of scores, then multiply by the same columns of \\(V\\). We could write this as \\(U_s = X_{training} V_S \\lambda^{-1/2}_S\\), where \\(S\\) refers to a subset of values of \\(k\\).\nNotice that \\(\\hat X_{training} = U_{S} V^t_S \\Lambda^{-1/2}_S = X_{training} V_S V_S^t\\) , \\(\\Lambda\\) and \\(V\\). Consider then an approximation to \\(X_{test}\\) as \\(\\hat X_{test} = X_{test} V_s V_S^t\\). Written otherwise \\[\n\\hat X_{i,test} = \\sum_{k \\in S} <x_{i,test}, v_k> v_k\n\\] which is the projection of subject \\(i\\)’s features into the linear space spanned by the basis defined by the principal component loadings.\nLet’s try this on our mole data.\n\ntest_matrix_fit = pca.inverse_transform(pca.transform(test_matrix))\nnp.mean(np.abs( test_matrix - test_matrix_fit))\n\n0.037923909133878414"
  },
  {
    "objectID": "unsupervised.html#ica",
    "href": "unsupervised.html#ica",
    "title": "10  Unsupervised learning",
    "section": "10.2 ICA",
    "text": "10.2 ICA\nICA, independent components analysis, tries to find linear transformations of the data that are statistically independent. Usually, independence is an assumption in ICA, not actually embedded in the loss function.\nLet \\(S_t\\) be an \\(\\mathbb{R}^p\\) collection of \\(p\\) source signals. Assume that the underlying signals are independent, so that \\(S_t \\sim F = F_1 \\times F_2 \\times \\ldots \\time f_p\\). Assume that the observed data is \\(X_t = M S_t\\) and \\(X_t \\sim G\\). It is typically assumed that \\(M\\) is invertible so that \\(S_t = M^{-1} X_t\\) and \\(M\\) and \\(M^{-1}\\) are called the mixing and unmixing matrices respectively. Note that, since we observe \\(X_t\\) over many repititions of \\(t\\), we can get an estimate of \\(G\\). Typically, it is also assumed that the \\(X_t\\) are iid over \\(t\\).\nOne way to characterize the estimation problem is to parameterize \\(F_1\\), \\(F_2\\) and \\(F_3\\) and use maximum likelihood, or equivalent [citations]. Another is to minimize some distance between \\(G\\) and \\(F_1\\), \\(F_2\\) and \\(F_3\\). Yet another is to actually maximize independence between the components of \\(S_t\\) using some estimate of independence [cite Matteson].\nThe most popular approaches try to find \\(M^{-1}\\) by maximizing non-Gaussianity. The logic goes that 1) interesting features tend to be non-Gaussian and 2) an appeal to the CLT over signals suggest that the mixed signals should be more Gaussian by being linear combinations of independent things. The latter claim is heuristic relative to the formal CLT. However, maximizing non-Gaussian components tends to work well in practice, thus validating the motivation empirically.\nOne form of ICA maximizes the kurtosis. If \\(Y\\) is a random variable, then \\(E[Y^4] - 3 E[Y^2]\\) is the kurtosis. One could then find \\(M^{-1}\\) that maximizes the empirical kurtosis of the unmixed signals. Another variation of non-Gaussianity maximizes neg-entropy. The neg-entropy of a density \\(h\\) is given by \\[\n- \\int h(y) \\log(h(y)) dy = - E_h[\\log h(Y)]\n\\] A well known theorem states that the Gaussian distribution has the largest entropy of all distributions with the same variance. Therefore, to maximize non-Gaussianity, we can minmize entropy, or equivalently maximize neg-entropy. We could subtract the entropy of the Gaussian distribution to consider this a cross entropy problem, but that only adds a constant to the loss function. The maximization of neg-entropy can be done many ways. We need the following. For a given \\(M^{-1}\\), estimate \\(G\\) from the collection \\(M^{-1} X_t\\), then calculate the neg-entropy of \\(f_j\\). Use that to then take an opimization step of \\(M\\) is the right direction. Some versions of estimation use a polynomial expansion of the \\(f_j\\), which then typically only requires higher order moments, like kurtosis. Fast ICA is a particular implmementation of maximizing neg-entropy.\nStatistical versions of ICA don’t require \\(M\\) to be invertible. Moreover, error terms can be added in which case you can see the connection between ICA and factor analytic models. However, factor analysis models tend to assume Gaussianity.\n\n10.2.1 Example\nConsider an example that PCA would have somewhat of a hard time with. In this case, our data is from a mixture of normals with half from a normal with a strong positive correlation and half with a strong negative correlation. Because the angle between the two is not 90 degrees PCA has no chance. No rotation of the axes satisfies the obvious structure in this data.\n\nn = 1000\n\nSigma = np.matrix([[4, 1.8], [1.8, 1]])\na = np.random.multivariate_normal( mean = [0, 0], cov = Sigma, size = int(n/2))\nSigma = np.matrix([[4, -1.8], [-1.8, 1]])\nb = np.random.multivariate_normal( mean = [0, 0], cov = Sigma, size = int(n/2))\nx = np.append( a, b, axis = 0)\nplt.scatter(x[:,0], x[:,1])\nplt.xlim([-6, 6])\nplt.ylim([-6, 6])\n\n(-6.0, 6.0)\n\n\n\n\n\nLet’s try fast ICA. Notice it comes much closer to discovering the structure we’d like to discover than PCA could. It pulls appart the two components to a fair degree. Also note, there’s a random starting point of ICA, so that I get fairly different fits over re-runs of the algorithm. I had to lower the tolerance to get a good fit.\nIndpendent components are order invariant and sign invariant.\n\ntransformer = FastICA(tol = 1e-7)\nicafit = transformer.fit(x)\ns = icafit.transform(x)\nplt.scatter(s[:,0], s[:,1])\nplt.xlim( [s.min(), s.max()])\nplt.ylim( [s.min(), s.max()])\n\n/home/bcaffo/miniconda3/envs/ds4bio/lib/python3.10/site-packages/sklearn/decomposition/_fastica.py:488: FutureWarning:\n\nFrom version 1.3 whiten='unit-variance' will be used by default.\n\n/home/bcaffo/miniconda3/envs/ds4bio/lib/python3.10/site-packages/sklearn/decomposition/_fastica.py:120: ConvergenceWarning:\n\nFastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n\n\n\n(-0.13074939616003947, 0.13161320076743127)\n\n\n\n\n\n\n\n10.2.2 Cocktail party example\nThe classic ICA problem is the so called cocktail party problem. In this, you have \\(p\\) sources and \\(p\\) microphones. The microphones each pick up a mixture of signals from the different sources. The goal is to unmix the sources into the components. Independence makes sense in the cocktail party example, since logically conversations would have some independence.\n\nimport audio2numpy as a2n\ns1, i1 = a2n.audio_from_file(\"mp3s/4.mp3\")\ns2, i2 = a2n.audio_from_file(\"mp3s/2.mp3\")\ns3, i3 = a2n.audio_from_file(\"mp3s/6.mp3\")\n\n## Get everything to be the same shape and sum the two audio channels\nn = np.min((s1.shape[0], s2.shape[0], s3.shape[0]))\ns1 = s1[0:n,:].mean(axis = 1)\ns2 = s2[0:n,:].mean(axis = 1)\ns3 = s3[0:n,:].mean(axis = 1)\n\ns = np.matrix([s1, s2, s3])\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nMix the signals.\n\nw = np.matrix( [ [.7, .2, .1], [.1, .7, .2], [.2, .1, .7] ])\nx = np.transpose(np.matmul(w, s))\n\nHere’s an example mixed signal\n\nIPython.display.Audio(data = x[:,1].reshape(n), rate = i1)\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nNow try to unmix using fastICA\n\ntransformer = FastICA(whiten=True, tol = 1e-7)\nicafit = transformer.fit(x)\n\n/home/bcaffo/miniconda3/envs/ds4bio/lib/python3.10/site-packages/sklearn/decomposition/_fastica.py:673: FutureWarning:\n\nFrom version 1.3 whiten=True should be specified as whiten='arbitrary-variance' (its current behaviour). This behavior is deprecated in 1.1 and will raise ValueError in 1.3.\n\n/home/bcaffo/miniconda3/envs/ds4bio/lib/python3.10/site-packages/sklearn/utils/validation.py:727: FutureWarning:\n\nnp.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n\n\n\n\nicafit.mixing_\n\narray([[ 16.57529717,  51.16265712,  12.70501389],\n       [ 31.47354705,   7.49713379,  44.06893612],\n       [108.62775255,  14.14784389,   5.51355073]])\n\n\nUnmixing matrix\n\nicafit.components_\n\narray([[-0.00262577, -0.0004616 ,  0.00974016],\n       [ 0.02080962, -0.00581283, -0.0014911 ],\n       [-0.00166489,  0.02401029, -0.00670264]])\n\n\nHere’s a scatterplot matrix where the real component is on the rows and the estimated component is on the columns.\n\nhat_s = np.transpose(icafit.transform(x))\n\nplt.figure(figsize=(10,10))\n\nfor i in range(3):\n  for j in range(3):\n    plt.subplot(3, 3, (3 * i + j) + 1)\n    plt.scatter(hat_s[i,:].squeeze(), np.asarray(s)[j,:])\n\n/home/bcaffo/miniconda3/envs/ds4bio/lib/python3.10/site-packages/sklearn/utils/validation.py:727: FutureWarning:\n\nnp.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n\n\n\n\n\n\nWe can now play the estimated sources and see how they turned out.\n\nfrom scipy.io.wavfile import write\ni = 0\ndata = (hat_s[i,:].reshape(n) / np.max(np.abs(hat_s[i,:]))) * .5\nIPython.display.Audio(data = data, rate = i1)\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\ni = 1\ndata = (hat_s[i,:].reshape(n) / np.max(np.abs(hat_s[i,:]))) * .5\nIPython.display.Audio(data = data, rate = i1)\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\ni = 2\ndata = (hat_s[i,:].reshape(n) / np.max(np.abs(hat_s[i,:]))) * .5\nIPython.display.Audio(data = data, rate = i1)\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\n\n10.2.3 Imaging example using ICA\nLet’s see what we get for the images. Logically, one would consider voxels as mixed sources and images as the iid replications. But, then the sources would not be images. Let’s try the other dimension and see what we get where subject images are mixtures of source images. This is analogous to finding a soure basis of subject images.\nThis is often done in ICA where people transpose matrices to investigate different problems.\n\ntransformer = FastICA(n_components=10, random_state=0,whiten='unit-variance', tol = 1e-7)\nicafit = transformer.fit_transform(np.transpose(train_matrix))\nicafit.shape\n\n/home/bcaffo/miniconda3/envs/ds4bio/lib/python3.10/site-packages/sklearn/decomposition/_fastica.py:120: ConvergenceWarning:\n\nFastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n\n\n\n(2352, 10)"
  },
  {
    "objectID": "unsupervised.html#autoencoders",
    "href": "unsupervised.html#autoencoders",
    "title": "10  Unsupervised learning",
    "section": "10.3 Autoencoders",
    "text": "10.3 Autoencoders\nAutoencoders are another unsupervised learning technique. Autoencoders take in a record and spit out a prediction of the same size. The goal is to represent the records as a NN. In an incomplete autoencoder, the model is regularized by the embedding (middle) layer being much lower than the input dimension. In this way, an autoencoder is a dimension reduction technique, reducing the input dimension size downto a much lower size (the encoder) then back out to the original size (the decoder). We can represent the autoencoder with a network diagram as below.\n\n\n\n\n\nLet \\(\\phi\\) be the first two layers of the network and \\(\\theta\\) be the last two. So, if we wanted to pass a data row, \\(x_i\\) through the network we would do \\(\\theta(\\phi(x_i))\\). We would call the network \\(\\phi\\) as the encoding network and \\(\\theta\\) as the decoding network. Consider training the network by minimizing\n\\[\n\\sum_{i=1}^n || x_i - \\theta(\\phi(x_i)) ||^2\n\\]\nover the weights. This sort of network is called an autoencoder. Notice that the same data is the input and output of the network. This kind of learning is called unsupervised, since we’re not trying to use \\(x\\) to predict an outcome \\(y\\). Instead, we’re trying to explore variation and find interesting features in \\(x\\) as a goal in itself without a “supervising” outcome, \\(y\\), to help out.\nNotice overfitting concerns are somewhat different in this network construction. If this model fits well, then it’s suggesting that 2 numbers can explain 8. That is, the network will have reduced the inputs to only two dimensions, that we could visualize for example. That is a form of parsimony that prevents overfitting. The middle layer is called the embedding. It is called this because an autoencoder is a form of non-linear embedding of our data into a lower dimensionional space.\nThere’s nothing to prevent us from having convolutional layers if the inputs are images. That’s what we’ll work on here. For convolutional autoencoders, it’s typical to increase the number of channels and decrease the image sizes as one works through the network.\n\n10.3.1 PCA and autoencoders\nWithout modification, autoencoders can be programmed that span the same space as PCA/SVD (Plaut 2018). Enforcing the orthogonality requires something like adding Lagrange terms to the loss function. There’s no reason why you would do this, since PCA is well developed and works just fine. However, it does suggest why NNs are such a large class of models.\nLet \\(X_i\\) be a collection of features for record \\(i\\). Then, the SVD approximates the data matrix \\(X\\) with \\(UV^t\\), where we’ve absorbed the singular values into either \\(U\\) or \\(V\\). Per record, this model for \\(K\\) components and column \\(k\\) from \\(V\\) of \\(v_k\\).\n\\[\n\\hat x_i = \\sum_{k=1}^K <x_i, v_k> v_k\n\\]\nTherefore, consider a neural network that specifies that the first layer defines \\(K\\) hidden units as\n\\[\nh_{ik} = <x_i, v_k>.\n\\]\nThat is, it has a linear activation function with no bias term and weights \\(v_{jk}\\) where \\(v_{jk}\\) is element \\(j\\) of vector \\(v_k\\). Then consider an output layer that defines\n\\[\n\\hat x_{ij} = \\sum_{k=1}^K h_{ik} v_{jk},\n\\]\nAgain, this is a linear activation function with weights \\(v_{jk}\\). So, we arrive at the conclusion, that PCA is an example of an autoencoder with two layers, constraints on the weights being common to both layers, and constraints on the loss function that enforces the orthonormality of the \\(v_k\\). Of course, as we saw with ordinary regression, whether or not we can actually get gradient descent to converge remains a harder issue than just using PCA directly. Furthermore, the autoencoder wouldn’t necessarily order the PCs similarly.\nFinally, we see that a two layer autoencoder -without the constraints- contains the PCA fit as a special case and spans the same space as the PCA fit. Similarly we see that such a two layer encoder is overspecified, as most NNs are.\n\n\n10.3.2 Example on dermamnist\nFirst, let’s set up our autoencoder by defining a python class then initializing it.\n\nkernel_size = 5\n\nclass autoencoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        self.conv1 = nn.Conv2d(3, 6, kernel_size)\n        self.conv2 = nn.Conv2d(6, 12, kernel_size)\n        self.pool  = nn.MaxPool2d(2, 2)\n        self.iconv1 = nn.ConvTranspose2d(12, 6, kernel_size+1, stride = 2)\n        self.iconv2 = nn.ConvTranspose2d(6, 3, kernel_size+1, stride = 2)\n\n    def encode(self, x):\n        x = F.relu(self.conv1(x))\n        x = self.pool(x)\n        x = F.relu(self.conv2(x))\n        x = self.pool(x)\n        return x\n    \n    def decode(self, x):\n        x = F.relu(self.iconv1(x))\n        ## Use the sigmoid as the final layer \n        ## since we've normalized pixel values to be between 0 and 1\n        x = torch.sigmoid(self.iconv2(x))\n        return(x)\n    \n    def forward(self, x):\n        return self.decode(self.encode(x))\n    \nautoencoder = autoencoder()\n\nWe can try out the autoencoder by\n\n## Here's some example data by grabbing one batch\ntryItOut, _ = iter(train_loader).next()\nprint(tryItOut.shape)\n\n## Let's encode that data\nencoded = autoencoder.encode(tryItOut)\nprint(encoded.shape)\n\n## Now let's decode the encoded data\ndecoded = autoencoder.decode(encoded)\nprint(decoded.shape)\n\n## Now let's run the whole thing through\nfedForward = autoencoder.forward(tryItOut)\nprint(fedForward.shape)\n\ntorch.Size([128, 3, 28, 28])\ntorch.Size([128, 12, 4, 4])\ntorch.Size([128, 3, 28, 28])\ntorch.Size([128, 3, 28, 28])\n\n\n\ntest = fedForward.detach().numpy()\n\n## Plot out the first 5 images, note this isn't very interesting, since\n## all of the weights haven't been trained\nplt.figure(figsize=(10,5))\nfor i in range(5): \n  plt.subplot(1, 5,i+1)\n  plt.xticks([])\n  plt.yticks([])\n  img = np.transpose(test[i,:,:,:], (1, 2, 0))\n  plt.imshow(img)\n\n\n\n\n\n#Optimizer\noptimizer = torch.optim.Adam(autoencoder.parameters(), lr = 0.001)\n\n#Epochs\nn_epochs = 20\n\nautoencoder.train()\n\nfor epoch in range(n_epochs):\n    for data, _ in train_loader:\n        images = data\n        optimizer.zero_grad()\n        outputs = autoencoder.forward(images)\n        loss = F.mse_loss(outputs, images)\n        loss.backward()\n        optimizer.step()\n\n\n## the data from the last iteration is called images\ntrainSample = images.detach().numpy()\n\nplt.figure(figsize=(10,5))\nfor i in range(5): \n  plt.subplot(1, 5,i+1)\n  plt.xticks([])\n  plt.yticks([])\n  img = np.transpose(trainSample[i,:,:,:], (1, 2, 0))\n  plt.imshow(img)\n\n## the output from the last iterations (feed forward through the network) is called outputs\ntrainOutput = outputs.detach().numpy()\n\nplt.figure(figsize=(10,5))\nfor i in range(5): \n  plt.subplot(2, 5,i+6)\n  plt.xticks([])\n  plt.yticks([])\n  img = np.transpose(trainOutput[i,:,:,:], (1, 2, 0))\n  plt.imshow(img)\n\n\n\n\n\n\n\nOn a test batch\n\ntest_batch, _ = iter(test_loader).next()\nx_test = test_batch.detach().numpy()\ntestSample = autoencoder.forward(test_batch).detach().numpy()\n\nplt.figure(figsize=(10,4))\n\n## Plot the original data\nfor i in range(5): \n  plt.subplot(2, 5, i + 1)\n  plt.xticks([])\n  plt.yticks([])\n  img = np.transpose(x_test[i,:,:,:], (1, 2, 0))\n  plt.imshow(img)\n# Plot the data having been run throught the convolutional autoencoder\nfor i in range(5): \n  plt.subplot(2, 5, i + 6)\n  plt.xticks([])\n  plt.yticks([])\n  img = np.transpose(testSample[i,:,:,:], (1, 2, 0))\n  plt.imshow(img)\n\n\n\n\n\n\n\n\nPlaut, Elad. 2018. “From Principal Subspaces to Principal Components with Linear Autoencoders.” arXiv Preprint arXiv:1804.10253."
  },
  {
    "objectID": "variationalAEs.html",
    "href": "variationalAEs.html",
    "title": "11  Variational autoencoders",
    "section": "",
    "text": "Since we’re talking about variational autoencoders, we should probably first talk about variational Bayes (VB). VB is a scaling solution for Bayesian inference.\nBayesian inference tends to follow something like this. We have a likelihood, \\(f(x | \\theta)\\), and a prior, \\(f(\\theta)\\). We use Bayes rule to calculate \\(f(\\theta | x) = f(x|\\theta)p(\\theta) / f(X)\\) where \\(f(x) = \\int f(x, \\theta) d\\theta\\). The posterior, \\(f(\\theta | x)\\) is our object of interest for which we will make inferences. Some problems arise. Often can’t calculate \\(f(x)\\), since the integral is intractable. Monte Carlo solutions generate \\(\\theta^{(i)} \\sim f(\\theta | x)\\) and then approximate expectations from the posterior, \\(E[h(\\theta) | X]\\) with \\(\\frac{1}{N} \\sum_{i=1}^N h(\\theta^{(i)})\\), which converges by the law of large numbers. Sometimes we can’t simulate iid variates from \\(f(\\theta | x)\\), so we’ll sample from \\(f(\\theta_k | \\theta_{\\sim k}, x)\\) for each \\(k\\). This is called Gibbs sampling and generates a Markov chain that, under assumptions, yields variables that satisfy LLNs. Sometimes we can’t do the Gibbs sampler and more elaborate sampling schemes need to be done. All in all, this makes Bayes analysis challenging and often difficult to scale.\nMany solutions use candidate distributions. Say, for example, \\(g(z|x)\\) is a distribution that we can sample from. Then notice that for samples \\(Z^{(i)}\\sim g(z|x)\\) \\[\n\\frac{\\sum_{i=1}^N h(Z^{(i)})\\frac{f(Z^{(i)|x})}{g(z^{(i)}|x)}}{\\sum_{i=1}^N \\frac{f(Z^{(i)|x})}{g(Z^{(i)}|x)}} = \\frac{\\sum_{i=1}^N h(Z^{(i)})\\frac{f(Z^{(i), x})}{g(Z^{(i)}|x)}}{\\sum_{i=1}^N \\frac{f(Z^{(i), x})}{g(Z^{(i)}|x)}} \\rightarrow \\frac{E[h(Z) | x]}{E[1 | x]} = E[h(Z) | x]\n\\] Of note we can calculate these weights, since we know \\(g(z|x)\\) and \\(f(z,x) = f(x|z)f(z)\\) is the likelihood times the prior.\nImportance sampling, Gibbs sampling and other Monte Carlo techniques get combined for complicated problems. However, modern problems often present a challenge that Monte Carlo techniques cannot solve. Enter variational Bayes. Instead of fixing up samples from \\(g\\) to be exactly what we want, why don’t we choose \\(g\\) as well as possible and simply use it instead of \\(f(z|x)\\)?\nThis is where variational Bayes comes in. It turns a MC sampling problem into an optimization problem. Wikipedia has a pretty nice introduction to the topic. The most common version of variational Bayes uses the KL divergence. I.e. choose \\(g\\) to minimize\n\\[\n\\int g(z|x) \\log\\left( \\frac{g(z|x)}{f(z|x)} \\right) dz\n= E_{g(z|x)}\\left[\\log\\left( \\frac{g(Z|x)}{f(Z|x)} \\right)\\right]\n\\equiv D_{KL}\\{g(Z|x) || f(Z|x) \\}\n\\]\nOften \\(g(z|x)\\) is chosen to be be \\(\\prod_j g(z_j|x)\\), or independent in the components of \\(z\\). Then it can be shown that the best approximation to the posterior, in the sense of minimizing the KL divergence, sets\n\\[\n\\hat g (z_j | x) \\propto \\exp\\left\\{ E_{i\\neq j}[\\log\\{f(Z_i | x)\\}]\\right\\}\n\\]\nIt is called variational Bayes, since it uses calculus of variations to solve this equation. Go over the Wikipedia article’s Gaussian / Gamma prior example. There you can see the derivation of the variational posterior approximation in a case where it can be done analytically. In most cases, one is left with doing it via algorithmmic optimization."
  },
  {
    "objectID": "variationalAEs.html#vaes",
    "href": "variationalAEs.html#vaes",
    "title": "11  Variational autoencoders",
    "section": "11.2 VAEs",
    "text": "11.2 VAEs\n\n11.2.1 Introduction to VAEs\nVariational autoencoders were introduced in (Kingma and Welling 2013). A really good tutorial can be found here and some sample code on MNIST can be found here. An alternate way to think about autoencoders is via variational Bayes arguments. Let \\(x_i\\) be a record for \\(i = 1,\\ldots,n\\). For now, let’s drop the subscript \\(i\\). Define the following:\n\n\\(p_\\theta(x | z)\\) is the likelihood of \\(z\\) when viewed as a function of \\(z\\) or is the data generating distribution if viewed as a function of \\(x\\);\n\\(p_\\theta(x)\\) is the data marginal, often called the evidence in this literature;\n\\(p_\\theta(z | x)\\) is the posterior (of \\(z\\) given \\(x\\));\nNote \\(p_\\theta(z | x) = p_\\theta(x, z) / p_\\theta(x)\\);\nNote \\(p_\\theta(x) = \\int p_\\theta(x, z) dz\\);\n\nWe could view any latent probability distribution as an autoencoder, where \\(p_\\theta(z | x)\\) is the encoder and \\(p_\\theta(x | z)\\) is the decoder. Note, if \\(x\\) is an image, say, then it is the distribution \\(p_\\theta(x | z)\\) that characterizes an image given its latent representation \\(z\\). So, if we want an image, we need to look at the mean, posterior mode or even just a sample generation of \\(p_\\theta(x | z)\\).\nOne issue with this approach is that computing is quite hard for problems of sufficient scale. Variational Bayes uses approximations instead of the actual distributions. Let \\(q_\\phi(z | x)\\) be an approximiation of the posterior. Typical variational Bayes uses minmizers of the KL divergence. Variational autoencoders do that as well. However, VAEs tend to maximize the ELBO, evidence lower bound (ELBO). We define the ELBO as\n\\[\nL_{\\phi, \\theta}(x) = \\log\\{p_\\theta(x)\\} - E_{q_\\phi(z | x)} \\left[\\log\\left( \\frac{q_\\phi(Z | x)}{p_\\theta(Z | x)}  \\right)\\right]\n= \\log\\{p_\\theta(x)\\} - D_{KL}\\{q_\\phi(Z | x) || p_\\theta(Z | x)\\}\n\\] where \\(D_{KL}(a||b)\\) is the Kullback/Liebler divergence between distributions \\(a\\) and \\(b\\). The ELBO is a lower bound since the divergence term is positive and so the following inequality holds: \\[\nL_{\\phi, \\theta}(x) \\leq \\log\\{p_\\theta(x)\\}.\n\\] Where, recall, we call \\(p_\\theta(x)\\) the evidence (in this area). That it’s a bound on the evidence suggest that maximizing it is a good thing. Regardless, we can see that maximizing ELBO obviously does two good things. First, it maximizes \\(p_\\theta(x)\\), i.e. that the model fits the data well. Secondly, it minmizes \\(E_{q_\\phi(z | x)} \\left[\\log\\left( \\frac{q_\\phi(Z | x)}{p_\\theta(Z | X)} \\right)\\right]\\), or the KL divergence between the approximation and what it’s approximating.\nWe can rewrite the ELBO as (try for HW!): \\[\nE_{q_\\phi(z | x)} \\left[\\log\\left( p_\\theta(x | Z) \\right)\\right]\n- D_{KL}\\left\\{ q_\\phi(Z | x) || p_\\theta(Z) \\right\\}\n\\]\n\n\n11.2.2 Gaussian VAEs\nConsider the following assumptions:\n\n\\(p_\\theta(z)\\) is a multivariate standard normal (i.e. \\(N(J,I)\\)) for \\(J\\) a vector of zeros and \\(I\\) an identity matrix)\n\\(q_\\phi(z | x) = N(\\mu_\\phi(x), \\mbox{Diag}(\\sigma_\\phi(x)))^2\\)\n\\(p_\\theta(x | z)\\) is a multivariate normal with mean \\(\\theta_1(z)\\) and variance matrix \\(\\theta_2 I\\).\n\nWe can write \\(z = \\mu_\\phi(x) + \\mbox{Diag}(\\sigma(x)) \\epsilon\\) where \\(\\epsilon\\) is multivariate standard normal. Moreover, under these assumptions it can be shown that \\[\nD_{KL}\\left\\{ q_\\phi(Z | x) || p_\\theta(Z)\\right\\} = \\frac{1}{2}\\sum_{j=1}^J \\left(1 + \\log\\{\\sigma_{\\phi,j}(x)^2\\} - \\mu_{\\phi,j}(x)^2 - \\sigma_{\\phi,j}(x)^2 \\right)\n\\] where a subscript \\(j\\) refers to the vector component. If we have a Monte Carlo sample, \\(\\epsilon^{(l)}\\) then we can approximate it as follows:\n\\[\n\\begin{aligned}  \n& E_{q_\\phi(z | x)} \\left[\\log\\left\\{ p_\\theta(x | Z) \\right\\}\\right] \\\\\n\\approx & \\frac{1}{L} \\sum_{l=1}^L \\log\\{ p_\\theta(x | z = \\mu_\\phi(x) + \\mbox{Diag}(\\sigma_\\phi(x) \\epsilon^{(l)})\\} \\\\  \n= & -\\frac{1}{2L} \\sum_{l=1}^L ||x - \\theta_1\\{ \\mu_\\phi(x) + \\mbox{Diag}(\\sigma_\\phi(x) \\epsilon^{(l)}\\}||^2 / \\theta_2 \\\\\n\\end{aligned}\n\\]\nBoth of these expressions are the contributions of one row, \\(x\\). They get summed over \\(x\\) to obtain the full ELBO. Here we can see why expressing \\(z = \\mu_\\phi(x) + \\mbox{Diag}(\\sigma_\\phi(x) \\epsilon^{(l)\\) is important. This way our parameters, \\(\\mu_\\phi\\) and \\(\\sigma_\\phi\\) are in the loss function, not hidden in \\(z\\). The backprop algorithm doesn’t know how to take a derivative with respect to a realized random variable. It can, however, differentiate \\(\\mu_\\phi(x) + \\mbox{Diag}(\\sigma_\\phi(x) \\epsilon\\) for realized \\(\\epsilon\\).\n\n\n11.2.3 Example using chest x-rays\nWe’re omitting our imports and gettng the data. See the qmd file for the full list. Here we have epochs as 10 and batch size as 128.\n\n\nUsing downloaded and verified file: /home/bcaffo/.medmnist/chestmnist.npz\n\n\nUsing downloaded and verified file: /home/bcaffo/.medmnist/chestmnist.npz\n\n\nHere’s a montage of the training data.\n\n\n/home/bcaffo/miniconda3/envs/ds4bio/lib/python3.10/site-packages/medmnist/utils.py:25: FutureWarning:\n\n`multichannel` is a deprecated argument name for `montage`. It will be removed in version 1.0. Please use `channel_axis` instead.\n\n\n\n\n\n\nHere’s the shape of the dataset. Note, \\(28^2=784\\) is the vectorized size. Also, as these images are gray scale so that there’s only one color channel. We’re not going to use any convolutional layers or anything like that. We’ll just flatten everything.\n\ntemp, _ = next(iter(train_dataset))\ntemp = temp.numpy()\n[temp.shape, temp.max(), temp.min()]\n\n[(1, 28, 28), 0.93333334, 0.015686275]\n\n\nLet’s use Jackson Kang’s wonderful VAE tutorial from here.\n\nx_dim  = 784 ## This is 28**2, the length of the vectorized images\nhidden_dim = 400\nlatent_dim = 200\n\n\n11.2.3.1 Defining the encoder\nFirst we define the encoder. Remember, the encoder is \\(q_\\phi(z | x)\\), which we are assuming is multivariate independent normals with means and variances depending on \\(x\\). Notice this takes in the input, \\(x\\) and uses a NN to estimate \\(\\mu_\\phi(x)\\) and \\(\\log(\\sigma^2_\\phi(x))\\).\n\nclass Encoder(nn.Module):\n    def __init__(self, input_dim, hidden_dim, latent_dim):\n        super(Encoder, self).__init__()\n        self.FC_input = nn.Linear(input_dim, hidden_dim)\n        self.FC_input2 = nn.Linear(hidden_dim, hidden_dim)\n        self.FC_mean  = nn.Linear(hidden_dim, latent_dim)\n        self.FC_var   = nn.Linear (hidden_dim, latent_dim)\n        self.LeakyReLU = nn.LeakyReLU(0.2)\n        self.training = True\n        \n    def forward(self, x):\n        h_       = self.LeakyReLU(self.FC_input(x))\n        h_       = self.LeakyReLU(self.FC_input2(h_))\n        mean     = self.FC_mean(h_)\n        log_var  = self.FC_var(h_) \n        return mean, log_var\n\n\n\n11.2.3.2 Defining the decoder\nNext we define our decoder. Remember our decoder is \\(p_\\theta(x | z)\\). However, the distribution will be fully defined in the loss function. Instead, we’re just taking in \\(z\\) and giving out \\(\\hat x = \\theta_1(z)\\).\n\nclass Decoder(nn.Module):\n    def __init__(self, latent_dim, hidden_dim, output_dim):\n        super(Decoder, self).__init__()\n        self.FC_hidden = nn.Linear(latent_dim, hidden_dim)\n        self.FC_hidden2 = nn.Linear(hidden_dim, hidden_dim)\n        self.FC_output = nn.Linear(hidden_dim, output_dim)        \n        self.LeakyReLU = nn.LeakyReLU(0.2)\n    def forward(self, x):\n        h     = self.LeakyReLU(self.FC_hidden(x))\n        h     = self.LeakyReLU(self.FC_hidden2(h))\n        x_hat = torch.sigmoid(self.FC_output(h))\n        return x_hat\n\n\n\n11.2.3.3 Defining the full model = autoencoder / decoder\nTo run a datapoint, \\(x\\) through the model. First the encoder generates \\(\\mu_phi(x)\\) and \\(\\sigma_\\phi(x)\\). Next, it generates \\(z\\) from \\(q_\\phi(z|x)\\) using a reparameterization, then passes \\(z\\) through the decoder, \\(p_\\theta(x | z)\\).\n\nclass Model(nn.Module):\n    def __init__(self, Encoder, Decoder):\n        super(Model, self).__init__()\n        self.Encoder = Encoder\n        self.Decoder = Decoder\n\n    def forward(self, x):\n        mean, log_var = self.Encoder(x)\n        sd = torch.exp(0.5 * log_var)\n        z = mean + sd * torch.randn_like(sd)\n        x_hat = self.Decoder(z)\n        return x_hat, mean, log_var\n\n\nencoder = Encoder(input_dim=x_dim, hidden_dim=hidden_dim, latent_dim=latent_dim)\ndecoder = Decoder(latent_dim=latent_dim, hidden_dim = hidden_dim, output_dim = x_dim)\nmodel = Model(Encoder=encoder, Decoder=decoder)\n\n\n\n11.2.3.4 Defining the ELBO\nThe ELBO is the sum of the reproduction loss and the KL divergence.\n\ndef loss_function(x, x_hat, mean, log_var):\n    reproduction_loss = nn.functional.mse_loss(x_hat, x, reduction='sum')\n    KLD      = - 0.5 * torch.sum(1+ log_var - mean.pow(2) - log_var.exp())\n    return reproduction_loss + KLD\n\noptimizer = Adam(model.parameters(), lr=lr)\n\n\n\n11.2.3.5 Training the neural network\n\nmodel.train()\nfor epoch in range(NUM_EPOCHS):\n    for batch_idx, (x, _) in enumerate(train_loader):\n        ## Note unlike the mnist data, the final batch isn't always the same size\n        batch_size = x.shape[0]\n        x = x.view(batch_size, x_dim)\n        optimizer.zero_grad()\n        x_hat, mean, log_var = model(x)\n        loss = loss_function(x, x_hat, mean, log_var)\n        loss.backward()\n        optimizer.step()\n\n\n\n11.2.3.6 Show an image passed through the VAE\n\nmodel.eval()\n\n#with torch.no_grad():\nx, _ = next(iter(test_loader))        \nbatch_size = x.shape[0]\nx = x.view(batch_size, x_dim)\nx_hat, _, _ = model(x)\n\n\nx     =     x.view(batch_size, 28, 28)\nx_hat = x_hat.view(batch_size, 28, 28)\n\nplt.figure(figsize=[10, 5])\n\nidx = 0\nplt.subplot(1, 2, 1)\nplt.imshow(x[idx].numpy(), cmap='gray', vmin = 0, vmax = 1)\nplt.xticks([])\nplt.yticks([])\n\n\nplt.subplot(1, 2, 2)\nplt.imshow(x_hat[idx].detach().numpy(), cmap='gray', vmin = 0, vmax = 1)\nplt.xticks([])\nplt.yticks([])\n\n([], [])\n\n\n\n\n\n\n\n11.2.3.7 Generate a random images from a VAE.\n\nno_im = 5\nnoise = torch.randn(no_im ** 2, latent_dim)\ngenerated_images = decoder(noise)\n\nim = generated_images.view(no_im ** 2, 28, 28)\n\nfor i in range(no_im ** 2):\n    plt.subplot(5, 5, i+1)\n    plt.imshow(im[i].detach().numpy(), cmap='gray', vmin = 0, vmax = 1)\n    plt.xticks([])\n    plt.yticks([])\n\n\n\n\n\n\n\n\nKingma, Diederik P, and Max Welling. 2013. “Auto-Encoding Variational Bayes.” arXiv Preprint arXiv:1312.6114."
  },
  {
    "objectID": "data_analysis_theory.html",
    "href": "data_analysis_theory.html",
    "title": "14  Data science, conceptually",
    "section": "",
    "text": "In this chapter, we’ll focus on data science theory, thinking and philosophy. We’re going to omit any standard treatment of statistical theory and inference, like maximum likelihood optimality and asymptotics, since those are well covered elsewhere. However, it’s worth mentioning that those topics are obviously part of data science theory.\nInstead, we’ll focus on meta-inferential and meta-empirical science questions, as well as some of the conceptual and theoretical language that’s worth knowing."
  },
  {
    "objectID": "data_analysis_theory.html#all-models-are-wrong",
    "href": "data_analysis_theory.html#all-models-are-wrong",
    "title": "14  Data science, conceptually",
    "section": "14.2 All models are wrong",
    "text": "14.2 All models are wrong\n\n14.2.1 Wrong means wrong\n“All models are wrong, but some are useful”, or some variant, is a quote from statistician George Box that is so well known and used that it has a lengthy wikipedia page. Restricting our attention to probabilistic models, it is interesting to note that this quote, which is near universally agreed upon, has implications that are often not. For example, the quote suggests that there is not, and never has been: an IID sample, normally distributed data (as in having been generated from a normal distribution), a true population stochastic model … In other words, there is no correct probabilistic model, ever, ever, ever (according to the quote).\nOne way to interpret this is that there are correct probability models, we just haven’t found them yet, or maybe can’t find them via some incompleteness law. If we ever find one, I guess we’d have to change the quote. But, I don’t think the quote is implying the existence of true probabilistic models that we don’t, or can’t, know. I tend to think it is suggesting that randomness doesn’t exist and hence probabilitity models are, like Newtonian mechanics, just models, not truth.\nThis is a well discussed topic in philosophy. On some meaningful level, the quote is obviously true. Most of the things we’re interested in public health are clearly purely functionally caused by antecedent variables, some of which we know and can measure and some of which we can’t. This is obviously true of of things like die rolls or casino games. Perhaps the most perfect example is random number generators, where we know the actual deterministic formula generating the numbers yet they are designed to appear as random as possible.\nBut does ranomness exist for some weird quantum setting, or is it just a useful model? The best answer for this question came from a famous result called Bell’s theorem, which suggests that hidden variables are not the missing ingridient to explain quantum phenomena. This theory has since been corraborated via experiments. To explain this theory using pure determinism (i.e. hidden variables) requires giving up some important notions in this area, such as locality. How much of a proof of the existence of randomness the theorem and experiments are is debatable (and heavily debated). Bell’s theorem notwithstanding, it still remains in question whether these measurements are truly random in some sense or just well modeled by randomness. But, I’ll stop here, since I don’t understand this stuff at all.\nFor our purposes, this seems irrelevant. Even if randomness does exist at the scale of the extremely small, our data generally references systems where we believe that determinism holds. There is no Bell’s theorem of disease. Typically, we think observed and hidden variables explain the relevant majority of an individual or population’s state of health. Moreover, regardless if some models are right or it’s just true that all stochastic models are wrong, many models are extremely useful. It is more important to be able to accurately represent one’s assumptions and how they connect to the experimental setting than it is to argue that one’s assumptions are true on some bizarre abstract level. So, my recommendation is to ignore this line of thinking entirely, and instead focus on being able to articulate your assumptions and describe what it is you’re treating as random, regardless of whether or not it actually is."
  },
  {
    "objectID": "data_analysis_theory.html#some-models-are-useful",
    "href": "data_analysis_theory.html#some-models-are-useful",
    "title": "14  Data science, conceptually",
    "section": "14.3 Some models are useful",
    "text": "14.3 Some models are useful\nDo we even care if models are ultimately correct? The quote ends with, “some models are useful”. How are they useful? The smallest meaningful statement that I can come up with is a model is useful if it convinces you or someone else of a true statement. This allows for the possibility that a poorly fitting, or obviously incorrect model, can still be useful.\nScott Zeger had a wonderful quote, I recall, which is that models are lenses through which we look at data. I really like this statement, since it mirrors the richness of ways that we can use models. Infrared, satellite and microscopic images all tell very different stories, but no one is the true depiction of world, though all are useful. The idea of models as lens through which we look at data is further explored in the research on exploratory modeling (Wickham 2006). The idea is to use modeling in the same way that we do exploratory data analysis.\nAnother useful definition of a model is that it connects our data to a hypothetical or actual population. This requires defining the population of interest. Having a good sense of the target population is an important step in data analysis. Without this step, we are creating estimators without estimands. Defined this way, our model helps us generalize our results beyond our sample. Kass discusses this idea of modeling in (Kass 2011). He expouses a form of “statistical pragmatism”. Notably, he makes a stark delineation between statisticl concepts and the “real world”. As an example\n\nStatistical inferences of all kinds use statistical models, which embody theoretical assumptions. … like scientific models, statistical models exist in an abstract framework; to distinguish this framework from the real world inhabited by data we may call it a “theoretical world.” Random variables, confidence intervals, and posterior probabilities all live in this theoretical world. When we use a statistical model to make a statistical inference we implicitly assert that the variation exhibited by data is captured reasonably well by the statistical model, so that the theoretical world corresponds reasonably well to the real world. Conclusions are drawn by applying a statistical inference technique, which is a theoretical construct, to some real data."
  },
  {
    "objectID": "data_analysis_theory.html#discussing-randomness-in-modeling",
    "href": "data_analysis_theory.html#discussing-randomness-in-modeling",
    "title": "14  Data science, conceptually",
    "section": "14.4 Discussing randomness in modeling",
    "text": "14.4 Discussing randomness in modeling\nIt is almost always a useful exercise to ask, “What exactly is it that I’m treating as if random in a problem?”. Consider the following question:\n\nI create a confidence interval using the binomial distribution for the prevalance of a disease from a cohort study.\nIn the same study, I fit a regression model on a continue measure of the disease outcome with an exposure predictor along with confounders.\n\nIn 1., it seems natural to say that it’s the sampling that’s random. In other words, there’s a population percentage of people with the disease and each subject in my sample is a random draw from that distribution. In 2. a reasonable assumption would be that my errors are an accumulation of unmodeled factors and that this accumulation is well modeled by assuming that the errors are IID.\nGerenerally a few commmon themes arise when discussing randomness in models, along with some combination of these things. In addition, there is a distinction in how one views randomness from Bayesian or frequency perspectives. This is covered well elsewhere. Finally, there are ideas, such as exchangeability, which can offer weaker assumptions and still provide inference.\nHere we’ll discuss two major directions of randomness that are discussed in statistics.\n\n14.4.1 Design based randomness\nIn randomized trials and finite population studies, often the randomness being modeled is the randomness in the design. This has a benefit of very good knowledge of the source of randomness if the design was actually followed. Researchers will also make inferences using design based arguments, even if that design was not utilized. For example, imagine estimating a prevalence from a sample that wasn’t actually random. You could make a statement such as “A 95% confidence interval pevalence would be [0.3, 0.70] if the sample were random.”\nIt should be noted that using randomization for design based inference only makes statements about your specific sample. For example, let \\(Y_{i}(j)\\) be the outcome one would have seen if subject \\(i\\) received treatment \\(j=0,1\\). We then observe \\(Y_i(T_i) = Y_i\\) where \\(T_i\\) is the actual treatment. Our estimate of the treatment effect is:\n\\[\nE_{obs} =\n\\frac{\\sum_{i : T_i = 1} Y_{i}}{n_1}\n-\n\\frac{\\sum_{i : T_i = 0} Y_{i}}{n_0}\n\\]\nConsider calculating a P-value under the null assumption \\(Y_i(1) = Y_i(0) = Y_i\\) by redoing the randomization. Let \\(T_i^{(m)}\\) where \\(m\\) is resample be a hypergeometric (i.e. observing the sample sizes \\(n_1\\) and \\(n_0\\)) reallocation of treatment assignment.\n\\[\nE_m =\n\\frac{1}{M}\n\\left(\\frac{\\sum_{i : T_i^{(m)} = 1} Y_{i}}{n_1}\n-\n\\frac{\\sum_{i : T_i^{(m)} = 0} Y_{i}}{n_0}\\right)\n\\]\nThen a P-value could be calculated using \\[\n\\frac{1}{M} \\sum_{m=1}^M I(|E_m| > |E_{obs}|).\n\\]\nNotice, this inference is about the sample we observed, \\(Y_i\\), and maybe their counterfacultuals. But, without further assumptions conclusions would be relative to the sample. Typically, one does make these further assumptions, but note they typically require less direct knowledge of the source of randomness.\nSampling variation deserves a special mention as design based inference. Very often we make iid (random) sampling assumptions even if the assumptions are obviously false. It is important to acknowledge that conclusions are under this assumption.\n\n\n14.4.2 Accumulated errors\nIn many settings we assume that we are modeling mechanisms, for example when modeling \\(Y=f(x)+\\epsilon\\). Often, we assume that \\(\\epsilon\\) is comprised of acculated errors from unmodeled variables. There is further an assumption that these errors accumulate in a way that is well modeled by randomness. Some examples where this fails to be true is when we omit an important confounder from our model. There, the errors are systematic in a way that is essential for understanding the scientific phenomena that we are studying. In a later chapter, we’ll discuss causal diagrams and think about what variables we include and exclude from our models and how to unmodeled variables can influence our results."
  },
  {
    "objectID": "data_analysis_theory.html#summary",
    "href": "data_analysis_theory.html#summary",
    "title": "14  Data science, conceptually",
    "section": "14.5 Summary",
    "text": "14.5 Summary\nIt’s important to emphasize that rather than thinking of one way to think about modeling as being right and others wrong, one should be able to describe with precision what they are doing in their analysis. Here are some examples of questions one might ask themselves:\n\nWhat am I trying to accomplish with this model?\nIs there a (potentially hypothetical) population that is being generalized, what is it, and what are we estimating from it?\nHow were the data collected and does our model connect to the design well?\nWhat exactly is it that we’re modeling as if random?\nAre the modeling assumptions reasonable and is there any data evidence of that?\nHow robust are our conclusions to our modeling assumptions; have we tried multiple models?\n\nThis is perhaps just a subset of the questions one should ask themselves. I wish I could be more prescriptive, with a checklist or something like that. However, how models are used differs a great deal by the various scientific communities, within a community and even within individual statisticians over time."
  },
  {
    "objectID": "data_analysis_theory.html#example",
    "href": "data_analysis_theory.html#example",
    "title": "14  Data science, conceptually",
    "section": "14.6 Example",
    "text": "14.6 Example\nHere’s a neat example of using sampling assumptions to make an inference that could create some discussion. Assume repeated sampling sampling to capture bunnies in a field on two occasions where the bunnies are tagged on the first occasion if caught. The goal is to figure out the number of bunnies in the field.\n\n\n\n\nCaught 2\nNot caught 2\n\n\n\n\nCaught 1\n\\(n_{11}\\)\n\\(n_{12}\\)\n\n\nNot caught 1\n\\(n_{21}\\)\n\\(n_{22}\\)\n\n\n\nNote, we do not get to see \\(n_{22}\\) and knowledge of this number is equivalent to knowing the population size. Let \\(N=\\sum_{i,j} n_{ij}\\) be the total sample size and \\(\\pi_{ij}\\) be the probability of capture or not on occasion \\(i\\) and \\(j\\) so that \\(E[n_{ij}] = N \\pi_{ij}\\). Assume capture between occasions is indepnedent and where each bunny lands is an independent draw from a multinomial distribution with probabilities \\(\\pi_{ij}\\). Under the independence assumption the odds ratio is 1. Therefore, we assume \\[\n\\frac{n_{11}n_{22}}{n_{12}n_{22}} \\approx 1\n\\] Setting the left and right equal, we can solve for \\(n_{22}\\) as \\[\n\\hat n_{22} = \\frac{n_{12}n_{22}}{n_{11}}\n\\]\nQuestions:\n\nWhat are we assuming is random in this estimation process?\nHow can we make inference using these assumptions?\nIs the model likely to be useful?\nHow could we relax the assumptions?"
  },
  {
    "objectID": "data_analysis_theory.html#reading",
    "href": "data_analysis_theory.html#reading",
    "title": "14  Data science, conceptually",
    "section": "14.7 Reading",
    "text": "14.7 Reading\n\nKass discusses statistical inference and modeling (Kass 2011)\nWickham, Exploratory Model Inference (Wickham 2006)\nUnwin and Volinsky on Exploratory Model Inference (Unwin, Volinsky, and Winkler 2003)\nBreiman discussing modeling (Breiman 2001)\nDiaconis discussing magical thinking (Diaconis 2006)\n\n\n\n\n\nBreiman, Leo. 2001. “Statistical Modeling: The Two Cultures (with Comments and a Rejoinder by the Author).” Statistical Science 16 (3): 199–231.\n\n\nDiaconis, Persi. 2006. “Theories of Data Analysis: From Magical Thinking Through Classical Statistics.” Exploring Data Tables, Trends, and Shapes, 1–36.\n\n\nKass, Robert E. 2011. “Statistical Inference: The Big Picture.” Statistical Science: A Review Journal of the Institute of Mathematical Statistics 26 (1): 1.\n\n\nUnwin, Antony, Chris Volinsky, and Sylvia Winkler. 2003. “Parallel Coordinates for Exploratory Modelling Analysis.” Computational Statistics & Data Analysis 43 (4): 553–64.\n\n\nWickham, Hadley. 2006. “Exploratory Model Analysis.”"
  },
  {
    "objectID": "stat_language.html",
    "href": "stat_language.html",
    "title": "15  Statistics and language",
    "section": "",
    "text": "Statistics is a complex discipline requiring a large collection of ideas and definitions. Non-experts in the field might be surprised to find out that many common statistical terms lack a unified defition among experts, even within a statastical paradigm or application context. More troubling, experts’ personal defintions of concepts believed to be essential can be divergent, or even contradictory, to that of other experts. One such examples it the term “representative sample”, which is nearly universally thought of as an important concept, yet has no unified definition within the field of statistics. This fact was noted and largely studied by the celebrated statisticians William Kruskal and Frederick Mosteller in a series of articles in the late 1970s and early 1980s (Kruskal and Mosteller 1979a, 1979b, 1979c, 1980). These articles gave nine definitions of a representative sample and surveyed the statistical and application literature demonstrating that all nine of the uses occur commonly.\n\n\nWe’ll return to “representative sampling” in a bit. For the time being, consider the phrase “the data are Gaussian” or “the data are normal”. What do these statements mean? Some possibilities\n\nThe data, when plotted via a histogram or whatever, looks like a Gaussian distribution.\nA statistical test for non-Guassianity failed to reject.\nThe data were actually generated from a Gaussian distribution.\nThe data is not inconsistent with having been generated from a normal distribution.\nIID draws from a normal distribution is an accurate model of the data generating process.\n\nThese are all very related, but different, statements. For example, a small dataset can be simulated from a Gaussian distribution (3 or 5 depending on your point of view), yet not look at all like a Gaussian distribution (fails 1 or 2). Data can reject a test of non-Gaussianity yet (fails 2.) yet appears Gaussian in a plot to many observers (1.). One could argue (and I believe) 3 is never true.\nThis is an issue, since it’s generally not clear from the context what the person speaking the phrase actually means and 1-5 can have different implications for interpretation. Kass (2011) discusses this phrase explicitly:\n\n… “Let us assume the data are normally distributed” and we proceed to make a statistical inference, we do not need to take these words literally as asserting that the data form a random sample. Instead, this kind of language is a convenient and familiar shorthand for the much weaker assertion that, for our specified purposes, the variability of the data are adequately consistent with variability that would occur in a random sample. This linguistic amenity is used routinely in both frequentist and Bayesian frameworks.\n\nHere, he clearly is asserting something like 4 or 5. Note the specific addition of the word “assume”, which is doing a fair amount of work in the sentence. Hihgly experienced statisticians, like Dr. Kass, have built up an intuitive sense of exactly how much detail to include when writing about analyses. For those newer to the field, I would suggest the spelling things out more until this sense is developed: “We assume that the data are a random sample from a normal distribution, which is supported by our exploratory plots”.\n\n\n\nIn survey sampling, sample representativeness has a fairly strict definition. A sample is representative if observed predictor variables have the same distribution as the population you would like to generalize to [FIND A REFERENCE FOR THIS]. If not, weights are often used to enforce this. Outside of this domain, the phrase is basically meaningless.\nThe largest breakdown in calling a sample representative is whether or not it is a description of the process or a description of the sample itself. For example, consider these questions:\n\nAre all random samples representative?\nDoes representativeness require luck even if the sampling is random?\nIf you know that a sample is representativve, do you need statistical inference?\n\nThe most common definition of a representative sample is one that has inherited key characteristics of the population, which is the strongest version of the term. This assumes that we were lucky with our sample and somehow got a mirror of the population. This definition is problematic. It assumes that key aspects of the target population are simply known and, in contradiction, also need to be treated with uncertainty.\n\n\n\nRepresentative sampling\n\n\nAs an example consider the image above. Are the data representative of the normal distribution outlined in the figure? All five points being above the population mean lead many to conclude that the sample is not representative. Five points being above the mean occurs with 1/32 = 3.125% probability for a random sample from any symmetric distribution. So, even for this small of a dataset, it’s unlikely. However, if one is to assume that a representative sample must have at least one measurement on either side of the mean, a finite 100% confidence interval for the mean could be obtained as the min and max. No statistician would argue the point that a 100% interval can be obtained via the min and max. So our assumption of sample representativeness lies in direct contrast with our probability model. Our intution of sample representativeness makes absurdly strong assumptions.\nBizarrely, researchers often invoke representativeness, but do not exploit the assumption. Typically, we assume process representativeness; that is that the sample is random. Perhaps the most charitable way to summarize this line of thinking is: hoping for the best (assuming sample representativeness), but preparing for the worst (assuming process representativeness).\nThis idea of hoping for the best while preparing for the worst perhaps belies a general distrust of formal statistical inference, even among practicing statisticians. That is, by peforming standard statistical inference, such as confidence or credible intervals, one is addressing uncertainty in the best ways known. However, an assumption of sample representative appears to be the hope of actually realized correct evidence.\nAn assumption of sample representativeness allows researchers to keep their analyses more fully within the data, avoiding or downplaying the role of formal inference. One does not need to generalize to the population if one has a realized mirror image of it in hand. This would fall in line with the idea of a preference of thinking in the terms of sample representativeness as another instance of preference for the more direct and literal System I thinking over labored and conceptual System II thinking.\nKahneman and Tversky (1972) argue on the psychological basis for representativeness\n\n\npeople expect samples to be highly similar to their parent population, and also to represent the randomness of the sampling process (Tversky & Kahneman, 1971, 1974); (ii) people often rely on representativeness as a heuristic for judgment and prediction (Kahneman & Tversky, 1972, 1973).\n\n\n\n\n\nA common phrase in biostatistics is that of balance with respect to a treatment. Let \\(Y\\) be an outcome, \\(T\\) be a treatement and \\(X\\) be a confounder. Balance typically refers to the relationship between \\(X\\) and \\(T\\). However, what is not at all clear from most contexts is whether blance refers to the process, i.e. randomization, or it refers to actual balance in the observed sample. In this case, however, both uses are interesting in their own right. I would suggest saying something like “observed balance” when talking about the sample and “design balance” when talking about the process.\nIt is important to emphasize that the distinction has meaning. Consider the distinction between observed and design balance in a trial. Imagine a randomized trial that was design balanced but balance was not observed in the data. For example, consider studying an Alzheimer’s disease drug and the treatment was randomly assigned. However, simply by chance, there were more subjects who had a poor pre-treatment memory trajectory assigned to the placebo than the control. A signficant effect was observed when disregarding the pre-treatment trajectory and when factoring in the pre-treatment trajectory it was not signficant. That is, is the treatment effect just due to the fact that more people assigned the treatment were on a bad path with respect to their disease or was it due to the treatment itself? Which one should you conclude?"
  },
  {
    "objectID": "stat_language.html#summary",
    "href": "stat_language.html#summary",
    "title": "15  Statistics and language",
    "section": "15.2 Summary",
    "text": "15.2 Summary\nOur statistical models connect our data to a population. Our language connects our models to an audience. Tukey described the need for fuzzy concepts in applied data analysis [CITATION]. However, we could all try to commit to eliminate unnecessary fuzzy concepts. The most important of these is differenatiating between sample properties and process or conceptual processes."
  },
  {
    "objectID": "stat_language.html#reading",
    "href": "stat_language.html#reading",
    "title": "15  Statistics and language",
    "section": "15.3 Reading",
    "text": "15.3 Reading\n\nKaplan, Rogness, and Fisher (2014)\nKahneman and Tversky (1972)\n\n\n\n\n\nKahneman, Daniel, and Amos Tversky. 1972. “Subjective Probability: A Judgment of Representativeness.” Cognitive Psychology 3 (3): 430–54.\n\n\nKaplan, Jennifer J, Neal T Rogness, and Diane G Fisher. 2014. “Exploiting Lexical Ambiguity to Help Students Understand the Meaning of Random.” Statistics Education Research Journal 13 (1): 9–24.\n\n\nKass, Robert E. 2011. “Statistical Inference: The Big Picture.” Statistical Science: A Review Journal of the Institute of Mathematical Statistics 26 (1): 1.\n\n\nKruskal, William, and Frederick Mosteller. 1979a. “Representative Sampling, i: Non-Scientific Literature.” International Statistical Review/Revue Internationale de Statistique, 13–24.\n\n\n———. 1979b. “Representative Sampling, II: Scientific Literature, Excluding Statistics.” International Statistical Review/Revue Internationale de Statistique, 111–27.\n\n\n———. 1979c. “Representative Sampling, III: The Current Statistical Literature.” International Statistical Review/Revue Internationale de Statistique, 245–65.\n\n\n———. 1980. “Representative Sampling, IV: The History of the Concept in Statistics, 1895-1939.” International Statistical Review/Revue Internationale de Statistique, 169–95."
  },
  {
    "objectID": "data_science_as_a_science.html",
    "href": "data_science_as_a_science.html",
    "title": "16  Data science as an applied science",
    "section": "",
    "text": "The term “Data science” is typically used to refer to a set of tools, techniques and thought processes used to perform scientific inquiries using data. But is that a scientific topic worthy of study in its own right? It is. And, from a practical, theoretical and philosophical level, it’s already extremely well studied in statistics, computer science, engineering and philosophy departments.\nDespite these fields, at the Data Science Lab at JHU, we’ve had lengthy discussions about data science as an inductive, empirical applied science, like biology or medicine, rather than an deductive discipline, like mathematics or statistical theory, or rather than a set of heuristics, like rules of thumb and agreed upon best practices. An inductive, empirical, applied science itself needs data science, since it’s empirical and thus depends on data. So, maybe there’s an infinite regress of some sort making this an ultimately doomed endeavor. But, nevertheless, we persist.\nThere are some fields of data analysis that are well covered, let’s talk about them first.\n\n\nGeneral EDA and visualization is covered in the next chapter. There is a vast literature of experiments to understand perception of visual data. This is a more neatly circumscribed area of data science as a science. This is possibly because the general field of visual perception, from a variety of angles, is well developed. Let’s cover a specific example, observer studies in radiology.\n\n\nIn diagnostic radiology, new technology in the form of new machines for imaging or new ways of processing images, are being invented constantly. To evaluate these new technologies, a gold standard is to have randomized studies where the underlying truth is known. The images from the new technology and images from a control technology are then randomized to observers. Several issues come about in observer studies. First, establishing truth can be hard. This is often done by digital or physical phantoms or in instances where patients are longitudinally followed up and the disease status is made apparent. A second issue is in the cost associated with observers. Often, instead of attending radiologists, the studies are done with residents or fellows, or students who have received specific training to qualify as reasonable proxies. An interesting alternative approach is to have digital observers.\nMy friends at the JHU Division of Medical Imaging Physics do this quite well. In one process, they first create highly accurate models of the human/non-human animal being studied (so called pantoms, see here). Next they create accurate models of the imaging system, say X-Ray CT or positron imaging. Suppose that they want to study two different ways of performing tomography, say a Bayes algorithm or an EM algorithm. They take generated images from their digital phantom and process them using the two candidate algorithms. Then they use human or mathematical observers to try to diagnose the disease using the processed images. Here’s some examples He et al. (2004).\n\n\n\n\nOne of the most celebrated versions of data science as a science is the understanding of how we perceive chance, randomness and decision making under uncertainty, by Kahneman, Tversky, Slovic and many others. A good starting point for this is the book Kahneman et al. (1982).\nOne key point is that we all, regardless of training, rely on heuristics and biases to mentally represent uncertainty. An example is the lack of use of prior probabilities\n\nSubjects were shown brief personality descriptions of several individuals, allegedly sampled at random from a group of 100 professionals - engineers and lawyers. The subjects were asked to assess, for each description, the probability that it belonged to an engineer rather than to a lawyer. In one experimental condition, subjects were told that the group from which the descriptions had been drawn consisted of 70 engineers and 30 lawyers. In another condition, subjects were told that the group consisted of 30 engineers and 70 lawyers. The odds that any particular description belongs to an engineer rather than to a lawyer should be higher in the first condition, where there is a majority of engineers, than in the second condition, where there is a majority of lawyers. Specifically, it can be shown by applying Bayes’ rule that the ratio of these odds should be (.7/.3)2, or 5.44, for each description. In a sharp violation of Bayes’ rule, the subjects in the two conditions produced essentially the same probability judgments. Apparently, subjects evaluated the likelihood that a particular description belonged to an engineer rather than to a lawyer by the degree to which this description was representa￾tive of the two stereotypes, with little or no regard for the prior probabili￾ties of the categories.\n\nA major thrust of this work was on sample representativeness (Kahneman and Tversky 1972). They summarize their findings as:\n\n\npeople expect samples to be highly similar to their parent population, and also to represent the randomness of the sampling process (Tversky & Kahneman, 1971, 1974); (ii) people often rely on representativeness as a heuristic for judgment and prediction (Kahneman & Tversky, 1972, 1973).\n\n\nThese biases creep into our judgements. Kahneman and Tversky (1972) discusses people’s ability to rank \\(P(A)\\) and \\(P(A\\cap B)\\), the latter being mathematically guaranteed to be the smaller. They observed a strong conjunction bias. For example:\n\nLinda is 31 years old, single, outspoken and very bright. She maJored in philosophy. As a student, she was deeply concerned with issues of discrimination and social justice, and also participated in anti-nuclear demonstrations.\n\nWhen asked to rank the probability of “Linda is a bank teller” (T), “Linda is active in the feminist movement.” (F) versus “Linda is a bank teller and active in the feminist movement” (T&F). They similarly describe Bill as having stereotypical accountant traits and ask to rank “Bill is an accountant.”, “Bill plays jazz for a hobby.” and “Bill is an accountant who plays jazz for a hobby.” In 90% of the cases, the subjects ranked the intersection in between the two marginal probabilities (eg. ranking F > T&F > T for Linda).\n\n\n\n\nGilland, Karen L, Benjamin MW Tsui, Yujin Qi, and Grant T Gullberg. 2006. “Comparison of Channelized Hotelling and Human Observers in Determining Optimum OS-EM Reconstruction Parameters for Myocardial SPECT.” IEEE Transactions on Nuclear Science 53 (3): 1200–1204.\n\n\nHe, Xin, Eric C Frey, Jonathan M Links, Karen L Gilland, William P Segars, and Benjamin MW Tsui. 2004. “A Mathematical Observer Study for the Evaluation and Optimization of Compensation Methods for Myocardial SPECT Using a Phantom Population That Realistically Models Patient Variability.” IEEE Transactions on Nuclear Science 51 (1): 218–24.\n\n\nKahneman, Daniel, Stewart Paul Slovic, Paul Slovic, and Amos Tversky. 1982. Judgment Under Uncertainty: Heuristics and Biases. Cambridge university press.\n\n\nKahneman, Daniel, and Amos Tversky. 1972. “Subjective Probability: A Judgment of Representativeness.” Cognitive Psychology 3 (3): 430–54."
  },
  {
    "objectID": "graphics.html",
    "href": "graphics.html",
    "title": "17  Theory of graphical display",
    "section": "",
    "text": "One of the main design arguments for the graphical display of information is data / ink maximization Tufte (1990). This is the idea that idea that as much of the “ink” (non-background pixels) of the plot as possible should be displaying data.\nData/ink maximalization has been criticized empirically. For example, Inbar, Tractinsky, and Meyer (2007) conducted a study with 87 undergraduates and found a clear preference for the non-maximized variations. Another line of argument discusses the “paradox of simplicity” Norman (2007), Eytam, Tractinsky, and Lowengart (2017), whereby we have a strong aesthetic preference for simplicity, but also want flexibility and maximum utility.\n\nBertin (1983)\n\n\n\n\nWickham et al. (2010)\n\n\n\n\n\nCleveland (1987)\nCleveland and McGill (1984)\nCleveland and Devlin (1980)\nCarswell (1992)\nCleveland and McGill (1986)\nMagical thinking Diaconis (2006)"
  },
  {
    "objectID": "graphics.html#implementation",
    "href": "graphics.html#implementation",
    "title": "17  Theory of graphical display",
    "section": "17.2 Implementation",
    "text": "17.2 Implementation\n\n17.2.1 Grammar of graphics\n\nWilkinson (2012)\nWilkinson (2013)\nWickham (2010)\n\n\n\n17.2.2 Narative storytelling\nEdward and Jeffrey (Segel and Heer (2010)) argue regarding the use of modern interactive tools in data narrative storytelling. They give seven canonical genres of narrative visulation."
  },
  {
    "objectID": "graphics.html#graph-galleries-and-further-reading",
    "href": "graphics.html#graph-galleries-and-further-reading",
    "title": "17  Theory of graphical display",
    "section": "17.3 Graph galleries and further reading",
    "text": "17.3 Graph galleries and further reading\n\n17.3.1 Further reading\n\nKarl Broman on How to display data badly\nKarl Broman Data Vizualization\nKarl Broman 10 worst plots\nKarl Broman Data visualization\n\n\n\n17.3.2 Graph galleries\n\nR graph gallery\nMatplotlib graph gallery\nPlotly\nD3 gallery\nVega gallery\nSeaborn\n\n\n\n17.3.3 Historically famous graphics\n\nhttps://medium.com/stotle-inc/the-greatest-graph-in-history-1155e0c25671Z\nhttps://plotlygraphs.medium.com/seven-modern-remakes-of-the-most-famous-graphs-ever-made-8ef30da1ab00\nhttps://www.datavis.ca/gallery/historical.php\nhttps://towardsdatascience.com/a-short-history-of-data-visualisation-de2f81ed0b23\nhttps://www.tableau.com/learn/articles/best-beautiful-data-visualization-examples\n\n\n\n17.3.4 Infographics in the media\n\nhttps://www.nytimes.com/spotlight/graphics\nJHU covid map\n\n\n\n\n\n\nBertin, Jacques. 1983. Semiology of Graphics. University of Wisconsin press.\n\n\nCarswell, C Melody. 1992. “Choosing Specifiers: An Evaluation of the Basic Tasks Model of Graphical Perception.” Human Factors 34 (5): 535–54.\n\n\nCleveland, William S. 1987. “Research in Statistical Graphics.” Journal of the American Statistical Association 82 (398): 419–23.\n\n\nCleveland, William S, and Susan J Devlin. 1980. “Calendar Effects in Monthly Time Series: Detection by Spectrum Analysis and Graphical Methods.” Journal of the American Statistical Association 75 (371): 487–96.\n\n\nCleveland, William S, and Robert McGill. 1984. “The Many Faces of a Scatterplot.” Journal of the American Statistical Association 79 (388): 807–22.\n\n\n———. 1986. “An Experiment in Graphical Perception.” International Journal of Man-Machine Studies 25 (5): 491–500.\n\n\nDiaconis, Persi. 2006. “Theories of Data Analysis: From Magical Thinking Through Classical Statistics.” Exploring Data Tables, Trends, and Shapes, 1–36.\n\n\nEytam, Eleanor, Noam Tractinsky, and Oded Lowengart. 2017. “The Paradox of Simplicity: Effects of Role on the Preference and Choice of Product Visual Simplicity Level.” International Journal of Human-Computer Studies 105: 43–55.\n\n\nInbar, Ohad, Noam Tractinsky, and Joachim Meyer. 2007. “Minimalism in Information Visualization: Attitudes Towards Maximizing the Data-Ink Ratio.” In Proceedings of the 14th European Conference on Cognitive Ergonomics: Invent! Explore!, 185–88.\n\n\nNorman, Donald A. 2007. “Simplicity Is Highly Overrated.” Interactions 14 (2): 40–41.\n\n\nSegel, Edward, and Jeffrey Heer. 2010. “Narrative Visualization: Telling Stories with Data.” IEEE Transactions on Visualization and Computer Graphics 16 (6): 1139–48.\n\n\nTufte, ER. 1990. “Data-Ink Maximization and Graphical Design.” Oikos, 130–44.\n\n\nWickham, Hadley. 2010. “A Layered Grammar of Graphics.” Journal of Computational and Graphical Statistics 19 (1): 3–28.\n\n\nWickham, Hadley, Dianne Cook, Heike Hofmann, and Andreas Buja. 2010. “Graphical Inference for Infovis.” IEEE Transactions on Visualization and Computer Graphics 16 (6): 973–79.\n\n\nWilkinson, Leland. 2012. “The Grammar of Graphics.” In Handbook of Computational Statistics, 375–414. Springer.\n\n\n———. 2013. The Grammar of Graphics. Springer Science & Business Media."
  },
  {
    "objectID": "causal.html",
    "href": "causal.html",
    "title": "18  Causal DAGs",
    "section": "",
    "text": "Causal models differ from associational models in that they codify causal directions not just associations. In our program, you might have learned of the use of propensity scores, counterfactuals or randomization to study causality. There, typically the goal is to make causal statements with as few assumptions as possible or at least understanding the assumptions. Typically the object of study is the estimation of an effect avergated over covarites.\nCausal graphs take a different approach, even if they wind up at a similar same place. Here, the goal is to postulate hyptothetical causal relationships and use those hypothetical relationships to estimate causal effects. In that sense, they use more assumptions. But, on the other hand, the very act of postulating causal relationships has a benefit unto itself. Moreover, causal graphs intersect with understanding causal theory more generally.\n\n\nA graph, \\(G\\) is a collection of nodes, say \\(V=\\{1,\\ldots, p\\}\\) and a set edges between the nodes, i.e. a set of elements \\((i,j)\\). The graph is directed if \\((i,j)\\) is considered different then \\((j,i)\\).\nNode \\(i\\) is a parent of node \\(j\\) if \\((i,j) \\in E\\) and \\((j,i)\\notin E\\). Similarly, node \\(i\\) is a child of node \\(j\\) if \\((j,i) \\in E\\) and \\((i,j)\\notin E\\). A node is a descendant of another if it is a child, or a child of a child and so on.\n\n\n\nA structural causal model (SCM) over a collection of variables, \\(X=(X_1, \\ldots, X_p)\\), postulates a set of functional relationships \\[\nX_j = f(P_j, \\epsilon_j)\n\\] where \\(P_j\\) are the antecedent causes of \\(X_j\\), called the parents of \\(X_j\\), and \\(\\epsilon_j\\) is an accumulation of variables treated as mutally independent. This defines a directed graph, \\(G\\) say, where a graph is collection of vertices corresponding to our variables, \\(V=\\{1,\\ldots, p\\}\\), corresponding to the \\(X_i\\), and edges, \\(E\\), which is a set of ordered pairs of nodes.\n\n\n\n\n\nIn this case, \\(P_1 = \\{\\}\\), \\(P_2 = \\{1\\}\\) and \\(P_3 = \\{1,2\\}\\). SCMs define a unique probability distribution over \\(X\\).\nGiven a cross-sectional sample, we can estimate the joint distribution of \\(P(X_1,\\ldots, X_p)\\) and all of its conditionals. These conditionals must agree with the causal graph in a sense. Specifically, it agrees with the conditional independence (Markov) model implied by the associated undirected graph. Causality comes in from our assuming arrows and using these assumptions in a specific way.\nOne specific way in which we use the assumptions is to investigate how the graph changes when we fix a node at a specific value, like an intevention, thus breaking its association with its parents. This operation is conceptual, but at times we can relate probabilities associated with interventions that were not realized. Consider an instrance where where \\(X_1\\) is a collection of confounders, \\(X_2\\) is an exposure and \\(X_3\\) is an outcome. Ideally, we’d like to know \\[\nP(X_3 ~|~ do(X_2) = x_2)\n\\] That is, the impact on the response if we were to set the exposure to \\(e_0\\).\n\n\n\nReturn to our previous diagram.\n\n\\(X1\\) is a confounder betweend \\(X2\\) and \\(X3\\)\n\\(X2\\) is a mediator between \\(X1\\) and \\(X3\\)\n\\(X3\\) is a collider between \\(X1\\) and \\(X2\\)\n\nConsider an example. \\(X1\\) is having a BMI > 35, \\(X2\\) is sleep disordered breathing and \\(X3\\) is hypertension.\n\n\n\n\n\nHere if we’re studying whether SDB causes HTN, BMI35 confounds the relationship as a possible common cause of both. We would need to adjust for BMI35 to make sure the association between SDB and HTN isn’t just due to this common cause.\nIf we were studying whether BMI35 causes HTN, we might be interested in how much of that effect is mediated (indirectly) through SDB and how much is directly from BMI35.\nIf we are studying the relationship between BMI35 and SDB directly, adjusting for HTN may cause an association. Consider the (fictitious) case where there is a large number of people who have SDB who are not obese, yet all have hypertension, for whatever the reason. Then, among the HTN, there could be a negative association between BMI35 and SDB, because of the large collection of patients would who have SDB and are not obese and the same for obese and not hyptertensive. That is, adjusting for HTN created an association. This is an example of Berkson’s paradox. This is a somewhat contrived example, but hopefully you get the point. The wikipedia article has a funny example where they consider \\(X_1\\) is whether or not the hamburger is good at a fast food restaurant, \\(X_2\\) is whether or not the fries are good and \\(X_3\\) is whether or not people eat there. Since few people would eat at a place where both the hamburger and fries are bad, conditioning on \\(X_3\\) can create a negative association.\nThe main point here is that adjusting for colliders may open up a pathway between the nodes.\nA path between two nodes \\(n_1\\) and \\(n_k\\) is a sequence of nodes, \\(v_1, v_2,\\ldots v_{k}\\), where \\(v_{i}\\) and \\(v_{i+1}\\) are connected. The path is directed if \\(v_{i}\\) points to \\(v_{i+1}\\) for \\(i=1,\\ldots,k\\). A graph is a Directed Acyclic Graph (DAG) if all edges are directed and there are no two nodes \\(v_i\\) and \\(v_j\\) with a directed path in both directions.\nA path between \\(v_1\\) and \\(v_k\\), \\(v_1,\\ldots, v_k\\), is blocked by a set of nodes, \\(S\\), if for some \\(v_j\\) in \\(S\\)\n\n\\(v_j\\in S\\) and \\(v_k\\) is a mediator or confounder between \\(v_{j-1}\\) and \\(v_{j+1}\\) in either direction or\n\\(v_j\\notin S\\) and all of the descendants of \\(v_j \\notin S\\) and \\(v_{j}\\) is a collider between \\(v_{j-1}\\) and \\(v_{j+1}\\).\n\nIn other words, a path is blocked if a mediator or confounder is included in \\(S\\) or a collider and all of its descendants is excluded from \\(S\\).\nFor 1. this is equivalent to saying one of\n\n\\(v_{j-1}\\rightarrow v_{j} \\rightarrow v_{j+1}\\)\n\\(v_{j-1}\\leftarrow v_{j} \\leftarrow v_{j+1}\\)\n\\(v_{j-1}\\leftarrow v_{j} \\rightarrow v_{j+1}\\)\n\nholds. For 2. recall a collider is \\(v_{j-1}\\rightarrow v_{j} \\leftarrow v_{j+1}\\).\nWe say that two nodes or groups of nodes are D-Separated by a set of nodes, \\(S\\), if every path between nodes in the two groups is blocked by \\(S\\).\nConsider the following graph.\n\n\n\n\n\nHere are the minimal valid D-separating paths between \\(X\\) and \\(Y\\):\n\n\\(S = \\{X_2, X_3\\}\\)\n\\(S = \\{X_3, X_5\\}\\)\n\\(S = \\{X_4, X_5\\}\\)\n\nHere are some invalid D-separating sets.\n\n\\(S\\) equal to any single node.\n\n\\(S=\\{X_3\\}\\) does not block the path \\(X\\leftarrow X_2 \\rightarrow X_3 \\leftarrow X_5 \\rightarrow Y\\).\n\\(S=\\{X_4\\}\\) or \\(S=\\{X_2\\}\\) does not block the path \\(X \\leftarrow X_3 \\leftarrow X_5 \\rightarrow Y\\).\n\\(S=\\{X_5\\}\\) does not block the path \\(X \\leftarrow X_3 \\leftarrow X_4 \\rightarrow Y\\).\n\n\\(S=\\{X_3, X_4\\}\\) does not block the path \\(X \\leftarrow X_2 \\rightarrow X_3 \\leftarrow X_5 \\rightarrow Y\\).\n\\(S=\\{X_2, X_4\\}\\) does not block the path \\(X\\leftarrow X_3 \\leftarrow X_5 \\rightarrow Y\\).\n\nImportantly, if \\(S\\) D-separates \\(X\\) and \\(Y\\) then in the implied distribution from the SCM \\(X\\) is independent from \\(Y\\) given \\(S\\), written \\(X \\perp Y ~|~ S\\). One can, for example, look at the empirical dependence between \\(X\\) and \\(Y\\) given \\(S\\) to investigate consistency with a graphical model."
  },
  {
    "objectID": "causal.html#do-calculus-and-backdoor-criterion",
    "href": "causal.html#do-calculus-and-backdoor-criterion",
    "title": "18  Causal DAGs",
    "section": "18.2 Do calculus and backdoor criterion",
    "text": "18.2 Do calculus and backdoor criterion\nRecall that specifying a causal graph model implies the independence relationships of a probability distribution under assumptions such as the SCM.\nConsider a theoretical intervention obtained by setting \\(X = a\\), which we write as \\(do(X) = a\\). We want to estimate \\(P(Y ~|~ do(X) = a)\\).\nA set \\(Z\\) satisfies the back door criterion with respect to nodes \\(X\\) and \\(Y\\) if\n\nNo descendant of \\(X\\) is in \\(Z\\).\n\\(Z\\) blocks every path \\(X\\) and \\(Y\\) that contains an arrow pointing to \\(Y\\).\n\nThe back door criteria is similar to D-separation. However, we only focus on arrows pointing into \\(Y\\) and don’t allow for descendants of \\(X\\). In the example we worked out above, it’s the same, since there are no descendants of \\(Y\\) and the only descendant of \\(X\\) is \\(Y\\).\nThe magic of the back door adjustment comes from the relationship, the adjustment formula:\n\\[\nP(Y ~|~ do(X) = x) = \\sum_{z\\in S} P(y ~ | x, z) p(z)\n\\]\nwhere \\(S\\) satisfies the back door criterion. If the \\(z\\) are all observed variables, then the RHS of this equation is estimable. Note the interesting statement that not all variables need to be observed, just \\(y\\), \\(x\\) and \\(z\\). So, in our previous example, adjusting for \\(S = \\{X_2, X_3\\}\\) allows us to estimate the causal effect of \\(X\\) on \\(Y\\), even if \\(X_4\\) and \\(X_5\\) are not measured.\nIt’s important to emphasize, that every aspect of the adjustment formula is theoretically estimable if \\(Y\\), \\(X\\) and the nodes in \\(S\\) are observed.\n\n18.2.1 Example graphs\nIn all the following, we’re interested in the causal effect of \\(X\\) and \\(Y\\). \\(Z\\) variables are observed and \\(U\\) variables are unobserved. Every variable is binary to make the discussion easier.\n\n18.2.1.1 Randomization\nIf \\(X\\) is randomized and everyone takes the treatment assigned to them then \\(X\\) has no parents other than the randomization mechanism,\\(R\\). We’re omitting any descendants of \\(X\\) since we don’t have to worray about them. Then, regardless of the complexity of the relationship between the collection of observed, unobserved, known and unknown variables, \\(Z, U\\), and \\(Y\\) we can estimate the causal effect simply without conditioning on anything.\nIn contrast, if some people ignore their randomized treatment status and elect to choose a different treatment one may have opened a backdoor path. For example, if the treatment can’t be blinded and those randomized to the control with the worst baseline symptoms elect to obtain the treatment elsewhere.\n\n\n\n\n\n\n\n18.2.1.2 Simple confounding\nThe diagram below shows classic confounding. Conditioning in \\(Z\\) allows for the estimation of the causal effect.\n\n\n\n\n\nNow the estimate of the adjusted effect (under our assumptions) is\n\\[\nP(Y ~|~ do(X) = x) = P(Y ~|~ X=x, z = 0)P(z = 0) + P(Y ~|~ X=x, Z=1)P(Z=1)\n\\]\nIn the following two examples, the unmeasured confounder \\(U\\) can be controlled for by conditioning on \\(Z\\) and exactly the same estimate can be used as in the simple confounding model.\n\n\n(-0.3, 1.3)\n\n\n\n\n\n\n\n18.2.1.3 Mediation\nIn mediation, all or part of the effect of \\(X\\) on \\(Y\\) flows through yet another variable \\(Z\\).\n\n\n\n\n\nThe backdoor criteria does not apply here, since \\(Z\\) is a descendant of \\(X\\). To answer the question “What is the causal effect of \\(X\\) on \\(Y\\)?” one need not adjust. However, mediation is typically studied in a different way. Instead, one asks questions such as “How much of the effect of \\(X\\) on \\(Y\\) flows or doesn’t flow through \\(Z\\)?”. To answer this question, one usually conditions on \\(Z\\) for a different goal than the backdoor adjustment is accomplishing.\nCinelli, Forney, and Pearl (2021) shows an interesting example of mediation where one would want to adjust for \\(Z\\) (left plot below).\n\n\n(-0.3, 1.3)\n\n\n\n\n\nIn this case, \\(M\\) still mediates the relationship between \\(X\\) and \\(Y\\). However, \\(Z\\) is in a backdoor path to \\(M\\). So, some of the variation in \\(M\\) that impacts \\(Y\\) could be due to \\(Z\\) rather than \\(X\\). The right plot is similar and makes the point more explicit. \\(Z\\) confounds the relationship between \\(X\\) and \\(Y\\) through \\(M\\). Without adjusting for \\(Z\\), the path \\(X\\leftarrow Z \\rightarrow M \\rightarrow Y\\) remains unblocked.\n\n\n18.2.1.4 Bad controls\nThe following are all unhelpful for conditioning on \\(Z\\) using the backdoor criteria.\nUpper left. Adjusting for colliders is the standard bad control. Below adjusting for \\(Z\\) open ups a backdoor path that was closed. From a common sense perspective, why would you want to adjust for a consequence of \\(X\\) and \\(Y\\) when exploring their relationship?\nIn the upper right diagram below, \\(Z\\) is a so-called instrumental variable. A good example is \\(Z\\) being the randomization indicator and \\(X\\) being the treatment the person actually took. It is important in this example to emphasize that use of the instrumental variable is often a very fruitful method of analysis. However, it’s not a useful backdoor adjustment and conditioning on \\(Z\\) simply removes most of the relevant variation in \\(X\\). If one wants to use \\(Z\\) as an instrumental variable in this setting, then specific methods taylored to instrumental variable use need to be employed.\nIn the lower left plot, \\(Z\\) is a descendant of \\(X\\). Conditioning on \\(Z\\) removes relevant pathway information regarding the relationship between \\(X\\) and \\(Y\\)>\nThe lower right plot is similar. Conditioning on \\(Z\\) removes variation in \\(M\\) which hinders our ability to study the relationship between \\(X\\) and \\(Y\\) through \\(M\\).\n\n\n\n\n\n\n\n18.2.1.5 Conditioning may help\nIn the upper left plot, adjusting for \\(Z\\) may reduce variability in \\(Y\\) to help focus on the relationship between \\(X\\) and \\(Y\\).\nIn the upper left plot, adjusting for \\(Z\\) may reduce variation in the mediator unrelated to the relationship between \\(X\\) and \\(Y\\).\n\n\n(-0.3, 1.3)"
  },
  {
    "objectID": "causal.html#reading",
    "href": "causal.html#reading",
    "title": "18  Causal DAGs",
    "section": "18.3 Reading:",
    "text": "18.3 Reading:\n\nThe definitive causal reference is Pearl (2009).\nI got a lot of this stuff from Peters, Janzing, and Schölkopf (2017), which you can read here\nAlso read Hardt and Recht (2021), which you can read here\nA crash course in good and bad controls, or here\ndagitty\n\n\n\n\n\nCinelli, Carlos, Andrew Forney, and Judea Pearl. 2021. “A Crash Course in Good and Bad Controls.” Sociological Methods & Research, 00491241221099552.\n\n\nHardt, Moritz, and Benjamin Recht. 2021. “Patterns, Predictions, and Actions: A Story about Machine Learning.” arXiv Preprint arXiv:2102.05242.\n\n\nPearl, Judea. 2009. Causality. Cambridge university press.\n\n\nPeters, Jonas, Dominik Janzing, and Bernhard Schölkopf. 2017. Elements of Causal Inference: Foundations and Learning Algorithms. The MIT Press."
  },
  {
    "objectID": "validation.html",
    "href": "validation.html",
    "title": "19  Machine learning validation",
    "section": "",
    "text": "(begg?)"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Bertin, Jacques. 1983. Semiology of Graphics. University of\nWisconsin press.\n\n\nBreiman, Leo. 2001. “Statistical Modeling: The Two Cultures (with\nComments and a Rejoinder by the Author).” Statistical\nScience 16 (3): 199–231.\n\n\nCarswell, C Melody. 1992. “Choosing Specifiers: An Evaluation of\nthe Basic Tasks Model of Graphical Perception.” Human\nFactors 34 (5): 535–54.\n\n\nCinelli, Carlos, Andrew Forney, and Judea Pearl. 2021. “A Crash\nCourse in Good and Bad Controls.” Sociological Methods &\nResearch, 00491241221099552.\n\n\nCleveland, William S. 1987. “Research in Statistical\nGraphics.” Journal of the American Statistical\nAssociation 82 (398): 419–23.\n\n\nCleveland, William S, and Susan J Devlin. 1980. “Calendar Effects\nin Monthly Time Series: Detection by Spectrum Analysis and Graphical\nMethods.” Journal of the American Statistical\nAssociation 75 (371): 487–96.\n\n\nCleveland, William S, and Robert McGill. 1984. “The Many Faces of\na Scatterplot.” Journal of the American Statistical\nAssociation 79 (388): 807–22.\n\n\n———. 1986. “An Experiment in Graphical Perception.”\nInternational Journal of Man-Machine Studies 25 (5): 491–500.\n\n\nDiaconis, Persi. 2006. “Theories of Data Analysis: From Magical\nThinking Through Classical Statistics.” Exploring Data\nTables, Trends, and Shapes, 1–36.\n\n\nDonoho, David. 2017. “50 Years of Data Science.”\nJournal of Computational and Graphical Statistics 26 (4):\n745–66.\n\n\nEytam, Eleanor, Noam Tractinsky, and Oded Lowengart. 2017. “The\nParadox of Simplicity: Effects of Role on the Preference and Choice of\nProduct Visual Simplicity Level.” International Journal of\nHuman-Computer Studies 105: 43–55.\n\n\nGilland, Karen L, Benjamin MW Tsui, Yujin Qi, and Grant T Gullberg.\n2006. “Comparison of Channelized Hotelling and Human Observers in\nDetermining Optimum OS-EM Reconstruction Parameters for Myocardial\nSPECT.” IEEE Transactions on Nuclear Science 53 (3):\n1200–1204.\n\n\nHardin, Johanna, Roger Hoerl, Nicholas J Horton, Deborah Nolan, Ben\nBaumer, Olaf Hall-Holt, Paul Murrell, et al. 2015. “Data Science\nin Statistics Curricula: Preparing Students to ‘Think with\nData’.” The American Statistician 69 (4): 343–53.\n\n\nHardt, Moritz, and Benjamin Recht. 2021. “Patterns, Predictions,\nand Actions: A Story about Machine Learning.” arXiv Preprint\narXiv:2102.05242.\n\n\nHicks, Stephanie C, and Rafael A Irizarry. 2018. “A Guide to\nTeaching Data Science.” The American Statistician 72\n(4): 382–91.\n\n\nInbar, Ohad, Noam Tractinsky, and Joachim Meyer. 2007. “Minimalism\nin Information Visualization: Attitudes Towards Maximizing the Data-Ink\nRatio.” In Proceedings of the 14th European Conference on\nCognitive Ergonomics: Invent! Explore!, 185–88.\n\n\nKahneman, Daniel, Stewart Paul Slovic, Paul Slovic, and Amos Tversky.\n1982. Judgment Under Uncertainty: Heuristics and Biases.\nCambridge university press.\n\n\nKahneman, Daniel, and Amos Tversky. 1972. “Subjective Probability:\nA Judgment of Representativeness.” Cognitive Psychology\n3 (3): 430–54.\n\n\nKaplan, Jennifer J, Neal T Rogness, and Diane G Fisher. 2014.\n“Exploiting Lexical Ambiguity to Help Students Understand the\nMeaning of Random.” Statistics Education Research\nJournal 13 (1): 9–24.\n\n\nKass, Robert E. 2011. “Statistical Inference: The Big\nPicture.” Statistical Science: A Review Journal of the\nInstitute of Mathematical Statistics 26 (1): 1.\n\n\nKass, Robert E, Brian S Caffo, Marie Davidian, Xiao-Li Meng, Bin Yu, and\nNancy Reid. 2016. “Ten Simple Rules for Effective Statistical\nPractice.” PLoS Computational Biology. Public Library of\nScience.\n\n\nKingma, Diederik P, and Max Welling. 2013. “Auto-Encoding\nVariational Bayes.” arXiv Preprint\narXiv:1312.6114.\n\n\nKruskal, William, and Frederick Mosteller. 1979a. “Representative\nSampling, i: Non-Scientific Literature.” International\nStatistical Review/Revue Internationale de Statistique, 13–24.\n\n\n———. 1979b. “Representative Sampling, II: Scientific Literature,\nExcluding Statistics.” International Statistical Review/Revue\nInternationale de Statistique, 111–27.\n\n\n———. 1979c. “Representative Sampling, III: The Current Statistical\nLiterature.” International Statistical Review/Revue\nInternationale de Statistique, 245–65.\n\n\n———. 1980. “Representative Sampling, IV: The History of the\nConcept in Statistics, 1895-1939.” International Statistical\nReview/Revue Internationale de Statistique, 169–95.\n\n\nLeek, Jeffery T, and Roger D Peng. 2015. “What Is the\nQuestion?” Science 347 (6228): 1314–15.\n\n\nNorman, Donald A. 2007. “Simplicity Is Highly Overrated.”\nInteractions 14 (2): 40–41.\n\n\nPearl, Judea. 2009. Causality. Cambridge university press.\n\n\nPeters, Jonas, Dominik Janzing, and Bernhard Schölkopf. 2017.\nElements of Causal Inference: Foundations and Learning\nAlgorithms. The MIT Press.\n\n\nPlaut, Elad. 2018. “From Principal Subspaces to Principal\nComponents with Linear Autoencoders.” arXiv Preprint\narXiv:1804.10253.\n\n\nSegel, Edward, and Jeffrey Heer. 2010. “Narrative Visualization:\nTelling Stories with Data.” IEEE Transactions on\nVisualization and Computer Graphics 16 (6): 1139–48.\n\n\nTufte, ER. 1990. “Data-Ink Maximization and Graphical\nDesign.” Oikos, 130–44.\n\n\nTukey, John W. 1962. “The Future of Data Analysis.” The\nAnnals of Mathematical Statistics 33 (1): 1–67.\n\n\nUnwin, Antony, Chris Volinsky, and Sylvia Winkler. 2003. “Parallel\nCoordinates for Exploratory Modelling Analysis.”\nComputational Statistics & Data Analysis 43 (4): 553–64.\n\n\nWickham, Hadley. 2006. “Exploratory Model Analysis.”\n\n\n———. 2010. “A Layered Grammar of Graphics.” Journal of\nComputational and Graphical Statistics 19 (1): 3–28.\n\n\nWickham, Hadley, Dianne Cook, Heike Hofmann, and Andreas Buja. 2010.\n“Graphical Inference for Infovis.” IEEE Transactions on\nVisualization and Computer Graphics 16 (6): 973–79.\n\n\nWilkinson, Leland. 2012. “The Grammar of Graphics.” In\nHandbook of Computational Statistics, 375–414. Springer.\n\n\n———. 2013. The Grammar of Graphics. Springer Science &\nBusiness Media.\n\n\nYang, Jiancheng, Rui Shi, Donglai Wei, Zequan Liu, Lin Zhao, Bilian Ke,\nHanspeter Pfister, and Bingbing Ni. 2021. “MedMNIST V2: A\nLarge-Scale Lightweight Benchmark for 2D and 3D Biomedical Image\nClassification.” arXiv Preprint arXiv:2110.14795.\n\n\nYoung, Kyle, Gareth Booth, Becks Simpson, Reuben Dutton, and Sally\nShrapnel. 2019. “Deep Neural Network or Dermatologist?” In\nInterpretability of Machine Intelligence in Medical Image Computing\nand Multimodal Learning for Clinical Decision Support, 48–55.\nSpringer."
  }
]