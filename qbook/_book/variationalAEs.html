<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.245">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Advanced Data Science for Public Health - 11&nbsp; Variational autoencoders</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./nlp.html" rel="next">
<link href="./unsupervised.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Variational autoencoders</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Advanced Data Science for Public Health</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/smart-stats/advanced_ds4bio_book" title="Source Code" class="sidebar-tool px-1"><i class="bi bi-github"></i></a>
</div>
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Preface</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./interactive.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Interactive graphics</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./webscraping.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Advanced web scrapping</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./images.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Working with images</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./databases.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Databases</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./pipelines.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Pipelines</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./data_structures.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Data structures</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./diymlai.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">DIY ML/AI</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./convnet_example.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Example convnet</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./unsupervised.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Unsupervised learning</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./variationalAEs.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Variational autoencoders</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./nlp.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">NLP</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./data_analysis_theory.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Data science, conceptually</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./stat_language.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Statistics and language</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./data_science_as_a_science.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Data science as an applied science</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./graphics.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Theory of graphical display</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./causal.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Causal DAGs</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#variational-bayes" id="toc-variational-bayes" class="nav-link active" data-scroll-target="#variational-bayes"><span class="toc-section-number">11.1</span>  Variational Bayes</a></li>
  <li><a href="#vaes" id="toc-vaes" class="nav-link" data-scroll-target="#vaes"><span class="toc-section-number">11.2</span>  VAEs</a>
  <ul class="collapse">
  <li><a href="#introduction-to-vaes" id="toc-introduction-to-vaes" class="nav-link" data-scroll-target="#introduction-to-vaes"><span class="toc-section-number">11.2.1</span>  Introduction to VAEs</a></li>
  <li><a href="#gaussian-vaes" id="toc-gaussian-vaes" class="nav-link" data-scroll-target="#gaussian-vaes"><span class="toc-section-number">11.2.2</span>  Gaussian VAEs</a></li>
  <li><a href="#example-using-chest-x-rays" id="toc-example-using-chest-x-rays" class="nav-link" data-scroll-target="#example-using-chest-x-rays"><span class="toc-section-number">11.2.3</span>  Example using chest x-rays</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Variational autoencoders</span></h1>
</div>



<div class="quarto-title-meta">

    
    
  </div>
  

</header>

<section id="variational-bayes" class="level2" data-number="11.1">
<h2 data-number="11.1" class="anchored" data-anchor-id="variational-bayes"><span class="header-section-number">11.1</span> Variational Bayes</h2>
<p>Since we’re talking about variational autoencoders, we should probably first talk about variational Bayes (VB). VB is a scaling solution for Bayesian inference.</p>
<p>Bayesian inference tends to follow something like this. We have a likelihood, <span class="math inline">\(f(x | \theta)\)</span>, and a prior, <span class="math inline">\(f(\theta)\)</span>. We use Bayes rule to calculate <span class="math inline">\(f(\theta | x) = f(x|\theta)p(\theta) / f(X)\)</span> where <span class="math inline">\(f(x) = \int f(x, \theta) d\theta\)</span>. The posterior, <span class="math inline">\(f(\theta | x)\)</span> is our object of interest for which we will make inferences. Some problems arise. Often can’t calculate <span class="math inline">\(f(x)\)</span>, since the integral is intractable. Monte Carlo solutions generate <span class="math inline">\(\theta^{(i)} \sim f(\theta | x)\)</span> and then approximate expectations from the posterior, <span class="math inline">\(E[h(\theta) | X]\)</span> with <span class="math inline">\(\frac{1}{N} \sum_{i=1}^N h(\theta^{(i)})\)</span>, which converges by the law of large numbers. Sometimes we can’t simulate iid variates from <span class="math inline">\(f(\theta | x)\)</span>, so we’ll sample from <span class="math inline">\(f(\theta_k | \theta_{\sim k}, x)\)</span> for each <span class="math inline">\(k\)</span>. This is called Gibbs sampling and generates a Markov chain that, under assumptions, yields variables that satisfy LLNs. Sometimes we can’t do the Gibbs sampler and more elaborate sampling schemes need to be done. All in all, this makes Bayes analysis challenging and often difficult to scale.</p>
<p>Many solutions use candidate distributions. Say, for example, <span class="math inline">\(g(z|x)\)</span> is a distribution that we can sample from. Then notice that for samples <span class="math inline">\(Z^{(i)}\sim g(z|x)\)</span> <span class="math display">\[
\frac{\sum_{i=1}^N h(Z^{(i)})\frac{f(Z^{(i)|x})}{g(z^{(i)}|x)}}{\sum_{i=1}^N \frac{f(Z^{(i)|x})}{g(Z^{(i)}|x)}} = \frac{\sum_{i=1}^N h(Z^{(i)})\frac{f(Z^{(i), x})}{g(Z^{(i)}|x)}}{\sum_{i=1}^N \frac{f(Z^{(i), x})}{g(Z^{(i)}|x)}} \rightarrow \frac{E[h(Z) | x]}{E[1 | x]} = E[h(Z) | x]
\]</span> Of note we can calculate these weights, since we know <span class="math inline">\(g(z|x)\)</span> and <span class="math inline">\(f(z,x) = f(x|z)f(z)\)</span> is the likelihood times the prior.</p>
<p>Importance sampling, Gibbs sampling and other Monte Carlo techniques get combined for complicated problems. However, modern problems often present a challenge that Monte Carlo techniques cannot solve. Enter variational Bayes. Instead of fixing up samples from <span class="math inline">\(g\)</span> to be exactly what we want, why don’t we choose <span class="math inline">\(g\)</span> as well as possible and simply use it instead of <span class="math inline">\(f(z|x)\)</span>?</p>
<p>This is where variational Bayes comes in. It turns a MC sampling problem into an optimization problem. Wikipedia <a href="https://en.wikipedia.org/wiki/Variational_Bayesian_methods">has a pretty nice introduction to the topic</a>. The most common version of variational Bayes uses the KL divergence. I.e. choose <span class="math inline">\(g\)</span> to minimize</p>
<p><span class="math display">\[
\int g(z|x) \log\left( \frac{g(z|x)}{f(z|x)} \right) dz
= E_{g(z|x)}\left[\log\left( \frac{g(Z|x)}{f(Z|x)} \right)\right]
\equiv D_{KL}\{g(Z|x) || f(Z|x) \}
\]</span></p>
<p>Often <span class="math inline">\(g(z|x)\)</span> is chosen to be be <span class="math inline">\(\prod_j g(z_j|x)\)</span>, or independent in the components of <span class="math inline">\(z\)</span>. Then it can be shown that the best approximation to the posterior, in the sense of minimizing the KL divergence, sets</p>
<p><span class="math display">\[
\hat g (z_j | x) \propto \exp\left\{ E_{i\neq j}[\log\{f(Z_i | x)\}]\right\}
\]</span></p>
<p>It is called variational Bayes, since it uses calculus of variations to solve this equation. Go over the Wikipedia article’s Gaussian / Gamma prior example. There you can see the derivation of the variational posterior approximation in a case where it can be done analytically. In most cases, one is left with doing it via algorithmmic optimization.</p>
</section>
<section id="vaes" class="level2" data-number="11.2">
<h2 data-number="11.2" class="anchored" data-anchor-id="vaes"><span class="header-section-number">11.2</span> VAEs</h2>
<section id="introduction-to-vaes" class="level3" data-number="11.2.1">
<h3 data-number="11.2.1" class="anchored" data-anchor-id="introduction-to-vaes"><span class="header-section-number">11.2.1</span> Introduction to VAEs</h3>
<p>Variational autoencoders were introduced in <span class="citation" data-cites="kingma2013auto">(<a href="references.html#ref-kingma2013auto" role="doc-biblioref">Kingma and Welling 2013</a>)</span>. A really good tutorial can be found <a href="https://jaan.io/what-is-variational-autoencoder-vae-tutorial/">here</a> and some sample code on MNIST can be found <a href="https://github.com/Jackson-Kang/Pytorch-VAE-tutorial/blob/master/01_Variational_AutoEncoder.ipynb">here</a>. An alternate way to think about autoencoders is via variational Bayes arguments. Let <span class="math inline">\(x_i\)</span> be a record for <span class="math inline">\(i = 1,\ldots,n\)</span>. For now, let’s drop the subscript <span class="math inline">\(i\)</span>. Define the following:</p>
<ul>
<li><span class="math inline">\(p_\theta(x | z)\)</span> is the likelihood of <span class="math inline">\(z\)</span> when viewed as a function of <span class="math inline">\(z\)</span> or is the data generating distribution if viewed as a function of <span class="math inline">\(x\)</span>;</li>
<li><span class="math inline">\(p_\theta(x)\)</span> is the data marginal, often called the evidence in this literature;</li>
<li><span class="math inline">\(p_\theta(z | x)\)</span> is the posterior (of <span class="math inline">\(z\)</span> given <span class="math inline">\(x\)</span>);</li>
<li>Note <span class="math inline">\(p_\theta(z | x) = p_\theta(x, z) / p_\theta(x)\)</span>;</li>
<li>Note <span class="math inline">\(p_\theta(x) = \int p_\theta(x, z) dz\)</span>;</li>
</ul>
<p>We could view any latent probability distribution as an autoencoder, where <span class="math inline">\(p_\theta(z | x)\)</span> is the encoder and <span class="math inline">\(p_\theta(x | z)\)</span> is the decoder. Note, if <span class="math inline">\(x\)</span> is an image, say, then it is the <em>distribution</em> <span class="math inline">\(p_\theta(x | z)\)</span> that characterizes an image given its latent representation <span class="math inline">\(z\)</span>. So, if we want an image, we need to look at the mean, posterior mode or even just a sample generation of <span class="math inline">\(p_\theta(x | z)\)</span>.</p>
<p>One issue with this approach is that computing is quite hard for problems of sufficient scale. Variational Bayes uses approximations instead of the actual distributions. Let <span class="math inline">\(q_\phi(z | x)\)</span> be an approximiation of the posterior. Typical variational Bayes uses minmizers of the KL divergence. Variational autoencoders do that as well. However, VAEs tend to maximize the ELBO, evidence lower bound (ELBO). We define the ELBO as</p>
<p><span class="math display">\[
L_{\phi, \theta}(x) = \log\{p_\theta(x)\} - E_{q_\phi(z | x)} \left[\log\left( \frac{q_\phi(Z | x)}{p_\theta(Z | x)}  \right)\right]
= \log\{p_\theta(x)\} - D_{KL}\{q_\phi(Z | x) || p_\theta(Z | x)\}
\]</span> where <span class="math inline">\(D_{KL}(a||b)\)</span> is the Kullback/Liebler divergence between distributions <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>. The ELBO is a lower bound since the divergence term is positive and so the following inequality holds: <span class="math display">\[
L_{\phi, \theta}(x) \leq \log\{p_\theta(x)\}.
\]</span> Where, recall, we call <span class="math inline">\(p_\theta(x)\)</span> the evidence (in this area). That it’s a bound on the evidence suggest that maximizing it is a good thing. Regardless, we can see that maximizing ELBO obviously does two good things. First, it maximizes <span class="math inline">\(p_\theta(x)\)</span>, i.e.&nbsp;that the model fits the data well. Secondly, it minmizes <span class="math inline">\(E_{q_\phi(z | x)} \left[\log\left( \frac{q_\phi(Z | x)}{p_\theta(Z | X)} \right)\right]\)</span>, or the KL divergence between the approximation and what it’s approximating.</p>
<p>We can rewrite the ELBO as (try for HW!): <span class="math display">\[
E_{q_\phi(z | x)} \left[\log\left( p_\theta(x | Z) \right)\right]
- D_{KL}\left\{ q_\phi(Z | x) || p_\theta(Z) \right\}
\]</span></p>
</section>
<section id="gaussian-vaes" class="level3" data-number="11.2.2">
<h3 data-number="11.2.2" class="anchored" data-anchor-id="gaussian-vaes"><span class="header-section-number">11.2.2</span> Gaussian VAEs</h3>
<p>Consider the following assumptions:</p>
<ol type="1">
<li><span class="math inline">\(p_\theta(z)\)</span> is a multivariate standard normal (i.e.&nbsp;<span class="math inline">\(N(J,I)\)</span>) for <span class="math inline">\(J\)</span> a vector of zeros and <span class="math inline">\(I\)</span> an identity matrix)</li>
<li><span class="math inline">\(q_\phi(z | x) = N(\mu_\phi(x), \mbox{Diag}(\sigma_\phi(x)))^2\)</span></li>
<li><span class="math inline">\(p_\theta(x | z)\)</span> is a multivariate normal with mean <span class="math inline">\(\theta_1(z)\)</span> and variance matrix <span class="math inline">\(\theta_2 I\)</span>.</li>
</ol>
<p>We can write <span class="math inline">\(z = \mu_\phi(x) + \mbox{Diag}(\sigma(x)) \epsilon\)</span> where <span class="math inline">\(\epsilon\)</span> is multivariate standard normal. Moreover, under these assumptions it can be shown that <span class="math display">\[
D_{KL}\left\{ q_\phi(Z | x) || p_\theta(Z)\right\} = \frac{1}{2}\sum_{j=1}^J \left(1 + \log\{\sigma_{\phi,j}(x)^2\} - \mu_{\phi,j}(x)^2 - \sigma_{\phi,j}(x)^2 \right)
\]</span> where a subscript <span class="math inline">\(j\)</span> refers to the vector component. If we have a Monte Carlo sample, <span class="math inline">\(\epsilon^{(l)}\)</span> then we can approximate it as follows:</p>
<p><span class="math display">\[
\begin{aligned}  
&amp; E_{q_\phi(z | x)} \left[\log\left\{ p_\theta(x | Z) \right\}\right] \\
\approx &amp; \frac{1}{L} \sum_{l=1}^L \log\{ p_\theta(x | z = \mu_\phi(x) + \mbox{Diag}(\sigma_\phi(x) \epsilon^{(l)})\} \\  
= &amp; -\frac{1}{2L} \sum_{l=1}^L ||x - \theta_1\{ \mu_\phi(x) + \mbox{Diag}(\sigma_\phi(x) \epsilon^{(l)}\}||^2 / \theta_2 \\
\end{aligned}
\]</span></p>
<p>Both of these expressions are the contributions of one row, <span class="math inline">\(x\)</span>. They get summed over <span class="math inline">\(x\)</span> to obtain the full ELBO. Here we can see why expressing <span class="math inline">\(z = \mu_\phi(x) + \mbox{Diag}(\sigma_\phi(x) \epsilon^{(l)\)</span> is important. This way our parameters, <span class="math inline">\(\mu_\phi\)</span> and <span class="math inline">\(\sigma_\phi\)</span> are in the loss function, not hidden in <span class="math inline">\(z\)</span>. The backprop algorithm doesn’t know how to take a derivative with respect to a realized random variable. It can, however, differentiate <span class="math inline">\(\mu_\phi(x) + \mbox{Diag}(\sigma_\phi(x) \epsilon\)</span> for realized <span class="math inline">\(\epsilon\)</span>.</p>
</section>
<section id="example-using-chest-x-rays" class="level3" data-number="11.2.3">
<h3 data-number="11.2.3" class="anchored" data-anchor-id="example-using-chest-x-rays"><span class="header-section-number">11.2.3</span> Example using chest x-rays</h3>
<p>We’re omitting our imports and gettng the data. See the <a href="https://github.com/smart-stats/advanced_ds4bio_book/blob/main/qbook/variationalAEs.qmd">qmd file</a> for the full list. Here we have epochs as 10 and batch size as 128.</p>
<div class="cell" data-execution_count="2">
<div class="cell-output cell-output-stdout">
<pre><code>Using downloaded and verified file: /home/bcaffo/.medmnist/chestmnist.npz</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Using downloaded and verified file: /home/bcaffo/.medmnist/chestmnist.npz</code></pre>
</div>
</div>
<p>Here’s a montage of the training data.</p>
<div class="cell" data-execution_count="3">
<div class="cell-output cell-output-stderr">
<pre><code>/home/bcaffo/miniconda3/envs/ds4bio/lib/python3.10/site-packages/medmnist/utils.py:25: FutureWarning:

`multichannel` is a deprecated argument name for `montage`. It will be removed in version 1.0. Please use `channel_axis` instead.
</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="3">
<p><img src="variationalAEs_files/figure-html/cell-4-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>Here’s the shape of the dataset. Note, <span class="math inline">\(28^2=784\)</span> is the vectorized size. Also, as these images are gray scale so that there’s only one color channel. We’re not going to use any convolutional layers or anything like that. We’ll just flatten everything.</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>temp, _ <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(train_dataset))</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>temp <span class="op">=</span> temp.numpy()</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>[temp.shape, temp.<span class="bu">max</span>(), temp.<span class="bu">min</span>()]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="4">
<pre><code>[(1, 28, 28), 0.93333334, 0.015686275]</code></pre>
</div>
</div>
<p>Let’s use Jackson Kang’s wonderful VAE tutorial from <a href="https://github.com/Jackson-Kang/Pytorch-VAE-tutorial/blob/master/01_Variational_AutoEncoder.ipynb">here</a>.</p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>x_dim  <span class="op">=</span> <span class="dv">784</span> <span class="co">## This is 28**2, the length of the vectorized images</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>hidden_dim <span class="op">=</span> <span class="dv">400</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>latent_dim <span class="op">=</span> <span class="dv">200</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="defining-the-encoder" class="level4" data-number="11.2.3.1">
<h4 data-number="11.2.3.1" class="anchored" data-anchor-id="defining-the-encoder"><span class="header-section-number">11.2.3.1</span> Defining the encoder</h4>
<p>First we define the encoder. Remember, the encoder is <span class="math inline">\(q_\phi(z | x)\)</span>, which we are assuming is multivariate independent normals with means and variances depending on <span class="math inline">\(x\)</span>. Notice this takes in the input, <span class="math inline">\(x\)</span> and uses a NN to estimate <span class="math inline">\(\mu_\phi(x)\)</span> and <span class="math inline">\(\log(\sigma^2_\phi(x))\)</span>.</p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Encoder(nn.Module):</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_dim, hidden_dim, latent_dim):</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(Encoder, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.FC_input <span class="op">=</span> nn.Linear(input_dim, hidden_dim)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.FC_input2 <span class="op">=</span> nn.Linear(hidden_dim, hidden_dim)</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.FC_mean  <span class="op">=</span> nn.Linear(hidden_dim, latent_dim)</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.FC_var   <span class="op">=</span> nn.Linear (hidden_dim, latent_dim)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.LeakyReLU <span class="op">=</span> nn.LeakyReLU(<span class="fl">0.2</span>)</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.training <span class="op">=</span> <span class="va">True</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>        h_       <span class="op">=</span> <span class="va">self</span>.LeakyReLU(<span class="va">self</span>.FC_input(x))</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>        h_       <span class="op">=</span> <span class="va">self</span>.LeakyReLU(<span class="va">self</span>.FC_input2(h_))</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>        mean     <span class="op">=</span> <span class="va">self</span>.FC_mean(h_)</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>        log_var  <span class="op">=</span> <span class="va">self</span>.FC_var(h_) </span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> mean, log_var</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="defining-the-decoder" class="level4" data-number="11.2.3.2">
<h4 data-number="11.2.3.2" class="anchored" data-anchor-id="defining-the-decoder"><span class="header-section-number">11.2.3.2</span> Defining the decoder</h4>
<p>Next we define our decoder. Remember our decoder is <span class="math inline">\(p_\theta(x | z)\)</span>. However, the distribution will be fully defined in the loss function. Instead, we’re just taking in <span class="math inline">\(z\)</span> and giving out <span class="math inline">\(\hat x = \theta_1(z)\)</span>.</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Decoder(nn.Module):</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, latent_dim, hidden_dim, output_dim):</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(Decoder, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.FC_hidden <span class="op">=</span> nn.Linear(latent_dim, hidden_dim)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.FC_hidden2 <span class="op">=</span> nn.Linear(hidden_dim, hidden_dim)</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.FC_output <span class="op">=</span> nn.Linear(hidden_dim, output_dim)        </span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.LeakyReLU <span class="op">=</span> nn.LeakyReLU(<span class="fl">0.2</span>)</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>        h     <span class="op">=</span> <span class="va">self</span>.LeakyReLU(<span class="va">self</span>.FC_hidden(x))</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>        h     <span class="op">=</span> <span class="va">self</span>.LeakyReLU(<span class="va">self</span>.FC_hidden2(h))</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>        x_hat <span class="op">=</span> torch.sigmoid(<span class="va">self</span>.FC_output(h))</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x_hat</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="defining-the-full-model-autoencoder-decoder" class="level4" data-number="11.2.3.3">
<h4 data-number="11.2.3.3" class="anchored" data-anchor-id="defining-the-full-model-autoencoder-decoder"><span class="header-section-number">11.2.3.3</span> Defining the full model = autoencoder / decoder</h4>
<p>To run a datapoint, <span class="math inline">\(x\)</span> through the model. First the encoder generates <span class="math inline">\(\mu_phi(x)\)</span> and <span class="math inline">\(\sigma_\phi(x)\)</span>. Next, it generates <span class="math inline">\(z\)</span> from <span class="math inline">\(q_\phi(z|x)\)</span> using a reparameterization, then passes <span class="math inline">\(z\)</span> through the decoder, <span class="math inline">\(p_\theta(x | z)\)</span>.</p>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Model(nn.Module):</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, Encoder, Decoder):</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(Model, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.Encoder <span class="op">=</span> Encoder</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.Decoder <span class="op">=</span> Decoder</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>        mean, log_var <span class="op">=</span> <span class="va">self</span>.Encoder(x)</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>        sd <span class="op">=</span> torch.exp(<span class="fl">0.5</span> <span class="op">*</span> log_var)</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>        z <span class="op">=</span> mean <span class="op">+</span> sd <span class="op">*</span> torch.randn_like(sd)</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>        x_hat <span class="op">=</span> <span class="va">self</span>.Decoder(z)</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x_hat, mean, log_var</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>encoder <span class="op">=</span> Encoder(input_dim<span class="op">=</span>x_dim, hidden_dim<span class="op">=</span>hidden_dim, latent_dim<span class="op">=</span>latent_dim)</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>decoder <span class="op">=</span> Decoder(latent_dim<span class="op">=</span>latent_dim, hidden_dim <span class="op">=</span> hidden_dim, output_dim <span class="op">=</span> x_dim)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Model(Encoder<span class="op">=</span>encoder, Decoder<span class="op">=</span>decoder)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="defining-the-elbo" class="level4" data-number="11.2.3.4">
<h4 data-number="11.2.3.4" class="anchored" data-anchor-id="defining-the-elbo"><span class="header-section-number">11.2.3.4</span> Defining the ELBO</h4>
<p>The ELBO is the sum of the reproduction loss and the KL divergence.</p>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> loss_function(x, x_hat, mean, log_var):</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    reproduction_loss <span class="op">=</span> nn.functional.mse_loss(x_hat, x, reduction<span class="op">=</span><span class="st">'sum'</span>)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    KLD      <span class="op">=</span> <span class="op">-</span> <span class="fl">0.5</span> <span class="op">*</span> torch.<span class="bu">sum</span>(<span class="dv">1</span><span class="op">+</span> log_var <span class="op">-</span> mean.<span class="bu">pow</span>(<span class="dv">2</span>) <span class="op">-</span> log_var.exp())</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> reproduction_loss <span class="op">+</span> KLD</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> Adam(model.parameters(), lr<span class="op">=</span>lr)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="training-the-neural-network" class="level4" data-number="11.2.3.5">
<h4 data-number="11.2.3.5" class="anchored" data-anchor-id="training-the-neural-network"><span class="header-section-number">11.2.3.5</span> Training the neural network</h4>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>model.train()</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(NUM_EPOCHS):</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> batch_idx, (x, _) <span class="kw">in</span> <span class="bu">enumerate</span>(train_loader):</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>        <span class="co">## Note unlike the mnist data, the final batch isn't always the same size</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>        batch_size <span class="op">=</span> x.shape[<span class="dv">0</span>]</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.view(batch_size, x_dim)</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>        x_hat, mean, log_var <span class="op">=</span> model(x)</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> loss_function(x, x_hat, mean, log_var)</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="show-an-image-passed-through-the-vae" class="level4" data-number="11.2.3.6">
<h4 data-number="11.2.3.6" class="anchored" data-anchor-id="show-an-image-passed-through-the-vae"><span class="header-section-number">11.2.3.6</span> Show an image passed through the VAE</h4>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">eval</span>()</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="co">#with torch.no_grad():</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>x, _ <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(test_loader))        </span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> x.shape[<span class="dv">0</span>]</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> x.view(batch_size, x_dim)</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>x_hat, _, _ <span class="op">=</span> model(x)</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>x     <span class="op">=</span>     x.view(batch_size, <span class="dv">28</span>, <span class="dv">28</span>)</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>x_hat <span class="op">=</span> x_hat.view(batch_size, <span class="dv">28</span>, <span class="dv">28</span>)</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>[<span class="dv">10</span>, <span class="dv">5</span>])</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>idx <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>plt.imshow(x[idx].numpy(), cmap<span class="op">=</span><span class="st">'gray'</span>, vmin <span class="op">=</span> <span class="dv">0</span>, vmax <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>plt.xticks([])</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>plt.yticks([])</span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a>plt.imshow(x_hat[idx].detach().numpy(), cmap<span class="op">=</span><span class="st">'gray'</span>, vmin <span class="op">=</span> <span class="dv">0</span>, vmax <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>plt.xticks([])</span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a>plt.yticks([])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="12">
<pre><code>([], [])</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="variationalAEs_files/figure-html/cell-13-output-2.png" width="768" height="362"></p>
</div>
</div>
</section>
<section id="generate-a-random-images-from-a-vae." class="level4" data-number="11.2.3.7">
<h4 data-number="11.2.3.7" class="anchored" data-anchor-id="generate-a-random-images-from-a-vae."><span class="header-section-number">11.2.3.7</span> Generate a random images from a VAE.</h4>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>no_im <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>noise <span class="op">=</span> torch.randn(no_im <span class="op">**</span> <span class="dv">2</span>, latent_dim)</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>generated_images <span class="op">=</span> decoder(noise)</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>im <span class="op">=</span> generated_images.view(no_im <span class="op">**</span> <span class="dv">2</span>, <span class="dv">28</span>, <span class="dv">28</span>)</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(no_im <span class="op">**</span> <span class="dv">2</span>):</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>    plt.subplot(<span class="dv">5</span>, <span class="dv">5</span>, i<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>    plt.imshow(im[i].detach().numpy(), cmap<span class="op">=</span><span class="st">'gray'</span>, vmin <span class="op">=</span> <span class="dv">0</span>, vmax <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>    plt.xticks([])</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>    plt.yticks([])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="variationalAEs_files/figure-html/cell-14-output-1.png" width="519" height="393"></p>
</div>
</div>


<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography" style="display: none">
<div id="ref-kingma2013auto" class="csl-entry" role="doc-biblioentry">
Kingma, Diederik P, and Max Welling. 2013. <span>“Auto-Encoding Variational <span>B</span>ayes.”</span> <em>arXiv Preprint arXiv:1312.6114</em>.
</div>
</div>
</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./unsupervised.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Unsupervised learning</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./nlp.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">NLP</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>